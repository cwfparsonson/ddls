{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import ddls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise what type(s) of nodes (servers) you want in your cluster and what type(s) of worker(s) you want to populate them with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from ddls.devices.processors.gpus.A100 import A100\n",
    "\n",
    "node_config = {'type_1':\n",
    "                  {\n",
    "                      'num_nodes': 16,\n",
    "                      'workers_config': \n",
    "                          [\n",
    "                              {\n",
    "                               'num_workers': 4,\n",
    "                               'worker': A100\n",
    "                              }\n",
    "                          ]\n",
    "                  }\n",
    "              }\n",
    "\n",
    "print(node_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the topology to be populated by your nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topology_config = {'type':\n",
    "                      'torus',\n",
    "                   'kwargs':\n",
    "                      {\n",
    "                          'x_dims': 4,\n",
    "                          'y_dims': 4\n",
    "                      }\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the cluster environment using your node and topology configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from ddls.environments.cluster.cluster_environment import ClusterEnvironment\n",
    "\n",
    "env = ClusterEnvironment(topology_config=topology_config,\n",
    "                          node_config=node_config,\n",
    "                          path_to_save='/scratch/datasets/ddls/sims',\n",
    "                          save_freq=100,\n",
    "                          use_sqlite_database=True)\n",
    "print(env)\n",
    "env.topology.render()\n",
    "\n",
    "for node in env.topology.graph:\n",
    "    print(f'Node {node}: {env.topology.graph.nodes[node]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `ddls` computation graph(s) you want to run on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from ddls.utils import ddls_graph_from_pbtxt_file\n",
    "from ddls.plotting.plotting import plot_computation_graph\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "# get file paths\n",
    "path_to_files = '/scratch/datasets/ddls/jobs/tensorflow_synthetic_graphs/valid'\n",
    "file_paths = glob.glob(path_to_files + '/*')\n",
    "    \n",
    "# create ddls graph\n",
    "num_graphs = 100\n",
    "ddls_computation_graphs = [ddls_graph_from_pbtxt_file(file_path, processor_type_profiled='A100', verbose=False) for file_path in file_paths[:num_graphs]]\n",
    "\n",
    "# visualise\n",
    "visualise = False\n",
    "if visualise:\n",
    "    for graph in ddls_computation_graphs:\n",
    "        fig = plot_computation_graph(graph, scaling_factor=3, title='ddls_graph', show_fig=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise `ddls` job(s) from the computation graph(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from ddls.demands.jobs.job import Job\n",
    "\n",
    "jobs = [Job(computation_graph=graph, num_training_steps=2) for graph in ddls_computation_graphs]\n",
    "\n",
    "for job in jobs:\n",
    "    print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise an operation placement agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from ddls.managers.placers.random_job_placer import RandomJobPlacer\n",
    "from ddls.managers.schedulers.srpt_job_scheduler import SRPTJobScheduler\n",
    "\n",
    "control_plane = {\n",
    "    'job_placer': RandomJobPlacer(),\n",
    "    'job_scheduler': SRPTJobScheduler()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset cluster environment and run `Cluster`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from ddls.distributions.uniform import Uniform\n",
    "from ddls.utils import seed_stochastic_modules_globally\n",
    "\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "\n",
    "# seeds = [0, 1, 2]\n",
    "seeds = [0]\n",
    "for seed in seeds:\n",
    "    print(f'\\n\\n\\n~~~~~~~~~~~~~~~~~~~~~~~ Seed {seed} ~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    seed_stochastic_modules_globally(seed)\n",
    "    obs, action_set, reward, done, info = env.reset(jobs=jobs,\n",
    "                                                    job_sampling_mode='remove',\n",
    "                                                    job_interarrival_time_dist=Uniform(min_val=1, max_val=1000),\n",
    "                                                    max_simulation_run_time=float('inf'),\n",
    "                                                    job_queue_capacity=10,\n",
    "                                                    seed=seed,\n",
    "                                                    verbose=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while not done:\n",
    "        # make decisions\n",
    "        actions = {}\n",
    "        actions['job_placement'] = control_plane['job_placer'].get_placement(cluster=env)\n",
    "        actions['job_schedule'] = control_plane['job_scheduler'].get_schedule(new_placements=actions['job_placement'], cluster=env)\n",
    "\n",
    "        # pass actions to cluster environment and step the cluster\n",
    "        obs, action_set, reward, done, info = env.step(actions, verbose=False)\n",
    "\n",
    "        print(f'Step {env.step_counter} | Jobs arrived: {env.num_jobs_arrived} | completed: {len(env.jobs_completed)} | blocked: {len(env.jobs_blocked)} | running: {len(env.jobs_running)} | queued: {len(env.job_queue)}')\n",
    "\n",
    "    print(f'\\nCompleted simulation in {time.time() - start_time:.3f} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement logic for placing job ops of each job -> step env -> time job completion for 1 training step (can then think about $n$ training steps, network communication overhead, etc.)\n",
    "\n",
    "- Have mounted job onto devices\n",
    "- Now need to work out way of tracking ops running on each device efficiently and timing how long they take. Consider having global dict tracking operations which are running to avoid having to keep looping through all ops to check dependencies. Consider also having stopwatch object similar to Noah's where only tick it when have stacked enough sequential operations. N.B. Think should assume that, once ops have been placed on a device, they must be ran sequentially (i.e. cannot run multiple ops on one device at the same time; assume time profile is for e.g. GPU worker running just that one op with <= all its cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets load our SQLite logs and plot some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from collections import defaultdict\n",
    "from sqlitedict import SqliteDict\n",
    "import pprint\n",
    "import glob\n",
    "import time\n",
    "\n",
    "\n",
    "base_folder = '/scratch/datasets/ddls/sims/'\n",
    "base_name = 'cluster'\n",
    "ids = [20]\n",
    "\n",
    "\n",
    "steps_logs_dict = defaultdict(lambda : defaultdict(list))\n",
    "sim_logs_dict = defaultdict(lambda : defaultdict(list))\n",
    "start_time = time.time()\n",
    "for i in ids:\n",
    "    agent = base_name + f'_{i}'\n",
    "    paths = [reset_folder for reset_folder in glob.glob(base_folder + f'/{base_name}/{agent}/*/')]\n",
    "    \n",
    "    for path in paths:\n",
    "        with SqliteDict(path + '/steps_log.sqlite') as log:\n",
    "            for key, val in log.items():\n",
    "                steps_logs_dict[agent][key].extend(val)\n",
    "            log.close()\n",
    "\n",
    "        with SqliteDict(path + '/sim_log.sqlite') as log:\n",
    "            for key, val in log.items():\n",
    "                sim_logs_dict[agent][key].extend(val)\n",
    "            log.close()\n",
    "        \n",
    "print(f'\\nsteps_logs_dict: {steps_logs_dict}\\n')\n",
    "print(f'\\nsim_logs_dict: {sim_logs_dict}\\n')\n",
    "print(f'\\nAll data loaded in {time.time() - start_time:.3f} s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-level metrics\n",
    "\n",
    "E.g. How many jobs, if any, were completed at each step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def augment_steps_logs_dict(steps_logs_dict):\n",
    "    '''Calculates additional metrics for steps logs dict.'''\n",
    "    _steps_logs_dict = copy.deepcopy(steps_logs_dict)\n",
    "    for agent in steps_logs_dict.keys():\n",
    "        _steps_logs_dict[agent]['step_time'] = [steps_logs_dict[agent]['step_end_time'][i] - steps_logs_dict[agent]['step_start_time'][i] for i in range(len(steps_logs_dict[agent]['step_start_time']))]\n",
    "    return _steps_logs_dict\n",
    "steps_logs_dict = augment_steps_logs_dict(steps_logs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from ddls.plotting.plotting import plot_line\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# plot config\n",
    "x = 'step_counter'\n",
    "scaling_factor = 1\n",
    "metrics_to_skip = {'step_counter'}\n",
    "\n",
    "# make plots\n",
    "metrics = steps_logs_dict[agent].keys()\n",
    "metrics_to_plot = [metric for metric in metrics if metric not in metrics_to_skip]\n",
    "print(f'Metrics to plot: {metrics_to_plot}\\nMetrics to skip: {metrics_to_skip}\\n')\n",
    "for metric in metrics_to_plot:\n",
    "    print(f'Plotting metric {metric}')\n",
    "    plot_dict = {}\n",
    "    for _agent in steps_logs_dict.keys():\n",
    "        plot_dict['Agent'] = [_agent for _ in range(len(steps_logs_dict[_agent][metric]))]\n",
    "        plot_dict[x] = steps_logs_dict[_agent][x]\n",
    "        plot_dict[metric] = steps_logs_dict[agent][metric]\n",
    "    fig = plot_line(pd.DataFrame(plot_dict), \n",
    "                    x=x, \n",
    "                    y=metric, \n",
    "                    hue='Agent', \n",
    "                    xlabel=x, \n",
    "                    ylabel=metric, \n",
    "                    err_style='band', # 'band' 'bars'\n",
    "                    ci=68, # 95 68\n",
    "                    scaling_factor=scaling_factor,\n",
    "                    show_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sim-level metrics\n",
    "\n",
    "E.g. Mean job completion time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import numpy as np\n",
    "\n",
    "# def augment_sim_logs_dict(sim_logs_dict):\n",
    "#     '''Calculates additional metrics for steps logs dict.'''\n",
    "#     _sim_logs_dict = copy.deepcopy(sim_logs_dict)\n",
    "#     for agent in sim_logs_dict.keys():\n",
    "#         for metric in ['job_completion_time']:\n",
    "#             _sim_logs_dict[agent][f'mean_{metric}'] = np.mean(sim_logs_dict[agent][metric])\n",
    "#             _sim_logs_dict[agent][f'p99_{metric}'] = np.percentile(sim_logs_dict[agent][metric], 99)\n",
    "#             _sim_logs_dict[agent][f'median_{metric}'] = np.median(sim_logs_dict[agent][metric])\n",
    "#             _sim_logs_dict[agent][f'std_{metric}'] = np.std(sim_logs_dict[agent][metric])\n",
    "#     return _sim_logs_dict\n",
    "# sim_logs_dict = augment_sim_logs_dict(sim_logs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from ddls.plotting.plotting import plot_bar, plot_hist\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "\n",
    "# plot config\n",
    "scaling_factor = 1\n",
    "metrics_to_skip = {}\n",
    "estimators = {'mean': np.mean,\n",
    "              'median': np.mean,\n",
    "              'iqr': st.iqr,\n",
    "              'gmean': st.gmean}\n",
    "\n",
    "\n",
    "# make plots\n",
    "metrics = sim_logs_dict[agent].keys()\n",
    "metrics_to_plot = [metric for metric in metrics if metric not in metrics_to_skip]\n",
    "print(f'Metrics to plot: {metrics_to_plot}\\nMetrics to skip: {metrics_to_skip}\\n')\n",
    "for metric in metrics_to_plot:\n",
    "    print(f'Plotting metric {metric}')\n",
    "    plot_dict = {}\n",
    "    for _agent in sim_logs_dict.keys():\n",
    "        plot_dict['Agent'] = [_agent for _ in range(len(sim_logs_dict[_agent][metric]))]\n",
    "        plot_dict[metric] = sim_logs_dict[agent][metric]\n",
    "        \n",
    "    df = pd.DataFrame(plot_dict)\n",
    "        \n",
    "    # hist\n",
    "    fig = plot_hist(df,\n",
    "                    x=metric,\n",
    "                    hue='Agent',\n",
    "                    xlabel=metric,\n",
    "                    element='bars',\n",
    "                    fill=True,\n",
    "                    cumulative=False,\n",
    "                    stat='count',\n",
    "                    multiple='layer',\n",
    "                    scaling_factor=scaling_factor,\n",
    "                    show_fig=True)\n",
    "    \n",
    "    # cdf\n",
    "    fig = plot_hist(df,\n",
    "                    x=metric,\n",
    "                    hue='Agent',\n",
    "                    xlabel=metric,\n",
    "                    element='step',\n",
    "                    fill=False,\n",
    "                    cumulative=True,\n",
    "                    stat='density',\n",
    "                    common_norm=False,\n",
    "                    scaling_factor=scaling_factor,\n",
    "                    show_fig=True)\n",
    "        \n",
    "    # bar chart\n",
    "    for estimator_name, estimator in estimators.items():\n",
    "        fig = plot_bar(df, \n",
    "                        x='Agent', \n",
    "                        y=metric, \n",
    "                        xlabel='Agent', \n",
    "                        ylabel=metric, \n",
    "                        estimator=estimator,\n",
    "                        title=estimator_name,\n",
    "                        scaling_factor=scaling_factor,\n",
    "                        show_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddls",
   "language": "python",
   "name": "ddls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
