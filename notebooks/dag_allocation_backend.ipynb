{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34a43d7-1ab2-43fd-851d-d7a0576a3bc9",
   "metadata": {},
   "source": [
    "# General\n",
    "\n",
    "* This notebook contains the implementation of:\n",
    "    * A ```GreedyBlockAllocator``` that implements a collective-preferring greedy allocation heuristic to pack partitioned graphs into a (sub-set of a) RAMP topology\n",
    "    * A ```Partition``` class that will create a partitioned version of a DAG based on the original file provided by pipedream\n",
    "    \n",
    "* The notebook first introduces the ```GreedyBlockAllocator``` class with pseudo-code, then the ```Partition``` class with pseudo-code, then some real code showing how they are used together (bottom of the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc63db-fdee-42e9-89dc-ecbf946641aa",
   "metadata": {},
   "source": [
    "# GreedyBlockAllocator: Job allocation in RAMP\n",
    "\n",
    "## Given:\n",
    "\n",
    "* An un-partitioned and non-mirrored job-graph,\n",
    "```python\n",
    "job_graph = nx.DiGraph()\n",
    "```\n",
    "where\n",
    "```python\n",
    "job_graph.nodes[node] = {'activation':int/float, 'parameters':int/float}\n",
    "```\n",
    "\n",
    "* A list containing the IDs of the nodes that are going to be split (maintaining the pipedream ID convention)\n",
    "```python\n",
    "mp_split_ids = ['1','2','5',...]\n",
    "```\n",
    "\n",
    "* A list containing how many times each op will be split, with indices corresponding to those from ```mp_split_ids``` above\n",
    "```python\n",
    "mp_splits = [2,2,4,...]\n",
    "```\n",
    "For example this means that node '1' will be split 2 times, node '2' 2 times, node '5' 4 times and so on.\n",
    "\n",
    "* A RAMP topology where each node is identified by its unique ```(communication-group, rack, server)``` 3-tuple identifier and records it's available resourcs and which operations are on it, as well as a 3-tuple indicating the 'shape' of this RAMP topology w.r.t. the dimensions of RAMP\n",
    "```python\n",
    "ramp_topology = {(1,1,1):{'activation':X,'ops':['1a','4b']},...}\n",
    "ramp_shape = (2,4,2)\n",
    "```\n",
    "\n",
    "## Do an allocation:\n",
    "\n",
    "* Get some allocation 'preamble' (information related to the job to be allocated that's useful for the allocation heuristic)\n",
    "```python\n",
    "sequence, splits, op_server_info, parents, children = GreedyBlockAllocator.get_allocation_preamble(job_graph,mp_split_ids,mp_splits)\n",
    "```\n",
    "where\n",
    "    * ```sequence``` is an ordered (using topological sort) list specifying the order that ops can be greedily placed (i.e. start with ops that have no children, then do their children and so on)\n",
    "    * ```splits``` is an ordered list specifying the number of splits per op, with indices corresponding to ```sequence```\n",
    "    * ```op_server_info``` is a dict maintaining information about which servers are being used for which ops (i.e. ```{'1':[(1,1,1),(1,1,2)],...}``` specifies that op '1' is split across the servers (1,1,1) and (1,1,2)). This used to check where an ops parent(s) are to see if it can be placed across the same set of servers.\n",
    "    * ```parents``` is a dict relating parent ops to their children (i.e. ```{'1':['2','5'],...}``` indicates that ops 2 and 5 are both children of op 1).\n",
    "    * ```children``` is a dict relating child ops to their parents (i.e. ```{'2':['1'],...}``` indicates that op 1 is a parent of op 2).\n",
    "\n",
    "* Specify the 3-D (w.r.t. RAMP dimensions) shape of the 'meta-block'. The 'meta-block' is the set of servers that will be reserved for a job, where following this reservation the packing heuristic will try to place all ops within this 'meta-block'\n",
    "```python\n",
    "meta_block_shape = (2,4,2)\n",
    "```\n",
    "Note that none of the dimensions of the meta-block can be larger than the dimensions of the RAMP topology in use (otherwise it would be asking for resources that do not exist within that RAMP topology)\n",
    "    * NOTE: this shape is what should be output by an agent\n",
    "\n",
    "* Find a specific set of servers in the RAMP topology that can be reserved for the meta-block. Servers can only be reserved if there are no ops currently using them (i.e. different jobs cannot share servers).\n",
    "```python\n",
    "meta_block_info = GreedyBlockAllocator.find_meta_block(ramp_topology,ramp_shape,meta_shape)\n",
    "```\n",
    "where the possible return values of ```meta_block_info``` are:\n",
    "    * ```None``` if no such block can be found\n",
    "    * ```(meta_block, meta_block_shape, meta_block_origin)``` where\n",
    "        * ```meta_block``` is a list of 3-tuple server ids that exist in RAMP and will be reserved for this meta block\n",
    "        * ```meta_block_shape``` is a 3-tuple referring to the shape of the meta-block w.r.t. RAMP dimensions\n",
    "        * ```meta_block_origin``` is a 3-tuple identifying an effective top-left corner of this meta-block (though 'top-left' not exactly accurate since RAMP follows PAC-man rules).\n",
    "* If ```meta_block_info is not None``` then call for an allocation attempt to take place\n",
    "```python\n",
    "allocated = GreedyBlockAllocator.allocate(ramp_topology,graph,sequence,splits,meta_block_info,parents,op_server_info)\n",
    "```\n",
    "where the possible return values of ```allocated``` are:\n",
    "    * ```None``` if the allocation was not possible (i.e. resources for some op could not be found during allocation)\n",
    "    * ```(ramp_topology, op_server_info)``` where both of these values are as described earlier, but updated w.r.t. the resources allocated\n",
    "    \n",
    "* If allocation was successful, then update the relevant parameters;\n",
    "```python\n",
    "ramp_topology, op_server_info = allocated\n",
    "```\n",
    "\n",
    "## Pseudo-code:\n",
    "\n",
    "```python\n",
    "# required inputs (handled outside of allocation logic)\n",
    "job_graph = get_graph_from_wherever()\n",
    "mp_split_ids, mp_splits = get_splits_from_wherever(job_graph)\n",
    "meta_block_shape = get_meta_block_shape_from_agent(job_graph)\n",
    "ramp_topology, ramp_shape = get_ramp_details_from_wherever()\n",
    "\n",
    "# get the allocation 'preamble' (job info used for the heuristic allocator)\n",
    "sequence, splits, op_server_info, parents, children = GreedyBlockAllocator.get_allocation_preamble(job_graph,mp_split_ids,mp_splits)\n",
    "\n",
    "# get a meta-block of a particular shape which the heuristic allocator will try to pack the job fully into\n",
    "meta_block_info = GreedyBlockAllocator.find_meta_block(ramp_topology,ramp_shape,meta_shape)\n",
    "\n",
    "# if a meta-block was successfully found...\n",
    "if meta_block_info:\n",
    "    # try to allocate the job\n",
    "    allocated = GreedyBlockAllocator.allocate(ramp_topology,graph,sequence,splits,meta_block_info,parents,op_server_info)\n",
    "    # if the allocation was successful...\n",
    "    if allocated:\n",
    "        # update the topology and op-server info for use in the next job allocation\n",
    "        ramp_topology, op_server_info = allocated\n",
    "```\n",
    "\n",
    "\n",
    "# Notes on meta-block allocation: allocating blocks of resources that all ops for a single job will be packed into)\n",
    "\n",
    "* The meta block is allocated w.r.t. a specific shape that is given by some agent (or whatever). This shape is found in the RAMP topology when servers can be found that have no ops currently using them (i.e. different jobs cannot share servers, but ops within the same job can).\n",
    "* PAC-man type rules apply to the search process, where if a shape search exceedes some boundary it will continue on the opposite end (i.e. the loop that iterates over RAMP dimensions is done using modulo so that dimensions are never exceeded).\n",
    "\n",
    "# General questions\n",
    "\n",
    "* [George/Alessandro] does it only make sense to put children on same servers as a parent if N(parent sub-ops) == N(children sub-ops)?\n",
    "    * If more children, then you are effectively undermining the partitioning attempt (e.g. 4 sub-ops put in 2 servers)\n",
    "    * If less children, is this a collective?\n",
    "        * I think no becuase if e.g. 4 servers where children are only on 2 of them, than there is an uneven message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80dcace3-62a8-4db0-a4f3-e41c135a75a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "\n",
    "def dummy_ramp(shape):\n",
    "    '''\n",
    "    Generates a dummy ramp 'network' which is actually\n",
    "    just a dictionary. This is fine though, since the\n",
    "    topology of ramp is not really interacted with in \n",
    "    any case.\n",
    "    '''\n",
    "    c = shape[0]\n",
    "    r = shape[1]\n",
    "    s = shape[2]\n",
    "    ramp = {}\n",
    "    for i in range(c):\n",
    "        for j in range(r):\n",
    "            for k in range(s):\n",
    "                ramp[(i,j,k)] = {'mem':1,'ops':[]}\n",
    "\n",
    "    return ramp\n",
    "\n",
    "class GreedyBlockAllocator:\n",
    "    '''\n",
    "    NOTE: These are the following simplifications to make the heuristic doable:\n",
    "            - blocks are only constructed in a 'connected' way (i.e. if you need \n",
    "            1 server per rack for 2 racks and each has to be on a different\n",
    "            communication group, the groups will be checked as 1-2, 2-3 etc not\n",
    "            1-6, 2-5 and so on. This is to avoid exhaustive searches.\n",
    "            - everything is handled in multiples of 2 (i.e. it is assumed that\n",
    "            ops are partitioned to a multiple of 2 number of sub ops etc). This \n",
    "            keeps things simple w.r.t. symmetries since an odd number of servers\n",
    "            can only be placed on RAMP in a more constrained way if the collective\n",
    "            symmetry rules are to be followed, so this keeps things simple.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_factor_pairs(n):\n",
    "        '''\n",
    "        This function returns a list of tuples specifying\n",
    "        all integer factor-pairs for a given integer, n.\n",
    "\n",
    "        This is used for finding symmetric server-blocks \n",
    "        to be allocated for collective communication, where\n",
    "        before a block can be found, the possible 'shapes' \n",
    "        of block (given a number of servers required) need to be known.\n",
    "        '''\n",
    "        pairs = []\n",
    "\n",
    "        for i in range(1,n+1):\n",
    "\n",
    "            if n % i == 0:\n",
    "                pairs.append((int(n/i),i))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_block(ramp,block,op_size,mode):\n",
    "        '''\n",
    "        Iterate through each server in a block.\n",
    "\n",
    "        Return True if each server has enough resource\n",
    "        to support the requirement, False otherwise\n",
    "        '''\n",
    "        if block == []:\n",
    "            return False\n",
    "        for server in block:\n",
    "            if mode == 'sub':\n",
    "                if ramp[server]['mem'] < op_size:\n",
    "                    return False\n",
    "            if mode == 'meta':\n",
    "                if ramp[server]['ops'] != []:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_meta_block(C,R,S,ramp_shape,origin=(0,0,0)):\n",
    "        '''\n",
    "        Given an origin (i.e. a starting server in RAMP), \n",
    "        returns a set of servers that are part of a 'shape'\n",
    "        that is centred at that origin. \n",
    "\n",
    "        NOTE: A simplification here is that in the case of\n",
    "        one server-per-rack, servers of the same id are checked.\n",
    "        If this is not done, then a fully exhaustive search has to\n",
    "        be implemented which is infeasible (i.e. for group of racks\n",
    "        with one server each, every possible combination of servers\n",
    "        across racks has to be checked...).\n",
    "        '''\n",
    "        block = []\n",
    "        i,j,k = origin\n",
    "\n",
    "        for c in range(C):\n",
    "            for r in range(R):\n",
    "                for s in range(S):\n",
    "                    block.append(((i+c)%ramp_shape[0],(j+r)%ramp_shape[1],(k+s)%ramp_shape[2]))\n",
    "\n",
    "        return block\n",
    "    \n",
    "    @staticmethod\n",
    "    def _ff_meta_block(block_shapes,ramp_shape,ramp,mode,op_size=None,meta_block_origin=(0,0,0)):\n",
    "        '''\n",
    "        For each block shape, create the block.\n",
    "\n",
    "        When a block has been created, check it using\n",
    "        check_block to see if it's OK for allocation\n",
    "        w.r.t. server-resources. If OK, return the block.\n",
    "        Otherwise, return None.\n",
    "\n",
    "        The seach will start at the point 'meta_block_origin' which\n",
    "        should be the upper left hand corner of the block that has\n",
    "        been allocated to the job. This function will then find a\n",
    "        sub-block that can be given to a particular (partitioned) \n",
    "        op in that job.\n",
    "\n",
    "        NOTE: currently this code is using (0,0,0) as the upper left\n",
    "        hand corner of the meta-block. This will have to be an argument\n",
    "        as different block throughout allocation will have different \n",
    "        positions in the RAMP network.\n",
    "\n",
    "        NOTE: all functions here are only working in multiples of 2.\n",
    "        this is because accounting for odd server-numbers means extra \n",
    "        conditions to be handled when distributing over multiple racks\n",
    "        because of the RAMP symmetry rules for collectives.\n",
    "        '''\n",
    "\n",
    "        orgn_c, orgn_r, orgn_s = meta_block_origin\n",
    "        for shape in block_shapes:\n",
    "            #get the acceptable search ranges given how big the meta-block is\n",
    "            #all shapes will already be maximum the size of the meta-block, so this doesn't have to be checked\n",
    "            I = ramp_shape[0]-shape[0]+1\n",
    "            J = ramp_shape[1]-shape[1]+1\n",
    "            K = ramp_shape[2]-shape[2]+1\n",
    "            if I <= 0 or J <= 0 or K <= 0:\n",
    "                continue\n",
    "            else:\n",
    "                #get the size of the shape in each RAMP dimension\n",
    "                C,R,S = shape\n",
    "                for i in range(ramp_shape[0]):\n",
    "                    # j = i\n",
    "                    for j in range(ramp_shape[1]):\n",
    "                        for k in range(ramp_shape[2]):\n",
    "                            block = GreedyBlockAllocator._get_meta_block(C,R,S,ramp_shape,origin=((orgn_c+i),(orgn_r+j),(orgn_s+k)))\n",
    "\n",
    "                            if GreedyBlockAllocator._check_block(ramp,block,op_size,mode):\n",
    "                                if mode == 'sub':\n",
    "                                    return block\n",
    "                                if mode == 'meta':\n",
    "                                    return (block, shape, (orgn_c+i,orgn_r+j,orgn_s+k))\n",
    "\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _ff_block(block_shapes,meta_shape,ramp_shape,ramp,mode,op_size=None,meta_block_origin=(0,0,0)):\n",
    "        '''\n",
    "        For each block shape, create the block.\n",
    "\n",
    "        When a block has been created, check it using\n",
    "        check_block to see if it's OK for allocation\n",
    "        w.r.t. server-resources. If OK, return the block.\n",
    "        Otherwise, return None.\n",
    "\n",
    "        The seach will start at the point 'meta_block_origin' which\n",
    "        should be the upper left hand corner of the block that has\n",
    "        been allocated to the job. This function will then find a\n",
    "        sub-block that can be given to a particular (partitioned) \n",
    "        op in that job.\n",
    "\n",
    "        NOTE: currently this code is using (0,0,0) as the upper left\n",
    "        hand corner of the meta-block. This will have to be an argument\n",
    "        as different block throughout allocation will have different \n",
    "        positions in the RAMP network.\n",
    "\n",
    "        NOTE: all functions here are only working in multiples of 2.\n",
    "        this is because accounting for odd server-numbers means extra \n",
    "        conditions to be handled when distributing over multiple racks\n",
    "        because of the RAMP symmetry rules for collectives.\n",
    "        '''\n",
    "        # print(f'find_sub_block: {ramp.keys()}')\n",
    "        orgn_c, orgn_r, orgn_s = meta_block_origin\n",
    "        for shape in block_shapes:\n",
    "            #get the acceptable search ranges given how big the meta-block is\n",
    "            #all shapes will already be maximum the size of the meta-block, so this doesn't have to be checked\n",
    "            I = (meta_shape[0]-shape[0])+1\n",
    "            J = (meta_shape[1]-shape[1])+1\n",
    "            K = (meta_shape[2]-shape[2])+1\n",
    "            if I <= 0 or J <= 0 or K <= 0:\n",
    "                continue\n",
    "            else:\n",
    "                #get the size of the shape in each RAMP dimension\n",
    "                C,R,S = shape\n",
    "\n",
    "                for i in range(I):\n",
    "                    # j = i\n",
    "                    for j in range(J):\n",
    "                        for k in range(K):\n",
    "                            #get a block of shape (C,R,S) at origin (i,j,k)\n",
    "                            block = GreedyBlockAllocator._get_block(C,R,S,ramp_shape,origin=(orgn_c+i,orgn_r+j,orgn_s+k))\n",
    "                            if GreedyBlockAllocator._check_block(ramp,block,op_size,mode):\n",
    "                                if mode == 'sub':\n",
    "                                    return block\n",
    "                                if mode == 'meta':\n",
    "                                    return (block, shape, (orgn_c+i,orgn_r+j,orgn_s+k))\n",
    "\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_block(C,R,S,ramp_shape,origin=(0,0,0)):\n",
    "        '''\n",
    "        Given an origin (i.e. a starting server in RAMP), \n",
    "        returns a set of servers that are part of a 'shape'\n",
    "        that is centred at that origin. \n",
    "\n",
    "        NOTE: A simplification here is that in the case of\n",
    "        one server-per-rack, servers of the same id are checked.\n",
    "        If this is not done, then a fully exhaustive search has to\n",
    "        be implemented which is infeasible (i.e. for group of racks\n",
    "        with one server each, every possible combination of servers\n",
    "        across racks has to be checked...).\n",
    "        '''\n",
    "        block = []\n",
    "        i,j,k = origin\n",
    "\n",
    "        if S == -1:\n",
    "            for n in range(C):\n",
    "                block.append(((i+n)%(ramp_shape[0]+1),(j+n)%(ramp_shape[1]+1),k))\n",
    "        else:\n",
    "            for c in range(C):\n",
    "                for r in range(R):\n",
    "                    for s in range(S):\n",
    "                        block.append(((i+c)%(ramp_shape[0]),(j+r)%(ramp_shape[1]),(k+s)%(ramp_shape[2])))\n",
    "        return block\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_meta_block(ramp_topology,ramp_shape,meta_block_shape):\n",
    "        '''\n",
    "        Take an input of a RAMP topology and a required number of servers and \n",
    "        return a block of servers that is symmetric w.r.t. collective and where\n",
    "        every server is currently empty.\n",
    "        \n",
    "        Returned values should be a set of server IDs, the shape and the 'origin'\n",
    "        server in that set (i.e. the effective top left hand corner).\n",
    "        \n",
    "        meta_block_info = (meta_block, meta_block_shape, meta_block_origin) or None if empty.\n",
    "        '''\n",
    "        meta_block_info = GreedyBlockAllocator._ff_meta_block([meta_block_shape],ramp_shape,ramp_topology,'meta')\n",
    "        # print(f'find_meta_block: {ramp_topology.keys()}')\n",
    "        return meta_block_info \n",
    "    \n",
    "    @staticmethod\n",
    "    def find_sub_block(ramp_topology,ramp_shape,meta_block_shape,meta_block_origin,num_servers,op_size):\n",
    "        \n",
    "        pairs = GreedyBlockAllocator._get_factor_pairs(num_servers)\n",
    "        block_shapes = GreedyBlockAllocator._get_block_shapes(pairs,meta_block_shape)\n",
    "        #if no possible shapes try rack and CG distributed\n",
    "        block_shapes += [(num_servers,num_servers,-1),(num_servers,1,1)]\n",
    "        # print(f'find_sub_block: {ramp_topology.keys()}')\n",
    "        block = GreedyBlockAllocator._ff_block(block_shapes,meta_block_shape,ramp_shape,ramp_topology,'sub',op_size=op_size,meta_block_origin=meta_block_origin)\n",
    "        return block\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_parents_and_children(original_graph):\n",
    "        \n",
    "        parents = {}\n",
    "        children = {}\n",
    "        tmp_graph = original_graph.copy()\n",
    "        \n",
    "        for node in original_graph.nodes():\n",
    "                in_nodes = [in_edge[0] for in_edge in original_graph.in_edges(node)]\n",
    "                parents[node] = in_nodes\n",
    "                \n",
    "                out_nodes = [out_edge[1] for out_edge in original_graph.out_edges(node)]\n",
    "                children[node] = out_nodes\n",
    "                \n",
    "        return parents, children\n",
    "    \n",
    "    @staticmethod\n",
    "    def _topo_sort(parents,children):\n",
    "                \n",
    "        sequence = []\n",
    "        \n",
    "        queue = deque()\n",
    "        \n",
    "        for node in parents.keys():\n",
    "            if parents[node] == []:\n",
    "                queue.append(node)\n",
    "                sequence.append(node)\n",
    "                \n",
    "        while len(queue) > 0:\n",
    "            \n",
    "            node = queue.popleft()\n",
    "            for child in children[node]:\n",
    "                parents[child].remove(node)\n",
    "                if parents[child] == []:\n",
    "                    queue.append(child)\n",
    "                    sequence.append(child)\n",
    "                    \n",
    "        return sequence\n",
    "        \n",
    "    @staticmethod\n",
    "    def _regular_collective_placement(ramp,ramp_shape,job_graph,op,split,meta_block_info,op_server_info):\n",
    "        '''\n",
    "        This function allocates a split op to a set of servers in a meta-block nd returns a dictionary \n",
    "        of which sub-ops are allocated to which servers, and another dictionary indicating across which\n",
    "        servers are each op distributed (this does not refer to specific sub-ops and is used so that \n",
    "        the parent-checking allocation method can be implemented. Sub-ops are allocated one op per\n",
    "        server.\n",
    "        \n",
    "        Args:\n",
    "            ramp (dict): RAMP 'topology' as a dict like {(a,b,c):attributes}\n",
    "            job_graph (nx.DiGraph): un-partitioned and non-mirrored computational graph of a job\n",
    "            op (str): name of an op (e.g. '11')\n",
    "            split (int): how many times this op should be split\n",
    "            meta_block_info (tuple): return value of GreedyBlockAllocator.find_meta_block\n",
    "            op_server_info (dict): which ops are allocated across which servers (e.g. {'11':[(0,0,1),(0,0,2)]})\n",
    "                                    \n",
    "        '''\n",
    "        # print(f'_regular_collective_placement: {ramp.keys()}')\n",
    "        num_nodes = len(job_graph.nodes())\n",
    "        meta_block,meta_block_shape,meta_block_origin = meta_block_info\n",
    "\n",
    "        num_servers = split #NOTE: ops_per_server should never be more than splits. Needs to be ensured somewhere. EDIT: since putting multiple sub-ops on the same server is trivial (should then just partition it by a smaller amount) we will keep 1 op-per-server for now.\n",
    "        if num_servers > len(meta_block): #if there are fewer servers in the meta-block than asked for, no allocation\n",
    "            return None\n",
    "\n",
    "        op_size = job_graph.nodes[op]['activation']/split\n",
    "        meta_block = {server:ramp[server] for server in meta_block}\n",
    "\n",
    "        block = GreedyBlockAllocator.find_sub_block(ramp,ramp_shape,meta_block_shape,meta_block_origin,num_servers,op_size)\n",
    "\n",
    "        if not block: #if no block can be found (memory errors) then return None\n",
    "            return None\n",
    "        for j in range(len(block)):\n",
    "            ramp[block[j]]['mem'] -= op_size\n",
    "            if split > 1:\n",
    "                ramp[block[j]]['ops'].append(op+chr(97+j))\n",
    "                ramp[block[j]]['ops'].append(str(int(op)+num_nodes)+chr(97+j))\n",
    "            else:\n",
    "                ramp[block[j]]['ops'].append(op)\n",
    "                ramp[block[j]]['ops'].append(str(int(op)+num_nodes))\n",
    "            op_server_info[op].append(block[j])\n",
    "\n",
    "        #if allocation was feasible, return the updated (i.e. with server-memory reduced) RAMP topology for further allocations\n",
    "        return ramp, op_server_info\n",
    "    \n",
    "    @staticmethod\n",
    "    def _parent_collective_placement(ramp,job_graph,op,split,meta_block_info,parents,op_server_info):\n",
    "        '''\n",
    "        Similar to _regular_collective_placement except it checks if the given op's\n",
    "        parents are allocated already somewhere. It then checks if it can allocate\n",
    "        all the sub-ops evenly across the set of servers associated with (one of) the\n",
    "        parent(s). If there are more child sub-ops than servers used by the parent, then \n",
    "        they are packed in evenly across the servers. \n",
    "        \n",
    "        Args:\n",
    "            ramp (dict): RAMP 'topology' as a dict like {(a,b,c):attributes}\n",
    "            job_graph (nx.DiGraph): un-partitioned and non-mirrored computational graph of a job\n",
    "            op (str): name of an op (e.g. '11')\n",
    "            split (int): how many times this op should be split\n",
    "            parents (dict): information relating ops in job_graph to their parents (i.e. {'14':['12','13']} if 12 and 13 are parents of 14).\n",
    "            op_server_info (dict): which ops are allocated across which servers (e.g. {'11':[(0,0,1),(0,0,2)]})\n",
    "        '''\n",
    "        # #can't split an un-split node over it's split parents\n",
    "        # if split == 1:\n",
    "        #     return None\n",
    "        \n",
    "        op_requirement = job_graph.nodes[op]['activation']\n",
    "        num_nodes = len(job_graph.nodes())\n",
    "        \n",
    "        #get sets of servers corresponding to each parent\n",
    "        parents_servers = []\n",
    "        for parent in parents[op]:\n",
    "            if set(op_server_info[parent]).issubset(meta_block_info[0]):\n",
    "                parents_servers.append(op_server_info[parent])\n",
    "\n",
    "        #for each set of servers\n",
    "        for servers in parents_servers:\n",
    "            if split < len(servers):\n",
    "                continue\n",
    "            else:\n",
    "                #check if ops can fit evenly across the servers\n",
    "                available_resource = sum([ramp[server]['mem'] for server in servers])\n",
    "                if available_resource >= op_requirement:\n",
    "                    i = 0\n",
    "                    while i < split:\n",
    "                        for server in servers:\n",
    "                            ramp[server]['mem'] -= op_requirement/split\n",
    "                            # if split > 1:\n",
    "                            ramp[server]['ops'].append(op+chr(97+i))\n",
    "                            ramp[server]['ops'].append(str(int(op)+num_nodes)+chr(97+i))\n",
    "                            op_server_info[op].append(server)\n",
    "                            i += 1\n",
    "                    return ramp, op_server_info\n",
    "\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def allocate(ramp,ramp_shape,job_graph,sequence,splits,meta_block_info,parents,op_server_info):\n",
    "        \n",
    "        # op_server_info = {op:[] for op in job_graph.nodes()}\n",
    "        # print(f'allocate: {ramp.keys()}')\n",
    "        for i in range(len(sequence)):\n",
    "            op = sequence[i]\n",
    "            split = splits[i]\n",
    "            \n",
    "            #try allocating on same servers as parents\n",
    "            alloc = GreedyBlockAllocator._parent_collective_placement(ramp,job_graph,op,split,meta_block_info,parents,op_server_info)\n",
    "            \n",
    "            #if this doesn't work, try allocating somewhere else\n",
    "            if not alloc:\n",
    "                # print('no parents - regular allocation')\n",
    "                alloc = GreedyBlockAllocator._regular_collective_placement(ramp,ramp_shape,job_graph,op,split,meta_block_info,op_server_info)\n",
    "                \n",
    "            #if that didn't work either, return None (this means the allocation has failed)\n",
    "            if not alloc:\n",
    "                return alloc\n",
    "            \n",
    "            #if either of the allocation attempts worked, update ramp and op_server_info and go onto the next op\n",
    "            else:\n",
    "                ramp, op_server_info = alloc[0], alloc[1]\n",
    "                \n",
    "        return ramp, op_server_info\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_allocation_preamble(job_graph,mp_split_ids,mp_splits):\n",
    "        \n",
    "        parents, children = GreedyBlockAllocator._get_parents_and_children(job_graph)\n",
    "        sequence = GreedyBlockAllocator._topo_sort(deepcopy(parents),deepcopy(children))\n",
    "        op_server_info = {op:[] for op in job_graph.nodes()}\n",
    "        splits = []\n",
    "        for s in sequence:\n",
    "            if int(s) in mp_split_ids:\n",
    "                idx = mp_split_ids.index(int(s))\n",
    "                splits.append(mp_splits[idx])\n",
    "            else:\n",
    "                splits.append(1)\n",
    "        return sequence, splits, op_server_info, parents, children\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_block_shapes(pairs,ramp_shape):\n",
    "        '''\n",
    "        Given a set of factor-pairs (corresponding to a \n",
    "        particular number of servers that have to be \n",
    "        allocated into a block) and the size of the full \n",
    "        ramp meta-block that is to have the job packed\n",
    "        into it, return the set of acceptable 'shapes'\n",
    "        of block that can fit within the size of this \n",
    "        meta-block.\n",
    "        '''\n",
    "        blocks = []\n",
    "        # print(ramp_shape)\n",
    "        for pair in pairs:\n",
    "            if pair[0] > ramp_shape[0] or pair[0] > ramp_shape[1] or pair[1] > ramp_shape[2]:\n",
    "                continue\n",
    "            else:\n",
    "                # blocks.append((pair[0],pair[0],pair[1]))\n",
    "                blocks.append((pair[0],1,pair[1]))\n",
    "                blocks.append((pair[0],pair[1],1))\n",
    "        \n",
    "        return blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f53a52-b1f7-4cbf-b3d1-ab00cb13648b",
   "metadata": {},
   "source": [
    "# Partition: Partitioning DAGs from pipedream\n",
    "\n",
    "## General description\n",
    "\n",
    "* The ```Partition``` class allows for graphs to be read from pipedream files and re-drawn as partitioned (model and/or data) graphs\n",
    "* Arguments (__init__):\n",
    "    * ```graph``` (nx.DiGraph): Represents a DAG. These must be from pipedream file and read specifically using the Partition.pipedream_graph static-method. \n",
    "    * ```mp_split_ids``` (list[int]): A list of integers corresponding to the IDs of ops in the graph (which are also integer numbered) where the included IDs refer to the ops in the DAG that are going to be (model) partitioned. \n",
    "    * ```mp_splits``` (list[int]): A list of integers corresponding to the number of times an op will be split, where the indices of these values correspond to the indices in ```mp_split_ids```\n",
    "    * dp_splits (int, default=0): The number of times the model should be copied (i.e. for data-partitioning). This should not be used for now. \n",
    "* High level implementation details (excluding data-partitioning for now):\n",
    "    1. A mirror version of the graph is created and attached to the end of the original graph. \n",
    "        * This is so that the backward pass is explicitly represented in the graph when followed 'down' from source to sink via the dependencies.\n",
    "    2. Each node that is to be partitioned is split. \n",
    "        * The naming convention followed is that node IDs are strings of integers and post-partitioned sub-ops are appended with letters to distinguish them from each other.\n",
    "            * e.g. node '1' split 2 times will become {'1a','1b'}\n",
    "        * This process is applied simultaneously to both forward and backward pass 'versions' of each op. \n",
    "            * For the backward pass 'versions' of the partitioned op, an additional step is that an all to all connectivity structure is established between them. This is because on a backward pass, all sub-ops must coordinate their gradients etc with each other so collective communication is required, which in this case also requires all-to-all.\n",
    "    \n",
    "* Additional notes:\n",
    "    * Initialising a ```Partition``` object will return a single new graph which has the above process applied to it. \n",
    "    * A ```Partition``` object also has a ```self.original_graph``` attribute, which is just the graph as returned from ```Partition.pipedream_graph``` (i.e. the un-partitioned, un-mirrored version).  \n",
    "    \n",
    "## Pseudo-code:\n",
    "\n",
    "```python\n",
    "# load the networkx representation of the pipedream graph\n",
    "job_graph = Partition.pipedream_graph(PATH_TO_PIPEDREAM_FILE)\n",
    "\n",
    "# op '1' will be split into 2 sub-ops\n",
    "mp_split_ids, mp_splits = [1],[2]\n",
    "\n",
    "# get the partitioned graph\n",
    "partition_job_graph = Partition(job_graph,[6],[2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1508e76-e0fa-49a3-92b7-47d02f037d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import networkx as nx\n",
    "\n",
    "class Partition:\n",
    "    \n",
    "    def __init__(self,graph,mp_split_ids,mp_splits,dp_splits=0):\n",
    "        '''\n",
    "        high level implementation explanation:\n",
    "            1. take the forward only graph and create a forward + backward version of it\n",
    "            2. data partition the model dp_splits times\n",
    "                a. all output nodes are currently connected to each other (this needs to be reconsidered)\n",
    "                b. 'data' edge attribute is set to the size of 'activation' on the src node\n",
    "            3. model partition the nodes in mp_split_ids (list) to subnodes equal to same-index values in mp_splits (list)\n",
    "                a. for each sub-node for node mp_split_ids[i], the incoming 'data' attribute on the incoming edge is set to 'data'/mp_splits[i]. This is done since when ops are split, the amount of information going into and out of each op (into the next op) is split.\n",
    "        '''\n",
    "        \n",
    "        self.mp_split_ids = mp_split_ids\n",
    "        self.mp_splits = mp_splits\n",
    "        self.dp_splits = dp_splits\n",
    "        self.original_graph = graph\n",
    "        backward = self.mirror_graph(graph)\n",
    "        self.graph = self.combine_graphs(graph,backward)\n",
    "        \n",
    "        self.data_split_node()\n",
    "        self.model_split_node()\n",
    "        \n",
    "    def mirror_graph(self,graph):\n",
    "\n",
    "        forward_graph = graph.copy()\n",
    "        n = len(forward_graph.nodes())\n",
    "        nodes = []\n",
    "\n",
    "        for i in range(1,len(forward_graph.nodes())+1):\n",
    "\n",
    "            nodes.append((str(2*n-(i-1)),forward_graph.nodes[str(i)]))\n",
    "\n",
    "        edges = list(forward_graph.edges())\n",
    "\n",
    "        for i in range(len(edges)):\n",
    "\n",
    "            edges[i] = (str(2*n-(int(edges[i][1])-1)),str(2*n-(int(edges[i][0])-1)))\n",
    "\n",
    "        backward_graph = nx.DiGraph()\n",
    "        backward_graph.add_nodes_from(nodes)\n",
    "        backward_graph.add_edges_from(edges)\n",
    "\n",
    "        return backward_graph\n",
    "\n",
    "    def combine_graphs(self,forward,backward):\n",
    "\n",
    "        forward_nodes = list(forward.nodes())\n",
    "        backward_nodes = list(backward.nodes())\n",
    "\n",
    "        for i in list(forward.nodes()):\n",
    "            forward.nodes[str(i)]['compute'] = forward.nodes[str(i)]['forward']\n",
    "            del forward.nodes[str(i)]['forward']\n",
    "            del forward.nodes[str(i)]['backward']\n",
    "\n",
    "        for i in list(backward.nodes()):\n",
    "            backward.nodes[str(i)]['compute'] = backward.nodes[str(i)]['backward']\n",
    "            del backward.nodes[str(i)]['forward']\n",
    "            del backward.nodes[str(i)]['backward']\n",
    "\n",
    "        join_0, join_1 = max([int(nd) for nd in forward_nodes]),min([int(nd) for nd in backward_nodes])\n",
    "        joined = nx.union(forward,backward)\n",
    "\n",
    "        joined.add_edge(str(join_0),str(join_1))\n",
    "\n",
    "        for edge in joined.edges():\n",
    "            edge = tuple(edge)\n",
    "            joined.edges[edge[0],edge[1]]['communication'] = joined.nodes[edge[0]]['activation']\n",
    "\n",
    "        return joined\n",
    "    \n",
    "    def data_split_node(self):\n",
    "\n",
    "        '''goal: \n",
    "        - create n_splits copies of a given graph with no repeated node ids\n",
    "        - do this before node splits, so that the same set of nodes splits can be applied to all of these graphs at once\n",
    "        '''\n",
    "\n",
    "        og_nodes = [int(node) for node in list(self.graph.nodes())]\n",
    "        og_edges = [(int(edge[0]),int(edge[1])) for edge in self.graph.edges()]\n",
    "\n",
    "        self.highest_og_node = max(og_nodes)\n",
    "\n",
    "        all_highest_nodes = []\n",
    "\n",
    "        #currently assuming that data splitting doesn't require model size to change in any way\n",
    "        new_features = [self.graph.nodes[str(node)] for node in og_nodes]\n",
    "\n",
    "        new_graph = nx.DiGraph()\n",
    "\n",
    "        for i in range(self.dp_splits+1):\n",
    "            id_shift = i*self.highest_og_node\n",
    "            new_nodes = [(str(og_nodes[j]+id_shift),new_features[j]) for j in range(len(og_nodes))]\n",
    "            new_edges = [(str(edge[0]+id_shift),str(edge[1]+id_shift)) for edge in og_edges]\n",
    "\n",
    "            all_highest_nodes.append(self.highest_og_node+id_shift)\n",
    "\n",
    "            new_graph.add_nodes_from(new_nodes)\n",
    "            new_graph.add_edges_from(new_edges)\n",
    "\n",
    "        edge_features = {}\n",
    "        for edge in new_graph.edges():\n",
    "            edge_features[edge] = {'data':new_graph.nodes[edge[0]]['activation']}\n",
    "            \n",
    "        nx.set_edge_attributes(new_graph,edge_features)\n",
    "        \n",
    "        self.graph = new_graph\n",
    "\n",
    "    def model_split_node(self):\n",
    "\n",
    "        in_edge_features = {}\n",
    "        out_edge_features = {}\n",
    "        \n",
    "        #do for each op that should be split\n",
    "        for i in range(len(self.mp_split_ids)):\n",
    "\n",
    "            n_splits = self.mp_splits[i]\n",
    "            #do across each data-parallel split graph\n",
    "            for j in range(self.dp_splits+1):\n",
    "                node_ids = [\n",
    "                            str(self.mp_split_ids[i]+(j*self.highest_og_node)),\n",
    "                            str(self.highest_og_node - (self.mp_split_ids[i]-1)+(j*self.highest_og_node))\n",
    "                           ]\n",
    "                            \n",
    "                for k in range(len(node_ids)):\n",
    "                    \n",
    "                    node_id = node_ids[k]\n",
    "                    \n",
    "                    in_edges = [edge[0] for edge in self.graph.in_edges(node_id)]\n",
    "                    out_edges = [edge[1] for edge in self.graph.out_edges(node_id)]\n",
    "\n",
    "                    new_feature = {'activation':self.graph.nodes[node_id]['activation']/n_splits,\n",
    "                                   'parameter':self.graph.nodes[node_id]['parameter']/n_splits,\n",
    "                                   'compute':self.graph.nodes[node_id]['compute']/n_splits,\n",
    "                                   'type':self.graph.nodes[node_id]['type']\n",
    "                                  }\n",
    "\n",
    "                    nodes = [('{}{}'.format(node_id,chr(97+k)),new_feature) for k in range(n_splits)]\n",
    "\n",
    "                    new_edges = []\n",
    "                    for node in nodes:\n",
    "                        new_edges += [(edge,node[0]) for edge in in_edges]\n",
    "                        new_edges += [(node[0],edge) for edge in out_edges]\n",
    "\n",
    "                        #account for edge features for incoming edges to split nodes\n",
    "                        for edge in in_edges:\n",
    "                            in_edge_features[(edge,node[0])] = {'data':self.graph.nodes[edge]['activation']/len(nodes)}\n",
    "                            \n",
    "                    #extra step if on the back-prop for collective between all sub-ops to sync weights\n",
    "                    if k == 1:\n",
    "                        for l in range(len(nodes)):\n",
    "                            for m in range(len(nodes)):\n",
    "                                if l == m:\n",
    "                                    continue\n",
    "                                new_edges += [(nodes[l][0],nodes[m][0])]\n",
    "\n",
    "                    self.graph.remove_node(node_id)\n",
    "                    self.graph.add_nodes_from(nodes)\n",
    "                    self.graph.add_edges_from(new_edges)\n",
    "                \n",
    "        nx.set_edge_attributes(self.graph,in_edge_features)\n",
    "      \n",
    "    @staticmethod\n",
    "    def pipedream_graph(file_path:str):\n",
    "\n",
    "        graph = nx.DiGraph()\n",
    "\n",
    "        f = list(open(file_path,'r'))\n",
    "\n",
    "        nodes = []\n",
    "        edges = []\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            line = line.split(' -- ')\n",
    "            for idx, el in enumerate(line):\n",
    "                line[idx] = el.split('\\t')[-1]\n",
    "\n",
    "            #if this line represents a node\n",
    "            if len(line) > 2:\n",
    "\n",
    "                node_features = {}\n",
    "\n",
    "                #get node id\n",
    "                node_id = str(int(line[0][4:]))\n",
    "\n",
    "                #get op id\n",
    "                op_id = line[1].split('(')[0]\n",
    "\n",
    "                node_features['type'] = op_id\n",
    "\n",
    "                #get op details\n",
    "                op_details = str(line[1].split(op_id)[1][1:-1]).split(', ')\n",
    "\n",
    "\n",
    "                #get compute time and memory details\n",
    "                comp_and_memory = line[2].split(', ')\n",
    "                comp_memory_feats = ['forward','backward','activation','parameter']\n",
    "                for i in range(len(comp_memory_feats)):\n",
    "                    node_features[comp_memory_feats[i]] = float(comp_and_memory[i].split('=')[1].replace('\\n',''))\n",
    "\n",
    "                nodes.append((node_id,node_features))\n",
    "            else:\n",
    "\n",
    "                src = int(line[0][4:])\n",
    "                dst = int(line[1][4:])\n",
    "\n",
    "                edges.append((str(src),str(dst))) #assume only 1 data channel for now\n",
    "\n",
    "        #get initial graph\n",
    "        graph.add_nodes_from(nodes)\n",
    "        graph.add_edges_from(edges)\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7004743-9978-40b4-865a-d0fb27b8bb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta block found\n",
      "[x][x][ ][ ]  [x][x][ ][ ]  \n",
      "[x][x][ ][ ]  [x][x][ ][ ]  \n",
      "[ ][ ][ ][ ]  [ ][ ][ ][ ]  \n",
      "[ ][ ][ ][ ]  [ ][ ][ ][ ]  \n",
      "\n",
      "allocation found:\n",
      "op:1 - servers:[(0, 0, 0), (1, 0, 0)]\n",
      "op:2 - servers:[(0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# required inputs (handled outside of allocation logic)\n",
    "job_graph = Partition.pipedream_graph('/scratch/datasets/ddls/jobs/pipedream_graphs/image_classification/profiles/alexnet/graph.txt') #REPLACE_THIS_WITH_A_REAL_PATH_TO_A_PIPEDREAM_FILE)\n",
    "\n",
    "# Pipedream ops have large numbers. For debugging etc it's easier to have unit sized resources so this is forced here. Not necessary in real implementation.\n",
    "for node in job_graph.nodes():\n",
    "    job_graph.nodes[node]['activation'] = 2.0\n",
    "\n",
    "# specify which ops are going to be split and how many times\n",
    "mp_split_ids, mp_splits = [1,2],[2,4]\n",
    "partition_job_graph = Partition(job_graph,[6],[2])\n",
    "\n",
    "# specify size of the meta-block to be used for this job.\n",
    "meta_shape = (2,2,2)\n",
    "\n",
    "# specify the RAMP topology (this is using a simple dict-based method, can be implemented in some other way in theory). \n",
    "ramp_shape = (4,2,4)\n",
    "ramp_topology = dummy_ramp(ramp_shape)\n",
    "# print(f'script: {ramp_topology.keys()}')\n",
    "# get the allocation 'preamble' (job info used for the heuristic allocator)\n",
    "sequence, splits, op_server_info, parents, children = GreedyBlockAllocator.get_allocation_preamble(job_graph,mp_split_ids,mp_splits)\n",
    "\n",
    "# Only use the first 2 ops in the graph. This is also for debugging purposes (using very small RAMPs is easy to visualise etc). These two lines would not be used in real thing. \n",
    "sequence = sequence[:2]\n",
    "splits = splits[:2]\n",
    "\n",
    "# get a meta-block of a particular shape which the heuristic allocator will try to pack the job fully into\n",
    "meta_block_info = GreedyBlockAllocator.find_meta_block(ramp_topology,ramp_shape,meta_shape)\n",
    "\n",
    "# if a meta-block was successfully found...\n",
    "if meta_block_info:\n",
    "    print('meta block found')\n",
    "    # simple visualisation of the RAMP topology, where servers marked 'x' are those included in the meta-block (i.e. those reserved for this job). \n",
    "    image = ''\n",
    "    for c in range(ramp_shape[0]):\n",
    "        for r in range(ramp_shape[1]):\n",
    "            for s in range(ramp_shape[2]):\n",
    "                if (c,r,s) in meta_block_info[0]:\n",
    "                    image += '[x]'\n",
    "                else:\n",
    "                    image += '[ ]'\n",
    "            image += '  '\n",
    "        image += '\\n'\n",
    "    print(image)\n",
    "\n",
    "    # try to allocate the job\n",
    "    allocated = GreedyBlockAllocator.allocate(ramp_topology,ramp_shape,job_graph,sequence,splits,meta_block_info,parents,op_server_info)\n",
    "    # if the allocation was successful...\n",
    "    if allocated:\n",
    "        # print out which servers were allocated to which ops\n",
    "        print(f'allocation found:')\n",
    "        for k,v in allocated[1].items():\n",
    "            if v:\n",
    "                print(f'op:{k} - servers:{v}')\n",
    "        # update the topology and op-server info for use in the next job allocation\n",
    "        ramp_topology, op_server_info = allocated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150abc31-2b89-4590-9710-3f54757a3e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c8ef9-2f6b-4fcf-828c-b9e2a605f40f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a83d55-2b4d-4ba6-824d-bc2bdf1a3f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28af63-c649-425e-9fac-cc8b37ed73bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b765194-3300-4290-b0df-94e0245925a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddls",
   "language": "python",
   "name": "ddls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
