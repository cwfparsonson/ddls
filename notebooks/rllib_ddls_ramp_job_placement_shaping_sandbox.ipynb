{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b17696a-ae1b-4912-aaa9-3498cad5ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 12:56:17,686\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8269\u001b[39m\u001b[22m\n",
      "2022-06-23 12:56:24,346\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8269\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from ddls.devices.processors.gpus.A100 import A100\n",
    "from ddls.plotting.plotting import plot_computation_graph\n",
    "from ddls.environments.ramp_job_placement_shaping.ramp_job_placement_shaping_environment import RampJobPlacementShapingEnvironment\n",
    "from ddls.demands.jobs.job import Job\n",
    "from ddls.distributions.uniform import Uniform\n",
    "from ddls.utils import seed_stochastic_modules_globally\n",
    "\n",
    "from ddls.ml_models.policies import GNNPolicy\n",
    "from ddls.plotting.plotting import plot_line\n",
    "\n",
    "from ddls.loops.rllib_epoch_loop import RLlibEpochLoop\n",
    "from ddls.launchers.launcher import Launcher\n",
    "\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54a2a362-9f85-47a1-a822-0de2cd86d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "register_env('ramp_job_placement_shaping_environment', lambda env_config: RampJobPlacementShapingEnvironment(**env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3780635a-1a7a-4237-976e-5e68d8c7da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "ModelCatalog.register_custom_model('my_model', GNNPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73742a94-88a3-4602-bdc8-9dbf344d129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "node_config = {'type_1':\n",
    "                  {\n",
    "                      'num_nodes': 16, # 8 16\n",
    "                      'workers_config': \n",
    "                          [\n",
    "                              {\n",
    "                               'num_workers': 1, # NEED 1 WORKER PER SERVER FOR RAMP\n",
    "                               'worker': A100\n",
    "                              }\n",
    "                          ]\n",
    "                  }\n",
    "              }\n",
    "\n",
    "topology_config = {'type':\n",
    "                      'ramp',\n",
    "                   'kwargs':\n",
    "                      {\n",
    "                          'num_communication_groups': 2,\n",
    "                          'num_racks_per_communication_group': 2,\n",
    "                          'num_servers_per_rack': 4, # 2 4\n",
    "                          'num_channels': 2\n",
    "                      }\n",
    "                  }\n",
    "\n",
    "jobs_config = {'path_to_files': '/scratch/datasets/ddls/jobs/pipedream_graphs/image_classification/profiles/alexnet/',\n",
    "               'job_interarrival_time_dist': Uniform(min_val=1, max_val=1000),\n",
    "               'max_files': 20,\n",
    "               'job_sampling_mode': 'remove',\n",
    "               # 'job_sampling_mode': 'remove_and_replace',\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a7a537-89e0-41a5-b12b-4578d90def24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'env': 'ramp_job_placement_shaping_environment', 'env_config': {'node_config': {'type_1': {'num_nodes': 16, 'workers_config': [{'num_workers': 1, 'worker': <class 'ddls.devices.processors.gpus.A100.A100'>}]}}, 'topology_config': {'type': 'ramp', 'kwargs': {'num_communication_groups': 2, 'num_racks_per_communication_group': 2, 'num_servers_per_rack': 4, 'num_channels': 2}}, 'jobs_config': {'path_to_files': '/scratch/datasets/ddls/jobs/pipedream_graphs/image_classification/profiles/alexnet/', 'job_interarrival_time_dist': <ddls.distributions.uniform.Uniform object at 0x7efb574474f0>, 'max_files': 20, 'job_sampling_mode': 'remove'}, 'max_simulation_run_time': inf, 'job_queue_capacity': 100, 'reward_function': 'lookahead_job_completion_time', 'pad_obs_kwargs': {'max_nodes': 200}}, 'batch_mode': 'complete_episodes', 'train_batch_size': 4, 'sgd_minibatch_size': 4, 'model': {'fcnet_hiddens': [8], 'fcnet_activation': 'relu', 'custom_model': 'my_model', 'custom_model_config': {'in_features_node': 5, 'in_features_edge': 1, 'out_features_msg': 8, 'out_features_hidden': 16, 'out_features': 4, 'in_features_graph': 34, 'out_features_graph': 4, 'num_layers': 1, 'aggregator_type': 'mean', 'action_space_type': 'discrete'}}, 'num_workers': 4, 'num_gpus': 1, 'framework': 'torch'}\n"
     ]
    }
   ],
   "source": [
    "env_config = {'node_config': node_config,\n",
    "              'topology_config': topology_config,\n",
    "              'jobs_config': jobs_config,\n",
    "              # 'max_simulation_run_time': 1e4,\n",
    "              'max_simulation_run_time': float('inf'),\n",
    "              'job_queue_capacity': 100,\n",
    "              'reward_function': 'lookahead_job_completion_time',\n",
    "              'pad_obs_kwargs': {'max_nodes': 200}\n",
    "             }\n",
    "\n",
    "model_config = {\n",
    "        'in_features_node':5,\n",
    "        'in_features_edge':1,\n",
    "        'out_features_msg':8,\n",
    "        'out_features_hidden':16,\n",
    "        'out_features':4,\n",
    "        'in_features_graph':34, # CHANGE 130\n",
    "        'out_features_graph':4,\n",
    "        'num_layers':1,\n",
    "        'aggregator_type':'mean',\n",
    "        'action_space_type': 'discrete' # 'discrete' 'continuous'\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "rllib_config = {\n",
    "    \n",
    "    'seed': 0,\n",
    "    \n",
    "    'env': 'ramp_job_placement_shaping_environment',\n",
    "    \n",
    "    'env_config': env_config,\n",
    "    \n",
    "    'batch_mode': 'complete_episodes',\n",
    "    'train_batch_size': 4, # 1 32 128\n",
    "    'sgd_minibatch_size': 4, # 1 32 128\n",
    "    \n",
    "    'model':{\n",
    "            'fcnet_hiddens':[8],\n",
    "            'fcnet_activation':'relu',\n",
    "            'custom_model':'my_model',\n",
    "            'custom_model_config': model_config\n",
    "        },\n",
    "    \n",
    "    'num_workers': 4,\n",
    "    'num_gpus': 1,\n",
    "    \n",
    "    'framework': 'torch'\n",
    "    \n",
    "    }\n",
    "\n",
    "print(rllib_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1badf56-9449-413c-88e9-8f7d5844965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 12:56:26,510\tWARNING ppo.py:143 -- `train_batch_size` (4) cannot be achieved with your other settings (num_workers=4 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1.\n",
      "2022-06-23 12:56:26,511\tINFO ppo.py:166 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-06-23 12:56:26,512\tINFO trainer.py:743 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'num_workers': 4, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'complete_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 4, 'model': {'fcnet_hiddens': [8], 'fcnet_activation': 'relu', 'custom_model': 'my_model', 'custom_model_config': {'in_features_node': 5, 'in_features_edge': 1, 'out_features_msg': 8, 'out_features_hidden': 16, 'out_features': 4, 'in_features_graph': 34, 'out_features_graph': 4, 'num_layers': 1, 'aggregator_type': 'mean', 'action_space_type': 'discrete'}}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'ramp_job_placement_shaping_environment', 'observation_space': None, 'action_space': None, 'env_config': {'node_config': {'type_1': {'num_nodes': 16, 'workers_config': [{'num_workers': 1, 'worker': <class 'ddls.devices.processors.gpus.A100.A100'>}]}}, 'topology_config': {'type': 'ramp', 'kwargs': {'num_communication_groups': 2, 'num_racks_per_communication_group': 2, 'num_servers_per_rack': 4, 'num_channels': 2}}, 'jobs_config': {'path_to_files': '/scratch/datasets/ddls/jobs/pipedream_graphs/image_classification/profiles/alexnet/', 'job_interarrival_time_dist': <ddls.distributions.uniform.Uniform object at 0x7efb574474f0>, 'max_files': 20, 'job_sampling_mode': 'remove'}, 'max_simulation_run_time': inf, 'job_queue_capacity': 100, 'reward_function': 'lookahead_job_completion_time', 'pad_obs_kwargs': {'max_nodes': 200}}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': 0, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, 'simple_optimizer': -1, 'monitor': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 4, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m 2022-06-23 12:56:34,807\tWARNING catalog.py:544 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m 2022-06-23 12:56:34,881\tWARNING catalog.py:544 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m 2022-06-23 12:56:34,865\tWARNING catalog.py:544 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m 2022-06-23 12:56:35,003\tWARNING catalog.py:544 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "2022-06-23 12:56:36,382\tWARNING catalog.py:544 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "2022-06-23 12:56:41,770\tINFO trainable.py:124 -- Trainable.setup took 15.265 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-06-23 12:56:41,772\tWARNING util.py:57 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialised trainer.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# load default PPO config and update with custom config params\n",
    "ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "ppo_config.update(rllib_config)\n",
    "print(f'Config:\\n{ppo_config}')\n",
    "\n",
    "# initialise rllib trainer\n",
    "epoch_loop = ppo.PPOTrainer(config=ppo_config)\n",
    "print('\\nInitialised trainer.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4bb585-8938-4a33-8ab8-1ecdad4aff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Epoch 1 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/utils/metrics/learner_info.py:84: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(tower_data)\n",
      "/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/utils/metrics/learner_info.py:64: RuntimeWarning: Mean of empty slice\n",
      "  lambda *s: None if s[0] is None else np.nanmean(s, axis=0),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-56-43\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: 0.0013693919650025483\n",
      "episode_reward_min: 0.001369391959558368\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 4\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000000000007\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.639056841532389\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.4651812515846056e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 1.1700356541410884e-07\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 1.4630718965143638e-07\n",
      "  num_agent_steps_sampled: 4\n",
      "  num_agent_steps_trained: 4\n",
      "  num_steps_sampled: 4\n",
      "  num_steps_trained: 4\n",
      "iterations_since_restore: 1\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.624999999999996\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.45168399810791016\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 52.41551995277405\n",
      "  mean_inference_ms: 4.060655832290649\n",
      "  mean_raw_obs_processing_ms: 20.72688937187195\n",
      "time_since_restore: 2.0064990520477295\n",
      "time_this_iter_s: 2.0064990520477295\n",
      "time_total_s: 2.0064990520477295\n",
      "timers:\n",
      "  learn_throughput: 2.216\n",
      "  learn_time_ms: 1805.351\n",
      "  load_throughput: 2096.628\n",
      "  load_time_ms: 1.908\n",
      "  sample_throughput: 13.324\n",
      "  sample_time_ms: 300.22\n",
      "  update_time_ms: 2.403\n",
      "timestamp: 1655985403\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 4\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 2 of 500 -------\n",
      "agent_timesteps_total: 8\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-56-45\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: 0.0013659984236535166\n",
      "episode_reward_min: 0.0013601434694806753\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 8\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.10000000000000003\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6390564918518065\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.167910338357615e-07\n",
      "        policy_loss: -4.33636208375295e-05\n",
      "        total_loss: -4.333946853876114e-05\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 2.1797673737759354e-09\n",
      "  num_agent_steps_sampled: 8\n",
      "  num_agent_steps_trained: 8\n",
      "  num_steps_sampled: 8\n",
      "  num_steps_trained: 8\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 2\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.5\n",
      "  ram_util_percent: 12.55\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.38808584213256836\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 57.40571518739064\n",
      "  mean_inference_ms: 4.499609271685282\n",
      "  mean_raw_obs_processing_ms: 24.52180286248525\n",
      "time_since_restore: 3.9413270950317383\n",
      "time_this_iter_s: 1.9348280429840088\n",
      "time_total_s: 3.9413270950317383\n",
      "timers:\n",
      "  learn_throughput: 2.234\n",
      "  learn_time_ms: 1790.233\n",
      "  load_throughput: 2376.713\n",
      "  load_time_ms: 1.683\n",
      "  sample_throughput: 3.508\n",
      "  sample_time_ms: 1140.119\n",
      "  update_time_ms: 2.235\n",
      "timestamp: 1655985405\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 8\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 3 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 12\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-56-47\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.08208194611832222\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 12\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.05000000000000002\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.639053948720296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1332590171756843e-06\n",
      "        policy_loss: -0.0014623085657755534\n",
      "        total_loss: 0.24834418892860413\n",
      "        vf_explained_var: 2.7815500895182292e-08\n",
      "        vf_loss: 0.24980644782384237\n",
      "  num_agent_steps_sampled: 12\n",
      "  num_agent_steps_trained: 12\n",
      "  num_steps_sampled: 12\n",
      "  num_steps_trained: 12\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 3\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.599999999999998\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.34774839878082275\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 59.50810843043857\n",
      "  mean_inference_ms: 4.948667354053921\n",
      "  mean_raw_obs_processing_ms: 27.47658226225111\n",
      "time_since_restore: 5.852765798568726\n",
      "time_this_iter_s: 1.9114387035369873\n",
      "time_total_s: 5.852765798568726\n",
      "timers:\n",
      "  learn_throughput: 2.26\n",
      "  learn_time_ms: 1769.667\n",
      "  load_throughput: 2455.921\n",
      "  load_time_ms: 1.629\n",
      "  sample_throughput: 2.828\n",
      "  sample_time_ms: 1414.38\n",
      "  update_time_ms: 2.582\n",
      "timestamp: 1655985407\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 12\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 4 of 500 -------\n",
      "agent_timesteps_total: 16\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-56-49\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06121966442386532\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 16\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.02500000000000001\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.639048147201538\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0235744412057102e-06\n",
      "        policy_loss: -1.9334256649017334e-05\n",
      "        total_loss: 2.845252553621928e-06\n",
      "        vf_explained_var: -5.8372815450032554e-06\n",
      "        vf_loss: 2.2154404905450065e-05\n",
      "  num_agent_steps_sampled: 16\n",
      "  num_agent_steps_trained: 16\n",
      "  num_steps_sampled: 16\n",
      "  num_steps_trained: 16\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 4\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.099999999999998\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.3179781138896942\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 60.704230765501656\n",
      "  mean_inference_ms: 5.215885986884435\n",
      "  mean_raw_obs_processing_ms: 29.32575742403666\n",
      "time_since_restore: 7.746791839599609\n",
      "time_this_iter_s: 1.8940260410308838\n",
      "time_total_s: 7.746791839599609\n",
      "timers:\n",
      "  learn_throughput: 2.265\n",
      "  learn_time_ms: 1765.917\n",
      "  load_throughput: 2611.748\n",
      "  load_time_ms: 1.532\n",
      "  sample_throughput: 2.608\n",
      "  sample_time_ms: 1533.725\n",
      "  update_time_ms: 2.674\n",
      "timestamp: 1655985409\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 16\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 5 of 500 -------\n",
      "agent_timesteps_total: 20\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-56-51\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870185314581954\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 20\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.012500000000000004\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6390458424886067\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.161211285875955e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 2.5083798027480954e-05\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 2.5082652913018442e-05\n",
      "  num_agent_steps_sampled: 20\n",
      "  num_agent_steps_trained: 20\n",
      "  num_steps_sampled: 20\n",
      "  num_steps_trained: 20\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 5\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.366666666666664\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.2952990929285685\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 61.57296657562257\n",
      "  mean_inference_ms: 5.420621434847513\n",
      "  mean_raw_obs_processing_ms: 30.730985800425213\n",
      "time_since_restore: 9.718982458114624\n",
      "time_this_iter_s: 1.9721906185150146\n",
      "time_total_s: 9.718982458114624\n",
      "timers:\n",
      "  learn_throughput: 2.255\n",
      "  learn_time_ms: 1774.129\n",
      "  load_throughput: 2665.081\n",
      "  load_time_ms: 1.501\n",
      "  sample_throughput: 2.478\n",
      "  sample_time_ms: 1614.464\n",
      "  update_time_ms: 4.163\n",
      "timestamp: 1655985411\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 20\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 6 of 500 -------\n",
      "agent_timesteps_total: 24\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-56-53\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.040357389003118076\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 24\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.006250000000000002\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6390435536702475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.018515776105535e-07\n",
      "        policy_loss: -0.0001047944650053978\n",
      "        total_loss: -8.071844155589739e-05\n",
      "        vf_explained_var: -1.6142924626668293e-05\n",
      "        vf_loss: 2.4071304505923762e-05\n",
      "  num_agent_steps_sampled: 24\n",
      "  num_agent_steps_trained: 24\n",
      "  num_steps_sampled: 24\n",
      "  num_steps_trained: 24\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 6\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.53333333333333\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.27730831551173374\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 62.37504113288153\n",
      "  mean_inference_ms: 5.584300676035503\n",
      "  mean_raw_obs_processing_ms: 31.826457144722102\n",
      "time_since_restore: 11.84056568145752\n",
      "time_this_iter_s: 2.1215832233428955\n",
      "time_total_s: 11.84056568145752\n",
      "timers:\n",
      "  learn_throughput: 2.215\n",
      "  learn_time_ms: 1805.78\n",
      "  load_throughput: 2645.553\n",
      "  load_time_ms: 1.512\n",
      "  sample_throughput: 2.384\n",
      "  sample_time_ms: 1678.201\n",
      "  update_time_ms: 4.009\n",
      "timestamp: 1655985413\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 24\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 7 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 28\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-56-55\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07016025772227394\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 28\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.003125000000000001\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6390395005544027\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.39330069134788e-07\n",
      "        policy_loss: -0.0014851073424021403\n",
      "        total_loss: 0.24607057273387908\n",
      "        vf_explained_var: -3.9736429850260414e-08\n",
      "        vf_loss: 0.247555677096049\n",
      "  num_agent_steps_sampled: 28\n",
      "  num_agent_steps_trained: 28\n",
      "  num_steps_sampled: 28\n",
      "  num_steps_trained: 28\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 7\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.65\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.2625937465907765\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 62.68723397230615\n",
      "  mean_inference_ms: 5.708696475239838\n",
      "  mean_raw_obs_processing_ms: 32.62786206339492\n",
      "time_since_restore: 13.807608366012573\n",
      "time_this_iter_s: 1.9670426845550537\n",
      "time_total_s: 13.807608366012573\n",
      "timers:\n",
      "  learn_throughput: 2.21\n",
      "  learn_time_ms: 1809.624\n",
      "  load_throughput: 2601.58\n",
      "  load_time_ms: 1.538\n",
      "  sample_throughput: 2.297\n",
      "  sample_time_ms: 1741.633\n",
      "  update_time_ms: 3.722\n",
      "timestamp: 1655985415\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 28\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 8 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 32\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-56-57\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.09251212360618402\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 32\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0015625000000000005\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.639030679066976\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.2243018975132143e-06\n",
      "        policy_loss: -0.0023518572251001995\n",
      "        total_loss: 0.2430181622505188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 0.24537003089984258\n",
      "  num_agent_steps_sampled: 32\n",
      "  num_agent_steps_trained: 32\n",
      "  num_steps_sampled: 32\n",
      "  num_steps_trained: 32\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 8\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.899999999999995\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.2504672411651838\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 62.75884035738215\n",
      "  mean_inference_ms: 5.812856995515407\n",
      "  mean_raw_obs_processing_ms: 33.32410706059328\n",
      "time_since_restore: 15.87346363067627\n",
      "time_this_iter_s: 2.0658552646636963\n",
      "time_total_s: 15.87346363067627\n",
      "timers:\n",
      "  learn_throughput: 2.194\n",
      "  learn_time_ms: 1822.862\n",
      "  load_throughput: 2508.18\n",
      "  load_time_ms: 1.595\n",
      "  sample_throughput: 2.254\n",
      "  sample_time_ms: 1774.878\n",
      "  update_time_ms: 3.717\n",
      "timestamp: 1655985417\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 32\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 9 of 500 -------\n",
      "agent_timesteps_total: 36\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-00\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.08208084409857684\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 36\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.0007812500000000003\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6390141884485883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.306527817893463e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.00020336223678896204\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.00020336174444916347\n",
      "  num_agent_steps_sampled: 36\n",
      "  num_agent_steps_trained: 36\n",
      "  num_steps_sampled: 36\n",
      "  num_steps_trained: 36\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 9\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.866666666666664\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.24078848658415375\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 62.87074065660464\n",
      "  mean_inference_ms: 5.933142099956566\n",
      "  mean_raw_obs_processing_ms: 33.846840178314764\n",
      "time_since_restore: 17.974022388458252\n",
      "time_this_iter_s: 2.1005587577819824\n",
      "time_total_s: 17.974022388458252\n",
      "timers:\n",
      "  learn_throughput: 2.178\n",
      "  learn_time_ms: 1836.431\n",
      "  load_throughput: 2224.865\n",
      "  load_time_ms: 1.798\n",
      "  sample_throughput: 2.212\n",
      "  sample_time_ms: 1808.18\n",
      "  update_time_ms: 3.853\n",
      "timestamp: 1655985420\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 36\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 10 of 500 -------\n",
      "agent_timesteps_total: 40\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-02\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07373604539525087\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 40\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.00039062500000000013\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6390135606129963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.681321524917924e-07\n",
      "        policy_loss: -5.298197890321414e-05\n",
      "        total_loss: 0.00015499470755457877\n",
      "        vf_explained_var: 0.00011282960573832194\n",
      "        vf_loss: 0.00020797685817039262\n",
      "  num_agent_steps_sampled: 40\n",
      "  num_agent_steps_trained: 40\n",
      "  num_steps_sampled: 40\n",
      "  num_steps_trained: 40\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 10\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.5\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.23232334555485545\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 63.215341140740534\n",
      "  mean_inference_ms: 6.0325194761952625\n",
      "  mean_raw_obs_processing_ms: 34.28836795975808\n",
      "time_since_restore: 20.1353702545166\n",
      "time_this_iter_s: 2.1613478660583496\n",
      "time_total_s: 20.1353702545166\n",
      "timers:\n",
      "  learn_throughput: 2.162\n",
      "  learn_time_ms: 1850.467\n",
      "  load_throughput: 2164.188\n",
      "  load_time_ms: 1.848\n",
      "  sample_throughput: 2.17\n",
      "  sample_time_ms: 1843.004\n",
      "  update_time_ms: 4.182\n",
      "timestamp: 1655985422\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 40\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 11 of 500 -------\n",
      "agent_timesteps_total: 44\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-04\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.066908491594457\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 44\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.00019531250000000007\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6390188852945964\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.858155489278337e-07\n",
      "        policy_loss: -2.597492809096972e-05\n",
      "        total_loss: 0.00017298348248004914\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 0.00019895853474736214\n",
      "  num_agent_steps_sampled: 44\n",
      "  num_agent_steps_trained: 44\n",
      "  num_steps_sampled: 44\n",
      "  num_steps_trained: 44\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 11\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.366666666666664\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.22478793336856276\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 63.5471799788271\n",
      "  mean_inference_ms: 6.117495566227297\n",
      "  mean_raw_obs_processing_ms: 34.655438980279946\n",
      "time_since_restore: 22.27735662460327\n",
      "time_this_iter_s: 2.14198637008667\n",
      "time_total_s: 22.27735662460327\n",
      "timers:\n",
      "  learn_throughput: 2.143\n",
      "  learn_time_ms: 1866.495\n",
      "  load_throughput: 2194.793\n",
      "  load_time_ms: 1.822\n",
      "  sample_throughput: 1.969\n",
      "  sample_time_ms: 2031.045\n",
      "  update_time_ms: 4.631\n",
      "timestamp: 1655985424\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 44\n",
      "training_iteration: 11\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 12 of 500 -------\n",
      "agent_timesteps_total: 48\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-06\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06121924030244247\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 48\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.765625000000003e-05\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6390220324198403\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.89955638537261e-07\n",
      "        policy_loss: -2.7271794776121774e-05\n",
      "        total_loss: 0.00016209113722046216\n",
      "        vf_explained_var: 1.1738141377766927e-05\n",
      "        vf_loss: 0.00018936305568786337\n",
      "  num_agent_steps_sampled: 48\n",
      "  num_agent_steps_trained: 48\n",
      "  num_steps_sampled: 48\n",
      "  num_steps_trained: 48\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 12\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.366666666666664\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.21823252885061648\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 64.13549542101494\n",
      "  mean_inference_ms: 6.19817730042804\n",
      "  mean_raw_obs_processing_ms: 35.0352576545483\n",
      "time_since_restore: 24.410166263580322\n",
      "time_this_iter_s: 2.132809638977051\n",
      "time_total_s: 24.410166263580322\n",
      "timers:\n",
      "  learn_throughput: 2.128\n",
      "  learn_time_ms: 1879.58\n",
      "  load_throughput: 2205.352\n",
      "  load_time_ms: 1.814\n",
      "  sample_throughput: 1.948\n",
      "  sample_time_ms: 2053.791\n",
      "  update_time_ms: 4.672\n",
      "timestamp: 1655985426\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 48\n",
      "training_iteration: 12\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 13 of 500 -------\n",
      "agent_timesteps_total: 52\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-08\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05640490430299143\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 52\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.8828125000000016e-05\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.639018170038859\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.488021623828293e-07\n",
      "        policy_loss: -4.318306843439738e-05\n",
      "        total_loss: 0.00013700531174739203\n",
      "        vf_explained_var: -7.535219192504883e-05\n",
      "        vf_loss: 0.00018018834913770357\n",
      "  num_agent_steps_sampled: 52\n",
      "  num_agent_steps_trained: 52\n",
      "  num_steps_sampled: 52\n",
      "  num_steps_trained: 52\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 13\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.166666666666668\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.2124617905926272\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 64.70023081552314\n",
      "  mean_inference_ms: 6.273972717974581\n",
      "  mean_raw_obs_processing_ms: 35.428230686518425\n",
      "time_since_restore: 26.435329914093018\n",
      "time_this_iter_s: 2.0251636505126953\n",
      "time_total_s: 26.435329914093018\n",
      "timers:\n",
      "  learn_throughput: 2.113\n",
      "  learn_time_ms: 1892.659\n",
      "  load_throughput: 2228.227\n",
      "  load_time_ms: 1.795\n",
      "  sample_throughput: 1.934\n",
      "  sample_time_ms: 2068.633\n",
      "  update_time_ms: 4.582\n",
      "timestamp: 1655985428\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 52\n",
      "training_iteration: 13\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 14 of 500 -------\n",
      "agent_timesteps_total: 56\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-10\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.052278341809747664\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 56\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4414062500000008e-05\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.639010993639628\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1372372076342194e-06\n",
      "        policy_loss: -8.522033070524533e-05\n",
      "        total_loss: 8.589858189225196e-05\n",
      "        vf_explained_var: 0.0001624147097269694\n",
      "        vf_loss: 0.00017111870450510955\n",
      "  num_agent_steps_sampled: 56\n",
      "  num_agent_steps_trained: 56\n",
      "  num_steps_sampled: 56\n",
      "  num_steps_trained: 56\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 14\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.366666666666664\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.2073909225183608\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 65.18102424206283\n",
      "  mean_inference_ms: 6.355088414514489\n",
      "  mean_raw_obs_processing_ms: 35.77593773645499\n",
      "time_since_restore: 28.49201512336731\n",
      "time_this_iter_s: 2.056685209274292\n",
      "time_total_s: 28.49201512336731\n",
      "timers:\n",
      "  learn_throughput: 2.1\n",
      "  learn_time_ms: 1904.857\n",
      "  load_throughput: 2209.913\n",
      "  load_time_ms: 1.81\n",
      "  sample_throughput: 1.918\n",
      "  sample_time_ms: 2085.696\n",
      "  update_time_ms: 4.643\n",
      "timestamp: 1655985430\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 56\n",
      "training_iteration: 14\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 15 of 500 -------\n",
      "agent_timesteps_total: 60\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-12\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870198948967874\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 60\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2207031250000004e-05\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6390023469924926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.249880243558436e-07\n",
      "        policy_loss: -4.6916337062915166e-05\n",
      "        total_loss: 0.00011539564778407415\n",
      "        vf_explained_var: 4.750490188598633e-05\n",
      "        vf_loss: 0.00016231219148418556\n",
      "  num_agent_steps_sampled: 60\n",
      "  num_agent_steps_trained: 60\n",
      "  num_steps_sampled: 60\n",
      "  num_steps_trained: 60\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 15\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.400000000000002\n",
      "  ram_util_percent: 12.633333333333333\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.20275614559418856\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 65.58677864807663\n",
      "  mean_inference_ms: 6.425607400781709\n",
      "  mean_raw_obs_processing_ms: 36.066828231409346\n",
      "time_since_restore: 30.483489513397217\n",
      "time_this_iter_s: 1.9914743900299072\n",
      "time_total_s: 30.483489513397217\n",
      "timers:\n",
      "  learn_throughput: 2.097\n",
      "  learn_time_ms: 1907.774\n",
      "  load_throughput: 2182.8\n",
      "  load_time_ms: 1.833\n",
      "  sample_throughput: 1.906\n",
      "  sample_time_ms: 2099.104\n",
      "  update_time_ms: 4.03\n",
      "timestamp: 1655985432\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 60\n",
      "training_iteration: 15\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 16 of 500 -------\n",
      "agent_timesteps_total: 64\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-14\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04557282569627398\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 64\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 6.103515625000002e-06\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.638995091120402\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0805073240286826e-06\n",
      "        policy_loss: -6.787863870461782e-05\n",
      "        total_loss: 8.585397154092789e-05\n",
      "        vf_explained_var: 8.826454480489095e-05\n",
      "        vf_loss: 0.0001537325306950758\n",
      "  num_agent_steps_sampled: 64\n",
      "  num_agent_steps_trained: 64\n",
      "  num_steps_sampled: 64\n",
      "  num_steps_trained: 64\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 16\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.700000000000003\n",
      "  ram_util_percent: 12.7\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19879712136793076\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 65.96048894861104\n",
      "  mean_inference_ms: 6.506591561962333\n",
      "  mean_raw_obs_processing_ms: 36.3165244803445\n",
      "time_since_restore: 32.45759868621826\n",
      "time_this_iter_s: 1.974109172821045\n",
      "time_total_s: 32.45759868621826\n",
      "timers:\n",
      "  learn_throughput: 2.113\n",
      "  learn_time_ms: 1892.959\n",
      "  load_throughput: 2218.944\n",
      "  load_time_ms: 1.803\n",
      "  sample_throughput: 1.904\n",
      "  sample_time_ms: 2101.148\n",
      "  update_time_ms: 4.038\n",
      "timestamp: 1655985434\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 64\n",
      "training_iteration: 16\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 17 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 68\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-16\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.057537539245176045\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 68\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.051757812500001e-06\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.638969810803731\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.909493928707282e-06\n",
      "        policy_loss: -0.0036184996366500854\n",
      "        total_loss: 0.24017160038153332\n",
      "        vf_explained_var: 3.9736429850260414e-08\n",
      "        vf_loss: 0.2437900980313619\n",
      "  num_agent_steps_sampled: 68\n",
      "  num_agent_steps_trained: 68\n",
      "  num_steps_sampled: 68\n",
      "  num_steps_trained: 68\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 17\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.2\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19535529977082838\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 66.24085075720083\n",
      "  mean_inference_ms: 6.576742945336828\n",
      "  mean_raw_obs_processing_ms: 36.56760453513555\n",
      "time_since_restore: 34.37532830238342\n",
      "time_this_iter_s: 1.9177296161651611\n",
      "time_total_s: 34.37532830238342\n",
      "timers:\n",
      "  learn_throughput: 2.122\n",
      "  learn_time_ms: 1885.215\n",
      "  load_throughput: 2272.997\n",
      "  load_time_ms: 1.76\n",
      "  sample_throughput: 1.916\n",
      "  sample_time_ms: 2087.835\n",
      "  update_time_ms: 4.253\n",
      "timestamp: 1655985436\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 68\n",
      "training_iteration: 17\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 18 of 500 -------\n",
      "agent_timesteps_total: 72\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-18\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.054264931955570445\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 72\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5258789062500005e-06\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.638928198814392\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.176628933947844e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0003442945870726059\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0003442945870726059\n",
      "  num_agent_steps_sampled: 72\n",
      "  num_agent_steps_trained: 72\n",
      "  num_steps_sampled: 72\n",
      "  num_steps_trained: 72\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 18\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.633333333333336\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19213231922293617\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 66.45572983038271\n",
      "  mean_inference_ms: 6.6385084671996255\n",
      "  mean_raw_obs_processing_ms: 36.78491149799189\n",
      "time_since_restore: 36.30193519592285\n",
      "time_this_iter_s: 1.9266068935394287\n",
      "time_total_s: 36.30193519592285\n",
      "timers:\n",
      "  learn_throughput: 2.136\n",
      "  learn_time_ms: 1873.08\n",
      "  load_throughput: 2364.687\n",
      "  load_time_ms: 1.692\n",
      "  sample_throughput: 1.925\n",
      "  sample_time_ms: 2078.181\n",
      "  update_time_ms: 4.249\n",
      "timestamp: 1655985438\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 72\n",
      "training_iteration: 18\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 19 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 76\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-20\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06451284755785597\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 76\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.629394531250003e-07\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6389034509658815\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.3632414366972324e-06\n",
      "        policy_loss: -0.0030986666679382324\n",
      "        total_loss: 0.2375253717104594\n",
      "        vf_explained_var: 6.357828776041667e-08\n",
      "        vf_loss: 0.24062402695417404\n",
      "  num_agent_steps_sampled: 76\n",
      "  num_agent_steps_trained: 76\n",
      "  num_steps_sampled: 76\n",
      "  num_steps_trained: 76\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 19\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.666666666666668\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18911053495443098\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 66.598274075469\n",
      "  mean_inference_ms: 6.691894652017376\n",
      "  mean_raw_obs_processing_ms: 36.966444954647784\n",
      "time_since_restore: 38.170427083969116\n",
      "time_this_iter_s: 1.8684918880462646\n",
      "time_total_s: 38.170427083969116\n",
      "timers:\n",
      "  learn_throughput: 2.159\n",
      "  learn_time_ms: 1852.501\n",
      "  load_throughput: 2707.531\n",
      "  load_time_ms: 1.477\n",
      "  sample_throughput: 1.937\n",
      "  sample_time_ms: 2065.534\n",
      "  update_time_ms: 4.293\n",
      "timestamp: 1655985440\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 76\n",
      "training_iteration: 19\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 20 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 80\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-22\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07373597379762088\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 80\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.8146972656250013e-07\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6388290087382\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.922533056136065e-06\n",
      "        policy_loss: -0.004236912727355957\n",
      "        total_loss: 0.23322179516156513\n",
      "        vf_explained_var: 6.75519307454427e-08\n",
      "        vf_loss: 0.237458698451519\n",
      "  num_agent_steps_sampled: 80\n",
      "  num_agent_steps_trained: 80\n",
      "  num_steps_sampled: 80\n",
      "  num_steps_trained: 80\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 20\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.25\n",
      "  ram_util_percent: 12.7\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18627637466215086\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 66.67941469906995\n",
      "  mean_inference_ms: 6.738337086556922\n",
      "  mean_raw_obs_processing_ms: 37.11756860510766\n",
      "time_since_restore: 40.04798078536987\n",
      "time_this_iter_s: 1.8775537014007568\n",
      "time_total_s: 40.04798078536987\n",
      "timers:\n",
      "  learn_throughput: 2.186\n",
      "  learn_time_ms: 1829.693\n",
      "  load_throughput: 2882.436\n",
      "  load_time_ms: 1.388\n",
      "  sample_throughput: 1.961\n",
      "  sample_time_ms: 2039.755\n",
      "  update_time_ms: 3.845\n",
      "timestamp: 1655985442\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 80\n",
      "training_iteration: 20\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 21 of 500 -------\n",
      "agent_timesteps_total: 84\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-24\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07015985688316347\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 84\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9073486328125006e-07\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6387428998947144\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5504310037310158e-06\n",
      "        policy_loss: -6.334039693077406e-05\n",
      "        total_loss: 0.0010572018412252266\n",
      "        vf_explained_var: 0.0003542641798655192\n",
      "        vf_loss: 0.0011205424399425587\n",
      "  num_agent_steps_sampled: 84\n",
      "  num_agent_steps_trained: 84\n",
      "  num_steps_sampled: 84\n",
      "  num_steps_trained: 84\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 21\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.200000000000003\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18397161303888745\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 66.77832412388722\n",
      "  mean_inference_ms: 6.7841861134573405\n",
      "  mean_raw_obs_processing_ms: 37.259255393293564\n",
      "time_since_restore: 41.999276876449585\n",
      "time_this_iter_s: 1.951296091079712\n",
      "time_total_s: 41.999276876449585\n",
      "timers:\n",
      "  learn_throughput: 2.208\n",
      "  learn_time_ms: 1811.679\n",
      "  load_throughput: 2957.901\n",
      "  load_time_ms: 1.352\n",
      "  sample_throughput: 1.985\n",
      "  sample_time_ms: 2015.039\n",
      "  update_time_ms: 3.341\n",
      "timestamp: 1655985444\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 84\n",
      "training_iteration: 21\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 22 of 500 -------\n",
      "agent_timesteps_total: 88\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-26\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0669086267135327\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 88\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.536743164062503e-08\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6387011766433717\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.610307607644548e-07\n",
      "        policy_loss: -9.470858300725619e-05\n",
      "        total_loss: 0.0010028870465854803\n",
      "        vf_explained_var: 0.00021308660507202148\n",
      "        vf_loss: 0.0010975956140706936\n",
      "  num_agent_steps_sampled: 88\n",
      "  num_agent_steps_trained: 88\n",
      "  num_steps_sampled: 88\n",
      "  num_steps_trained: 88\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 22\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.600000000000005\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18179860859331223\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 66.8549139175935\n",
      "  mean_inference_ms: 6.842783559460965\n",
      "  mean_raw_obs_processing_ms: 37.387969592669954\n",
      "time_since_restore: 43.87874507904053\n",
      "time_this_iter_s: 1.8794682025909424\n",
      "time_total_s: 43.87874507904053\n",
      "timers:\n",
      "  learn_throughput: 2.229\n",
      "  learn_time_ms: 1794.221\n",
      "  load_throughput: 3000.164\n",
      "  load_time_ms: 1.333\n",
      "  sample_throughput: 2.011\n",
      "  sample_time_ms: 1989.32\n",
      "  update_time_ms: 3.602\n",
      "timestamp: 1655985446\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 88\n",
      "training_iteration: 22\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 23 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 92\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-28\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0748245645170548\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 92\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.7683715820312516e-08\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.638635190327962\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.085219229134964e-06\n",
      "        policy_loss: -0.004083748658498128\n",
      "        total_loss: 0.2304176062345505\n",
      "        vf_explained_var: -2.384185791015625e-08\n",
      "        vf_loss: 0.2345013494292895\n",
      "  num_agent_steps_sampled: 92\n",
      "  num_agent_steps_trained: 92\n",
      "  num_steps_sampled: 92\n",
      "  num_steps_trained: 92\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 23\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.7\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1797682942181256\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 66.90571554428735\n",
      "  mean_inference_ms: 6.896455211060186\n",
      "  mean_raw_obs_processing_ms: 37.49527846938051\n",
      "time_since_restore: 45.904624223709106\n",
      "time_this_iter_s: 2.025879144668579\n",
      "time_total_s: 45.904624223709106\n",
      "timers:\n",
      "  learn_throughput: 2.227\n",
      "  learn_time_ms: 1796.27\n",
      "  load_throughput: 2896.419\n",
      "  load_time_ms: 1.381\n",
      "  sample_throughput: 2.032\n",
      "  sample_time_ms: 1968.783\n",
      "  update_time_ms: 3.676\n",
      "timestamp: 1655985448\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 92\n",
      "training_iteration: 23\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 24 of 500 -------\n",
      "agent_timesteps_total: 96\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-30\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0716499218460312\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 96\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.3841857910156258e-08\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6385182698567706\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.825968173662356e-06\n",
      "        policy_loss: -0.000101301788041989\n",
      "        total_loss: 0.0014864695879320303\n",
      "        vf_explained_var: 0.0001430511474609375\n",
      "        vf_loss: 0.001587771084935715\n",
      "  num_agent_steps_sampled: 96\n",
      "  num_agent_steps_trained: 96\n",
      "  num_steps_sampled: 96\n",
      "  num_steps_trained: 96\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 24\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.799999999999997\n",
      "  ram_util_percent: 12.7\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.17782286938808367\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 66.9527600963526\n",
      "  mean_inference_ms: 6.944253172319367\n",
      "  mean_raw_obs_processing_ms: 37.58570252558033\n",
      "time_since_restore: 47.79277229309082\n",
      "time_this_iter_s: 1.8881480693817139\n",
      "time_total_s: 47.79277229309082\n",
      "timers:\n",
      "  learn_throughput: 2.244\n",
      "  learn_time_ms: 1782.85\n",
      "  load_throughput: 2671.489\n",
      "  load_time_ms: 1.497\n",
      "  sample_throughput: 2.034\n",
      "  sample_time_ms: 1966.686\n",
      "  update_time_ms: 3.618\n",
      "timestamp: 1655985450\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 96\n",
      "training_iteration: 24\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 25 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 100\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-32\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07874293121519628\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 100\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1920928955078129e-08\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6384035031000774\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.2300449169706555e-06\n",
      "        policy_loss: -0.004398266474405925\n",
      "        total_loss: 0.22684779067834218\n",
      "        vf_explained_var: -5.960464477539063e-08\n",
      "        vf_loss: 0.23124605218569438\n",
      "  num_agent_steps_sampled: 100\n",
      "  num_agent_steps_trained: 100\n",
      "  num_steps_sampled: 100\n",
      "  num_steps_trained: 100\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 25\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.7\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.17595048668066618\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 66.97267117775381\n",
      "  mean_inference_ms: 6.986850216640768\n",
      "  mean_raw_obs_processing_ms: 37.66142594491939\n",
      "time_since_restore: 49.77192044258118\n",
      "time_this_iter_s: 1.9791481494903564\n",
      "time_total_s: 49.77192044258118\n",
      "timers:\n",
      "  learn_throughput: 2.243\n",
      "  learn_time_ms: 1783.472\n",
      "  load_throughput: 2756.735\n",
      "  load_time_ms: 1.451\n",
      "  sample_throughput: 2.052\n",
      "  sample_time_ms: 1949.77\n",
      "  update_time_ms: 3.715\n",
      "timestamp: 1655985452\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 100\n",
      "training_iteration: 25\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 26 of 500 -------\n",
      "agent_timesteps_total: 104\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-34\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0787431195058424\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 104\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.9604644775390645e-09\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.638250541687012\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5273020532428442e-06\n",
      "        policy_loss: -6.879735738039017e-05\n",
      "        total_loss: 0.0022487713334461055\n",
      "        vf_explained_var: 0.00014191865921020508\n",
      "        vf_loss: 0.0023175686442603667\n",
      "  num_agent_steps_sampled: 104\n",
      "  num_agent_steps_trained: 104\n",
      "  num_steps_sampled: 104\n",
      "  num_steps_trained: 104\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 26\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.96666666666667\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1630415334731557\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 67.59726573371458\n",
      "  mean_inference_ms: 7.143500185883006\n",
      "  mean_raw_obs_processing_ms: 38.42035700775904\n",
      "time_since_restore: 51.872918128967285\n",
      "time_this_iter_s: 2.1009976863861084\n",
      "time_total_s: 51.872918128967285\n",
      "timers:\n",
      "  learn_throughput: 2.233\n",
      "  learn_time_ms: 1791.572\n",
      "  load_throughput: 2734.583\n",
      "  load_time_ms: 1.463\n",
      "  sample_throughput: 2.048\n",
      "  sample_time_ms: 1953.594\n",
      "  update_time_ms: 4.121\n",
      "timestamp: 1655985454\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 104\n",
      "training_iteration: 26\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 27 of 500 -------\n",
      "agent_timesteps_total: 108\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-36\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07874303410489697\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 108\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.9802322387695322e-09\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.638223433494568\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1816610266493322e-07\n",
      "        policy_loss: -6.673528502384821e-05\n",
      "        total_loss: 0.0021443635846177737\n",
      "        vf_explained_var: 0.0004094243049621582\n",
      "        vf_loss: 0.0022110989938179653\n",
      "  num_agent_steps_sampled: 108\n",
      "  num_agent_steps_trained: 108\n",
      "  num_steps_sampled: 108\n",
      "  num_steps_trained: 108\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 27\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.099999999999998\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1551602667429834\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 67.8289033762117\n",
      "  mean_inference_ms: 7.2645623851577055\n",
      "  mean_raw_obs_processing_ms: 38.86974038011127\n",
      "time_since_restore: 53.933050870895386\n",
      "time_this_iter_s: 2.0601327419281006\n",
      "time_total_s: 53.933050870895386\n",
      "timers:\n",
      "  learn_throughput: 2.214\n",
      "  learn_time_ms: 1806.554\n",
      "  load_throughput: 2636.518\n",
      "  load_time_ms: 1.517\n",
      "  sample_throughput: 2.037\n",
      "  sample_time_ms: 1963.464\n",
      "  update_time_ms: 3.933\n",
      "timestamp: 1655985456\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 108\n",
      "training_iteration: 27\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 28 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 12:57:36,609\tWARNING worker.py:1245 -- (ip=128.40.41.23) The agent on node mammoth.ee.ucl.ac.uk failed to be restarted 5 times. There are 3 possible problems if you see this error.\n",
      "  1. The dashboard might not display correct information on this node.\n",
      "  2. Metrics on this node won't be reported.\n",
      "  3. runtime_env APIs won't work.\n",
      "Check out the `dashboard_agent.log` to see the detailed failure messages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 112\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-38\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872923820075028\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 112\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.4901161193847661e-09\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6382069031397504\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4711568282024623e-07\n",
      "        policy_loss: -4.4607526312271754e-05\n",
      "        total_loss: 0.001974727027118206\n",
      "        vf_explained_var: 0.00012424389521280925\n",
      "        vf_loss: 0.0020193347590975463\n",
      "  num_agent_steps_sampled: 112\n",
      "  num_agent_steps_trained: 112\n",
      "  num_steps_sampled: 112\n",
      "  num_steps_trained: 112\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 28\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.23333333333333\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1495040942536856\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.02194711789826\n",
      "  mean_inference_ms: 7.347627109816804\n",
      "  mean_raw_obs_processing_ms: 39.12136015483103\n",
      "time_since_restore: 56.02265405654907\n",
      "time_this_iter_s: 2.0896031856536865\n",
      "time_total_s: 56.02265405654907\n",
      "timers:\n",
      "  learn_throughput: 2.198\n",
      "  learn_time_ms: 1819.718\n",
      "  load_throughput: 2510.77\n",
      "  load_time_ms: 1.593\n",
      "  sample_throughput: 2.019\n",
      "  sample_time_ms: 1980.805\n",
      "  update_time_ms: 3.762\n",
      "timestamp: 1655985458\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 112\n",
      "training_iteration: 28\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 29 of 500 -------\n",
      "agent_timesteps_total: 116\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-40\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872914974847595\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 116\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.450580596923831e-10\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6381934245427447\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -7.142173975201635e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0018441944848746061\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0018441944848746061\n",
      "  num_agent_steps_sampled: 116\n",
      "  num_agent_steps_trained: 116\n",
      "  num_steps_sampled: 116\n",
      "  num_steps_trained: 116\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 29\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.166666666666668\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1454617867178783\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.19113140528741\n",
      "  mean_inference_ms: 7.434542920719718\n",
      "  mean_raw_obs_processing_ms: 39.33174174217743\n",
      "time_since_restore: 58.00089883804321\n",
      "time_this_iter_s: 1.9782447814941406\n",
      "time_total_s: 58.00089883804321\n",
      "timers:\n",
      "  learn_throughput: 2.191\n",
      "  learn_time_ms: 1825.994\n",
      "  load_throughput: 2532.104\n",
      "  load_time_ms: 1.58\n",
      "  sample_throughput: 2.003\n",
      "  sample_time_ms: 1997.208\n",
      "  update_time_ms: 3.57\n",
      "timestamp: 1655985460\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 116\n",
      "training_iteration: 29\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 30 of 500 -------\n",
      "agent_timesteps_total: 120\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-42\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872924557698734\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 120\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.7252902984619153e-10\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.638194521268209\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.417424222282231e-08\n",
      "        policy_loss: -1.3359946509202321e-05\n",
      "        total_loss: 0.0016720050324996313\n",
      "        vf_explained_var: 0.00029158592224121094\n",
      "        vf_loss: 0.001685365072141091\n",
      "  num_agent_steps_sampled: 120\n",
      "  num_agent_steps_trained: 120\n",
      "  num_steps_sampled: 120\n",
      "  num_steps_trained: 120\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 30\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.399999999999995\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1423294024073918\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.33410642543966\n",
      "  mean_inference_ms: 7.5121133667956475\n",
      "  mean_raw_obs_processing_ms: 39.48385235982532\n",
      "time_since_restore: 59.94494652748108\n",
      "time_this_iter_s: 1.9440476894378662\n",
      "time_total_s: 59.94494652748108\n",
      "timers:\n",
      "  learn_throughput: 2.187\n",
      "  learn_time_ms: 1829.37\n",
      "  load_throughput: 2441.459\n",
      "  load_time_ms: 1.638\n",
      "  sample_throughput: 1.994\n",
      "  sample_time_ms: 2005.863\n",
      "  update_time_ms: 3.552\n",
      "timestamp: 1655985462\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 120\n",
      "training_iteration: 30\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 31 of 500 -------\n",
      "agent_timesteps_total: 124\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-44\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872906716680281\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 124\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8626451492309577e-10\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.638196516036987\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -7.386254310404184e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0015431333449669182\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0015431333449669182\n",
      "  num_agent_steps_sampled: 124\n",
      "  num_agent_steps_trained: 124\n",
      "  num_steps_sampled: 124\n",
      "  num_steps_trained: 124\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 31\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 33.349999999999994\n",
      "  ram_util_percent: 12.7\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13984670929719828\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.41153250776124\n",
      "  mean_inference_ms: 7.594851564071218\n",
      "  mean_raw_obs_processing_ms: 39.608419617331826\n",
      "time_since_restore: 62.424670934677124\n",
      "time_this_iter_s: 2.479724407196045\n",
      "time_total_s: 62.424670934677124\n",
      "timers:\n",
      "  learn_throughput: 2.125\n",
      "  learn_time_ms: 1882.309\n",
      "  load_throughput: 2420.291\n",
      "  load_time_ms: 1.653\n",
      "  sample_throughput: 1.991\n",
      "  sample_time_ms: 2009.427\n",
      "  update_time_ms: 3.65\n",
      "timestamp: 1655985464\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 124\n",
      "training_iteration: 31\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 32 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 128\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-47\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0687290624722871\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 128\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.313225746154788e-11\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6381874879201255\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.780304433360527e-06\n",
      "        policy_loss: -0.004159031311670939\n",
      "        total_loss: 0.2277791331211726\n",
      "        vf_explained_var: 7.152557373046875e-08\n",
      "        vf_loss: 0.23193816244602203\n",
      "  num_agent_steps_sampled: 128\n",
      "  num_agent_steps_trained: 128\n",
      "  num_steps_sampled: 128\n",
      "  num_steps_trained: 128\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 32\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 33.833333333333336\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13782763807945905\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.56265602966721\n",
      "  mean_inference_ms: 7.67336235965003\n",
      "  mean_raw_obs_processing_ms: 39.736203324025766\n",
      "time_since_restore: 64.63065338134766\n",
      "time_this_iter_s: 2.2059824466705322\n",
      "time_total_s: 64.63065338134766\n",
      "timers:\n",
      "  learn_throughput: 2.094\n",
      "  learn_time_ms: 1910.386\n",
      "  load_throughput: 2424.278\n",
      "  load_time_ms: 1.65\n",
      "  sample_throughput: 1.935\n",
      "  sample_time_ms: 2067.283\n",
      "  update_time_ms: 3.426\n",
      "timestamp: 1655985467\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 128\n",
      "training_iteration: 32\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 33 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 132\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-49\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872907416258293\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 132\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.656612873077394e-11\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.638048799832662\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0000395498839983e-05\n",
      "        policy_loss: -0.007182978590329488\n",
      "        total_loss: 0.22102899849414825\n",
      "        vf_explained_var: -4.76837158203125e-08\n",
      "        vf_loss: 0.228211976091067\n",
      "  num_agent_steps_sampled: 132\n",
      "  num_agent_steps_trained: 132\n",
      "  num_steps_sampled: 132\n",
      "  num_steps_trained: 132\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 33\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.56666666666667\n",
      "  ram_util_percent: 12.766666666666666\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13617560769561973\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.75213289991103\n",
      "  mean_inference_ms: 7.759028570472839\n",
      "  mean_raw_obs_processing_ms: 39.838294525795526\n",
      "time_since_restore: 66.80552220344543\n",
      "time_this_iter_s: 2.1748688220977783\n",
      "time_total_s: 66.80552220344543\n",
      "timers:\n",
      "  learn_throughput: 2.082\n",
      "  learn_time_ms: 1921.067\n",
      "  load_throughput: 2500.256\n",
      "  load_time_ms: 1.6\n",
      "  sample_throughput: 1.908\n",
      "  sample_time_ms: 2096.876\n",
      "  update_time_ms: 3.361\n",
      "timestamp: 1655985469\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 132\n",
      "training_iteration: 33\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 34 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 136\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-51\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07874285775960517\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 136\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.328306436538697e-11\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6377917925516763\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3762514997021451e-05\n",
      "        policy_loss: -0.007322574655214946\n",
      "        total_loss: 0.21650578280289967\n",
      "        vf_explained_var: -1.9868214925130207e-08\n",
      "        vf_loss: 0.22382834901412327\n",
      "  num_agent_steps_sampled: 136\n",
      "  num_agent_steps_trained: 136\n",
      "  num_steps_sampled: 136\n",
      "  num_steps_trained: 136\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 34\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.73333333333333\n",
      "  ram_util_percent: 12.800000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13456638446607522\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.90092148829866\n",
      "  mean_inference_ms: 7.828651018985327\n",
      "  mean_raw_obs_processing_ms: 39.941803270716534\n",
      "time_since_restore: 68.97628974914551\n",
      "time_this_iter_s: 2.1707675457000732\n",
      "time_total_s: 68.97628974914551\n",
      "timers:\n",
      "  learn_throughput: 2.051\n",
      "  learn_time_ms: 1949.941\n",
      "  load_throughput: 2572.877\n",
      "  load_time_ms: 1.555\n",
      "  sample_throughput: 1.897\n",
      "  sample_time_ms: 2108.043\n",
      "  update_time_ms: 3.449\n",
      "timestamp: 1655985471\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 136\n",
      "training_iteration: 34\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 35 of 500 -------\n",
      "agent_timesteps_total: 140\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-54\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0787428613383031\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 140\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1641532182693485e-11\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6375821272532147\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.463177416861678e-06\n",
      "        policy_loss: -5.724464232722918e-05\n",
      "        total_loss: 0.004665009553233782\n",
      "        vf_explained_var: -0.0008466243743896484\n",
      "        vf_loss: 0.004722254009296497\n",
      "  num_agent_steps_sampled: 140\n",
      "  num_agent_steps_trained: 140\n",
      "  num_steps_sampled: 140\n",
      "  num_steps_trained: 140\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 35\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.56666666666667\n",
      "  ram_util_percent: 12.800000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13322013830543317\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.96091925279937\n",
      "  mean_inference_ms: 7.896774154889195\n",
      "  mean_raw_obs_processing_ms: 40.03344968673523\n",
      "time_since_restore: 71.4156596660614\n",
      "time_this_iter_s: 2.4393699169158936\n",
      "time_total_s: 71.4156596660614\n",
      "timers:\n",
      "  learn_throughput: 2.008\n",
      "  learn_time_ms: 1992.454\n",
      "  load_throughput: 2460.111\n",
      "  load_time_ms: 1.626\n",
      "  sample_throughput: 1.87\n",
      "  sample_time_ms: 2139.304\n",
      "  update_time_ms: 3.416\n",
      "timestamp: 1655985474\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 140\n",
      "training_iteration: 35\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 36 of 500 -------\n",
      "agent_timesteps_total: 144\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-56\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07874276751629669\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 144\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.820766091346743e-12\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.637567377090454\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.266196239972487e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0043778385811795795\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0043778385811795795\n",
      "  num_agent_steps_sampled: 144\n",
      "  num_agent_steps_trained: 144\n",
      "  num_steps_sampled: 144\n",
      "  num_steps_trained: 144\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 36\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.975\n",
      "  ram_util_percent: 12.8\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13211225979226884\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 69.01586068254757\n",
      "  mean_inference_ms: 7.962236862288442\n",
      "  mean_raw_obs_processing_ms: 40.11661343250367\n",
      "time_since_restore: 73.83077001571655\n",
      "time_this_iter_s: 2.4151103496551514\n",
      "time_total_s: 73.83077001571655\n",
      "timers:\n",
      "  learn_throughput: 1.972\n",
      "  learn_time_ms: 2028.16\n",
      "  load_throughput: 2374.795\n",
      "  load_time_ms: 1.684\n",
      "  sample_throughput: 1.835\n",
      "  sample_time_ms: 2180.021\n",
      "  update_time_ms: 3.064\n",
      "timestamp: 1655985476\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 144\n",
      "training_iteration: 36\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 37 of 500 -------\n",
      "agent_timesteps_total: 148\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-57-58\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07874259174078033\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 148\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.9103830456733713e-12\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.637573997179667\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.0415953132348174e-08\n",
      "        policy_loss: -2.2793530176083245e-05\n",
      "        total_loss: 0.003877479986598094\n",
      "        vf_explained_var: 4.7087669372558594e-05\n",
      "        vf_loss: 0.003900273656472564\n",
      "  num_agent_steps_sampled: 148\n",
      "  num_agent_steps_trained: 148\n",
      "  num_steps_sampled: 148\n",
      "  num_steps_trained: 148\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 37\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.5\n",
      "  ram_util_percent: 12.800000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13112567985091692\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.91554846888151\n",
      "  mean_inference_ms: 8.02254496252936\n",
      "  mean_raw_obs_processing_ms: 40.16431543029924\n",
      "time_since_restore: 76.23334527015686\n",
      "time_this_iter_s: 2.4025752544403076\n",
      "time_total_s: 76.23334527015686\n",
      "timers:\n",
      "  learn_throughput: 1.938\n",
      "  learn_time_ms: 2063.5\n",
      "  load_throughput: 2474.041\n",
      "  load_time_ms: 1.617\n",
      "  sample_throughput: 1.807\n",
      "  sample_time_ms: 2213.836\n",
      "  update_time_ms: 3.441\n",
      "timestamp: 1655985478\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 148\n",
      "training_iteration: 37\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 38 of 500 -------\n",
      "agent_timesteps_total: 152\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-01\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07874259686538373\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 152\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.4551915228366857e-12\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.637571422259013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.2792593932147916e-07\n",
      "        policy_loss: -4.316301395495733e-05\n",
      "        total_loss: 0.00344563452526927\n",
      "        vf_explained_var: 0.0006324648857116699\n",
      "        vf_loss: 0.0034887977487718064\n",
      "  num_agent_steps_sampled: 152\n",
      "  num_agent_steps_trained: 152\n",
      "  num_steps_sampled: 152\n",
      "  num_steps_trained: 152\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 38\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.6\n",
      "  ram_util_percent: 12.8\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13021425714937127\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.79647373299551\n",
      "  mean_inference_ms: 8.077207818956227\n",
      "  mean_raw_obs_processing_ms: 40.26513164346687\n",
      "time_since_restore: 78.83667159080505\n",
      "time_this_iter_s: 2.6033263206481934\n",
      "time_total_s: 78.83667159080505\n",
      "timers:\n",
      "  learn_throughput: 1.925\n",
      "  learn_time_ms: 2078.342\n",
      "  load_throughput: 2534.246\n",
      "  load_time_ms: 1.578\n",
      "  sample_throughput: 1.75\n",
      "  sample_time_ms: 2285.91\n",
      "  update_time_ms: 3.736\n",
      "timestamp: 1655985481\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 152\n",
      "training_iteration: 38\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 39 of 500 -------\n",
      "agent_timesteps_total: 156\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-04\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0787425948885596\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 156\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.275957614183428e-13\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6375645637512206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6139683945463426e-06\n",
      "        policy_loss: -8.054024850328763e-05\n",
      "        total_loss: 0.003053149829308192\n",
      "        vf_explained_var: 0.0002878665924072266\n",
      "        vf_loss: 0.0031336902097488443\n",
      "  num_agent_steps_sampled: 156\n",
      "  num_agent_steps_trained: 156\n",
      "  num_steps_sampled: 156\n",
      "  num_steps_trained: 156\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 39\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.375\n",
      "  ram_util_percent: 12.8\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12933099578665966\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.70468263964605\n",
      "  mean_inference_ms: 8.121076161991606\n",
      "  mean_raw_obs_processing_ms: 40.365298639786964\n",
      "time_since_restore: 81.36387395858765\n",
      "time_this_iter_s: 2.5272023677825928\n",
      "time_total_s: 81.36387395858765\n",
      "timers:\n",
      "  learn_throughput: 1.878\n",
      "  learn_time_ms: 2129.727\n",
      "  load_throughput: 2473.239\n",
      "  load_time_ms: 1.617\n",
      "  sample_throughput: 1.736\n",
      "  sample_time_ms: 2304.64\n",
      "  update_time_ms: 3.948\n",
      "timestamp: 1655985484\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 156\n",
      "training_iteration: 39\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 40 of 500 -------\n",
      "agent_timesteps_total: 160\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-06\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0787426831208709\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 160\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.637978807091714e-13\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.637546006838481\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.75818841450382e-07\n",
      "        policy_loss: -0.0001055624025563399\n",
      "        total_loss: 0.0027191531844437122\n",
      "        vf_explained_var: 0.0001989901065826416\n",
      "        vf_loss: 0.002824715773264567\n",
      "  num_agent_steps_sampled: 160\n",
      "  num_agent_steps_trained: 160\n",
      "  num_steps_sampled: 160\n",
      "  num_steps_trained: 160\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 40\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.93333333333333\n",
      "  ram_util_percent: 12.800000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12857488542667692\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.64754294858977\n",
      "  mean_inference_ms: 8.164154946527178\n",
      "  mean_raw_obs_processing_ms: 40.481875461125796\n",
      "time_since_restore: 83.79779553413391\n",
      "time_this_iter_s: 2.4339215755462646\n",
      "time_total_s: 83.79779553413391\n",
      "timers:\n",
      "  learn_throughput: 1.842\n",
      "  learn_time_ms: 2171.495\n",
      "  load_throughput: 2594.522\n",
      "  load_time_ms: 1.542\n",
      "  sample_throughput: 1.693\n",
      "  sample_time_ms: 2362.236\n",
      "  update_time_ms: 4.541\n",
      "timestamp: 1655985486\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 160\n",
      "training_iteration: 40\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 41 of 500 -------\n",
      "agent_timesteps_total: 164\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-08\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07874268043294759\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 164\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.818989403545857e-13\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6375133117039997\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.075845739260937e-07\n",
      "        policy_loss: -6.306966145833334e-05\n",
      "        total_loss: 0.002492266210416953\n",
      "        vf_explained_var: 0.00047652522722880044\n",
      "        vf_loss: 0.0025553356080005567\n",
      "  num_agent_steps_sampled: 164\n",
      "  num_agent_steps_trained: 164\n",
      "  num_steps_sampled: 164\n",
      "  num_steps_trained: 164\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 41\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.35\n",
      "  ram_util_percent: 12.8\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1277260284659849\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.65860107610257\n",
      "  mean_inference_ms: 8.193880828044945\n",
      "  mean_raw_obs_processing_ms: 40.5995597599964\n",
      "time_since_restore: 86.27409768104553\n",
      "time_this_iter_s: 2.476302146911621\n",
      "time_total_s: 86.27409768104553\n",
      "timers:\n",
      "  learn_throughput: 1.866\n",
      "  learn_time_ms: 2143.934\n",
      "  load_throughput: 2570.669\n",
      "  load_time_ms: 1.556\n",
      "  sample_throughput: 1.644\n",
      "  sample_time_ms: 2432.373\n",
      "  update_time_ms: 4.473\n",
      "timestamp: 1655985488\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 164\n",
      "training_iteration: 41\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 42 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 168\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-11\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.07874277005196592\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 168\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.094947017729285e-14\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6373133579889934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3848242997482884e-05\n",
      "        policy_loss: -0.007804243763287862\n",
      "        total_loss: 0.2195387581984202\n",
      "        vf_explained_var: -4.3710072835286455e-08\n",
      "        vf_loss: 0.2273430054386457\n",
      "  num_agent_steps_sampled: 168\n",
      "  num_agent_steps_trained: 168\n",
      "  num_steps_sampled: 168\n",
      "  num_steps_trained: 168\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 42\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.766666666666666\n",
      "  ram_util_percent: 12.800000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12690291450455568\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.6858435715186\n",
      "  mean_inference_ms: 8.23328915203769\n",
      "  mean_raw_obs_processing_ms: 40.70030775990726\n",
      "time_since_restore: 88.75394773483276\n",
      "time_this_iter_s: 2.4798500537872314\n",
      "time_total_s: 88.75394773483276\n",
      "timers:\n",
      "  learn_throughput: 1.84\n",
      "  learn_time_ms: 2173.42\n",
      "  load_throughput: 2436.566\n",
      "  load_time_ms: 1.642\n",
      "  sample_throughput: 1.665\n",
      "  sample_time_ms: 2402.096\n",
      "  update_time_ms: 4.456\n",
      "timestamp: 1655985491\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 168\n",
      "training_iteration: 42\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 43 of 500 -------\n",
      "agent_timesteps_total: 172\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-14\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0787429581433341\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 172\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.547473508864643e-14\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6369086503982544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.483787425405931e-06\n",
      "        policy_loss: -0.00010638839254776637\n",
      "        total_loss: 0.0033098988234996794\n",
      "        vf_explained_var: 0.0003871917724609375\n",
      "        vf_loss: 0.003416287316940725\n",
      "  num_agent_steps_sampled: 172\n",
      "  num_agent_steps_trained: 172\n",
      "  num_steps_sampled: 172\n",
      "  num_steps_trained: 172\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 43\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.0\n",
      "  ram_util_percent: 12.8\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1262178261324392\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.7409793737374\n",
      "  mean_inference_ms: 8.277907292605125\n",
      "  mean_raw_obs_processing_ms: 40.8071416279568\n",
      "time_since_restore: 91.34579849243164\n",
      "time_this_iter_s: 2.591850757598877\n",
      "time_total_s: 91.34579849243164\n",
      "timers:\n",
      "  learn_throughput: 1.805\n",
      "  learn_time_ms: 2215.69\n",
      "  load_throughput: 2407.821\n",
      "  load_time_ms: 1.661\n",
      "  sample_throughput: 1.644\n",
      "  sample_time_ms: 2432.876\n",
      "  update_time_ms: 4.451\n",
      "timestamp: 1655985494\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 172\n",
      "training_iteration: 43\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 44 of 500 -------\n",
      "agent_timesteps_total: 176\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-16\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0687291693287742\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 176\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.2737367544323214e-14\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.636833135286967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.018044657850017e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0032139153219759463\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0032139153219759463\n",
      "  num_agent_steps_sampled: 176\n",
      "  num_agent_steps_trained: 176\n",
      "  num_steps_sampled: 176\n",
      "  num_steps_trained: 176\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 44\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.0\n",
      "  ram_util_percent: 12.8\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12577945383833117\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.83793426250506\n",
      "  mean_inference_ms: 8.325455626408937\n",
      "  mean_raw_obs_processing_ms: 41.01302918403286\n",
      "time_since_restore: 94.11184048652649\n",
      "time_this_iter_s: 2.7660419940948486\n",
      "time_total_s: 94.11184048652649\n",
      "timers:\n",
      "  learn_throughput: 1.79\n",
      "  learn_time_ms: 2234.361\n",
      "  load_throughput: 2217.859\n",
      "  load_time_ms: 1.804\n",
      "  sample_throughput: 1.59\n",
      "  sample_time_ms: 2515.372\n",
      "  update_time_ms: 4.653\n",
      "timestamp: 1655985496\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 176\n",
      "training_iteration: 44\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 45 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 180\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-19\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872907267580682\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 180\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1368683772161607e-14\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6366271575291953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.666192035238249e-05\n",
      "        policy_loss: -0.008640975753466288\n",
      "        total_loss: 0.21638192137082418\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 0.2250228946407636\n",
      "  num_agent_steps_sampled: 180\n",
      "  num_agent_steps_trained: 180\n",
      "  num_steps_sampled: 180\n",
      "  num_steps_trained: 180\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 45\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.56666666666666\n",
      "  ram_util_percent: 12.800000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12541016172325195\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 68.96677972514618\n",
      "  mean_inference_ms: 8.374738797701356\n",
      "  mean_raw_obs_processing_ms: 41.22025437940222\n",
      "time_since_restore: 96.44007396697998\n",
      "time_this_iter_s: 2.328233480453491\n",
      "time_total_s: 96.44007396697998\n",
      "timers:\n",
      "  learn_throughput: 1.799\n",
      "  learn_time_ms: 2223.016\n",
      "  load_throughput: 2246.908\n",
      "  load_time_ms: 1.78\n",
      "  sample_throughput: 1.578\n",
      "  sample_time_ms: 2535.354\n",
      "  update_time_ms: 4.743\n",
      "timestamp: 1655985499\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 180\n",
      "training_iteration: 45\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 46 of 500 -------\n",
      "agent_timesteps_total: 184\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-21\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872888854078701\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 184\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.684341886080803e-15\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6361157417297365\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.506563563055049e-06\n",
      "        policy_loss: -4.735284795363744e-05\n",
      "        total_loss: 0.004124947606275479\n",
      "        vf_explained_var: -0.00023228128751118978\n",
      "        vf_loss: 0.004172300593927503\n",
      "  num_agent_steps_sampled: 184\n",
      "  num_agent_steps_trained: 184\n",
      "  num_steps_sampled: 184\n",
      "  num_steps_trained: 184\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 46\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.150000000000006\n",
      "  ram_util_percent: 12.8\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12479156465940892\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 69.0766762535673\n",
      "  mean_inference_ms: 8.420463554414427\n",
      "  mean_raw_obs_processing_ms: 41.426617919494554\n",
      "time_since_restore: 98.91285419464111\n",
      "time_this_iter_s: 2.472780227661133\n",
      "time_total_s: 98.91285419464111\n",
      "timers:\n",
      "  learn_throughput: 1.796\n",
      "  learn_time_ms: 2227.413\n",
      "  load_throughput: 2307.606\n",
      "  load_time_ms: 1.733\n",
      "  sample_throughput: 1.584\n",
      "  sample_time_ms: 2525.667\n",
      "  update_time_ms: 4.722\n",
      "timestamp: 1655985501\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 184\n",
      "training_iteration: 46\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 47 of 500 -------\n",
      "agent_timesteps_total: 188\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-24\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872880113618988\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 188\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.8421709430404017e-15\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.636018737157186\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.1995525306313842e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0038868029757092395\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0038868029757092395\n",
      "  num_agent_steps_sampled: 188\n",
      "  num_agent_steps_trained: 188\n",
      "  num_steps_sampled: 188\n",
      "  num_steps_trained: 188\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 47\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.300000000000004\n",
      "  ram_util_percent: 12.800000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12422795523210889\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 69.20423177636506\n",
      "  mean_inference_ms: 8.456559928608252\n",
      "  mean_raw_obs_processing_ms: 41.63747274526076\n",
      "time_since_restore: 101.32179832458496\n",
      "time_this_iter_s: 2.4089441299438477\n",
      "time_total_s: 101.32179832458496\n",
      "timers:\n",
      "  learn_throughput: 1.798\n",
      "  learn_time_ms: 2224.65\n",
      "  load_throughput: 2261.629\n",
      "  load_time_ms: 1.769\n",
      "  sample_throughput: 1.579\n",
      "  sample_time_ms: 2532.832\n",
      "  update_time_ms: 4.444\n",
      "timestamp: 1655985504\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 188\n",
      "training_iteration: 47\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 48 of 500 -------\n",
      "agent_timesteps_total: 192\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-26\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0587151049199378\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 192\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.4210854715202008e-15\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.636000887552897\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.530338853532157e-07\n",
      "        policy_loss: -6.035277619957924e-05\n",
      "        total_loss: 0.0034071619932850203\n",
      "        vf_explained_var: 0.00068130095799764\n",
      "        vf_loss: 0.0034675147850066423\n",
      "  num_agent_steps_sampled: 192\n",
      "  num_agent_steps_trained: 192\n",
      "  num_steps_sampled: 192\n",
      "  num_steps_trained: 192\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 48\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.03333333333333\n",
      "  ram_util_percent: 13.1\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12373006790292945\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 69.37505097820753\n",
      "  mean_inference_ms: 8.494673354391812\n",
      "  mean_raw_obs_processing_ms: 41.86900988500088\n",
      "time_since_restore: 103.21819186210632\n",
      "time_this_iter_s: 1.8963935375213623\n",
      "time_total_s: 103.21819186210632\n",
      "timers:\n",
      "  learn_throughput: 1.84\n",
      "  learn_time_ms: 2173.976\n",
      "  load_throughput: 2230.034\n",
      "  load_time_ms: 1.794\n",
      "  sample_throughput: 1.593\n",
      "  sample_time_ms: 2511.693\n",
      "  update_time_ms: 4.162\n",
      "timestamp: 1655985506\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 192\n",
      "training_iteration: 48\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 49 of 500 -------\n",
      "agent_timesteps_total: 196\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-27\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.058715003624838155\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 196\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.105427357601004e-16\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6359585920969644\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.618069442580842e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0030993007744352023\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0030993007744352023\n",
      "  num_agent_steps_sampled: 196\n",
      "  num_agent_steps_trained: 196\n",
      "  num_steps_sampled: 196\n",
      "  num_steps_trained: 196\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 49\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 17.133333333333336\n",
      "  ram_util_percent: 13.166666666666666\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12327273338449685\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 69.53265796139233\n",
      "  mean_inference_ms: 8.532434088949918\n",
      "  mean_raw_obs_processing_ms: 42.09932351987331\n",
      "time_since_restore: 104.95431733131409\n",
      "time_this_iter_s: 1.7361254692077637\n",
      "time_total_s: 104.95431733131409\n",
      "timers:\n",
      "  learn_throughput: 1.902\n",
      "  learn_time_ms: 2103.452\n",
      "  load_throughput: 2253.609\n",
      "  load_time_ms: 1.775\n",
      "  sample_throughput: 1.632\n",
      "  sample_time_ms: 2451.579\n",
      "  update_time_ms: 4.044\n",
      "timestamp: 1655985507\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 196\n",
      "training_iteration: 49\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 50 of 500 -------\n",
      "agent_timesteps_total: 200\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-30\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870122170334059\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 200\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.552713678800502e-16\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6359647194544475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -3.592460302570544e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.002774067983652155\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.002774067983652155\n",
      "  num_agent_steps_sampled: 200\n",
      "  num_agent_steps_trained: 200\n",
      "  num_steps_sampled: 200\n",
      "  num_steps_trained: 200\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 50\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.666666666666664\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12286915330146972\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 69.74038380562453\n",
      "  mean_inference_ms: 8.569893562034817\n",
      "  mean_raw_obs_processing_ms: 42.32888345349114\n",
      "time_since_restore: 107.37697219848633\n",
      "time_this_iter_s: 2.422654867172241\n",
      "time_total_s: 107.37697219848633\n",
      "timers:\n",
      "  learn_throughput: 1.897\n",
      "  learn_time_ms: 2108.427\n",
      "  load_throughput: 2198.791\n",
      "  load_time_ms: 1.819\n",
      "  sample_throughput: 1.683\n",
      "  sample_time_ms: 2376.278\n",
      "  update_time_ms: 3.492\n",
      "timestamp: 1655985510\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 200\n",
      "training_iteration: 50\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 51 of 500 -------\n",
      "agent_timesteps_total: 204\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-32\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870113239007224\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 204\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.776356839400251e-16\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.635950779914856\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.2986868014812293e-07\n",
      "        policy_loss: -4.964085916678111e-05\n",
      "        total_loss: 0.002439621277153492\n",
      "        vf_explained_var: -0.00023000439008076987\n",
      "        vf_loss: 0.0024892620199049513\n",
      "  num_agent_steps_sampled: 204\n",
      "  num_agent_steps_trained: 204\n",
      "  num_steps_sampled: 204\n",
      "  num_steps_trained: 204\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 51\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.25\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12262239856278973\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 69.9286653366324\n",
      "  mean_inference_ms: 8.617872711418514\n",
      "  mean_raw_obs_processing_ms: 42.6273077487019\n",
      "time_since_restore: 109.96039199829102\n",
      "time_this_iter_s: 2.5834197998046875\n",
      "time_total_s: 109.96039199829102\n",
      "timers:\n",
      "  learn_throughput: 1.9\n",
      "  learn_time_ms: 2105.152\n",
      "  load_throughput: 2203.904\n",
      "  load_time_ms: 1.815\n",
      "  sample_throughput: 1.671\n",
      "  sample_time_ms: 2393.769\n",
      "  update_time_ms: 3.46\n",
      "timestamp: 1655985512\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 204\n",
      "training_iteration: 51\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 52 of 500 -------\n",
      "agent_timesteps_total: 208\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-35\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870104246398541\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 208\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 8.881784197001255e-17\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6358741998672484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.386376677857091e-06\n",
      "        policy_loss: -8.65562508503596e-05\n",
      "        total_loss: 0.002154056200136741\n",
      "        vf_explained_var: -2.580881118774414e-05\n",
      "        vf_loss: 0.002240612489792208\n",
      "  num_agent_steps_sampled: 208\n",
      "  num_agent_steps_trained: 208\n",
      "  num_steps_sampled: 208\n",
      "  num_steps_trained: 208\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 52\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.96666666666666\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12245586864770756\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 70.12002677325779\n",
      "  mean_inference_ms: 8.665665447507969\n",
      "  mean_raw_obs_processing_ms: 42.92158033986348\n",
      "time_since_restore: 112.10024976730347\n",
      "time_this_iter_s: 2.139857769012451\n",
      "time_total_s: 112.10024976730347\n",
      "timers:\n",
      "  learn_throughput: 1.93\n",
      "  learn_time_ms: 2072.467\n",
      "  load_throughput: 2063.085\n",
      "  load_time_ms: 1.939\n",
      "  sample_throughput: 1.674\n",
      "  sample_time_ms: 2389.027\n",
      "  update_time_ms: 3.413\n",
      "timestamp: 1655985515\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 208\n",
      "training_iteration: 52\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 53 of 500 -------\n",
      "agent_timesteps_total: 212\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-37\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870094928144107\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 212\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.4408920985006276e-17\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6358092069625854\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.592102823158105e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0020183938206173478\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0020183938206173478\n",
      "  num_agent_steps_sampled: 212\n",
      "  num_agent_steps_trained: 212\n",
      "  num_steps_sampled: 212\n",
      "  num_steps_trained: 212\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 53\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.233333333333334\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12232462881146416\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 70.3073898527868\n",
      "  mean_inference_ms: 8.713079595042105\n",
      "  mean_raw_obs_processing_ms: 43.20769706930995\n",
      "time_since_restore: 114.31413888931274\n",
      "time_this_iter_s: 2.2138891220092773\n",
      "time_total_s: 114.31413888931274\n",
      "timers:\n",
      "  learn_throughput: 1.969\n",
      "  learn_time_ms: 2031.944\n",
      "  load_throughput: 1751.842\n",
      "  load_time_ms: 2.283\n",
      "  sample_throughput: 1.697\n",
      "  sample_time_ms: 2357.127\n",
      "  update_time_ms: 3.578\n",
      "timestamp: 1655985517\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 212\n",
      "training_iteration: 53\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 54 of 500 -------\n",
      "agent_timesteps_total: 216\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-39\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.048701047663598694\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 216\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.2204460492503138e-17\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6358012199401855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6402107500349908e-07\n",
      "        policy_loss: -2.053690453370412e-05\n",
      "        total_loss: 0.0017972532970209917\n",
      "        vf_explained_var: -4.622936248779297e-05\n",
      "        vf_loss: 0.0018177901588690777\n",
      "  num_agent_steps_sampled: 216\n",
      "  num_agent_steps_trained: 216\n",
      "  num_steps_sampled: 216\n",
      "  num_steps_trained: 216\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 54\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.5\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12218962609724926\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 70.49199853622842\n",
      "  mean_inference_ms: 8.760448887330218\n",
      "  mean_raw_obs_processing_ms: 43.477466224927866\n",
      "time_since_restore: 116.31278371810913\n",
      "time_this_iter_s: 1.9986448287963867\n",
      "time_total_s: 116.31278371810913\n",
      "timers:\n",
      "  learn_throughput: 2.008\n",
      "  learn_time_ms: 1992.126\n",
      "  load_throughput: 1892.651\n",
      "  load_time_ms: 2.113\n",
      "  sample_throughput: 1.753\n",
      "  sample_time_ms: 2281.233\n",
      "  update_time_ms: 3.165\n",
      "timestamp: 1655985519\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 216\n",
      "training_iteration: 54\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 55 of 500 -------\n",
      "agent_timesteps_total: 220\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-41\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870112835174863\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 220\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1102230246251569e-17\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6357834815979\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.284325091750361e-07\n",
      "        policy_loss: -7.542921230196953e-05\n",
      "        total_loss: 0.0015640956349670886\n",
      "        vf_explained_var: 0.0001081228256225586\n",
      "        vf_loss: 0.001639524979206423\n",
      "  num_agent_steps_sampled: 220\n",
      "  num_agent_steps_trained: 220\n",
      "  num_steps_sampled: 220\n",
      "  num_steps_trained: 220\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 55\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.300000000000004\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12230492263028013\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 70.6849613538358\n",
      "  mean_inference_ms: 8.821731122686737\n",
      "  mean_raw_obs_processing_ms: 43.751420450503176\n",
      "time_since_restore: 118.62234473228455\n",
      "time_this_iter_s: 2.309561014175415\n",
      "time_total_s: 118.62234473228455\n",
      "timers:\n",
      "  learn_throughput: 2.015\n",
      "  learn_time_ms: 1985.013\n",
      "  load_throughput: 1886.989\n",
      "  load_time_ms: 2.12\n",
      "  sample_throughput: 1.781\n",
      "  sample_time_ms: 2245.433\n",
      "  update_time_ms: 2.913\n",
      "timestamp: 1655985521\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 220\n",
      "training_iteration: 55\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 56 of 500 -------\n",
      "agent_timesteps_total: 224\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-43\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870121644291922\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 224\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.5511151231257846e-18\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6357604265213013\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.333779161157774e-07\n",
      "        policy_loss: -4.487227027614911e-05\n",
      "        total_loss: 0.0014349809226890406\n",
      "        vf_explained_var: 0.0005638003349304199\n",
      "        vf_loss: 0.0014798532628143826\n",
      "  num_agent_steps_sampled: 224\n",
      "  num_agent_steps_trained: 224\n",
      "  num_steps_sampled: 224\n",
      "  num_steps_trained: 224\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 56\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.36666666666667\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12242090202778616\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 70.89757890041719\n",
      "  mean_inference_ms: 8.869366502538648\n",
      "  mean_raw_obs_processing_ms: 44.00956394502973\n",
      "time_since_restore: 120.73406267166138\n",
      "time_this_iter_s: 2.111717939376831\n",
      "time_total_s: 120.73406267166138\n",
      "timers:\n",
      "  learn_throughput: 2.05\n",
      "  learn_time_ms: 1950.949\n",
      "  load_throughput: 1897.317\n",
      "  load_time_ms: 2.108\n",
      "  sample_throughput: 1.79\n",
      "  sample_time_ms: 2234.316\n",
      "  update_time_ms: 2.782\n",
      "timestamp: 1655985523\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 224\n",
      "training_iteration: 56\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 57 of 500 -------\n",
      "agent_timesteps_total: 228\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-46\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.038687520058424726\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 228\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.7755575615628923e-18\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6357207457224527\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.700216296920189e-07\n",
      "        policy_loss: -4.9321819096803667e-05\n",
      "        total_loss: 0.0012866369759043058\n",
      "        vf_explained_var: 0.0003167390823364258\n",
      "        vf_loss: 0.0013359588376867274\n",
      "  num_agent_steps_sampled: 228\n",
      "  num_agent_steps_trained: 228\n",
      "  num_steps_sampled: 228\n",
      "  num_steps_trained: 228\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 57\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.675000000000004\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12277750721277374\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 71.1103830778798\n",
      "  mean_inference_ms: 8.925797200602576\n",
      "  mean_raw_obs_processing_ms: 44.25850557904745\n",
      "time_since_restore: 123.0765221118927\n",
      "time_this_iter_s: 2.3424594402313232\n",
      "time_total_s: 123.0765221118927\n",
      "timers:\n",
      "  learn_throughput: 2.058\n",
      "  learn_time_ms: 1943.582\n",
      "  load_throughput: 1856.38\n",
      "  load_time_ms: 2.155\n",
      "  sample_throughput: 1.816\n",
      "  sample_time_ms: 2202.462\n",
      "  update_time_ms: 2.628\n",
      "timestamp: 1655985526\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 228\n",
      "training_iteration: 57\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 58 of 500 -------\n",
      "agent_timesteps_total: 232\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-48\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673725297841048\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 232\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3877787807814461e-18\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.635656960805257\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.778031907591261e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0012066153848233321\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0012066153848233321\n",
      "  num_agent_steps_sampled: 232\n",
      "  num_agent_steps_trained: 232\n",
      "  num_steps_sampled: 232\n",
      "  num_steps_trained: 232\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 58\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.333333333333336\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1231263173972867\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 71.33452342434606\n",
      "  mean_inference_ms: 8.975132515371314\n",
      "  mean_raw_obs_processing_ms: 44.494726237170056\n",
      "time_since_restore: 125.31302833557129\n",
      "time_this_iter_s: 2.236506223678589\n",
      "time_total_s: 125.31302833557129\n",
      "timers:\n",
      "  learn_throughput: 2.007\n",
      "  learn_time_ms: 1993.453\n",
      "  load_throughput: 1794.969\n",
      "  load_time_ms: 2.228\n",
      "  sample_throughput: 1.837\n",
      "  sample_time_ms: 2177.673\n",
      "  update_time_ms: 2.615\n",
      "timestamp: 1655985528\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 232\n",
      "training_iteration: 58\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 59 of 500 -------\n",
      "agent_timesteps_total: 236\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-50\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018660033854698922\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 236\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 6.938893903907231e-19\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.635643800099691\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -6.917933698484072e-08\n",
      "        policy_loss: -2.088105926911036e-05\n",
      "        total_loss: 0.0010666841951509317\n",
      "        vf_explained_var: 0.00042894879976908366\n",
      "        vf_loss: 0.0010875651612877845\n",
      "  num_agent_steps_sampled: 236\n",
      "  num_agent_steps_trained: 236\n",
      "  num_steps_sampled: 236\n",
      "  num_steps_trained: 236\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 59\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.46666666666667\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12352846709726988\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 71.57674925297096\n",
      "  mean_inference_ms: 9.027630645942589\n",
      "  mean_raw_obs_processing_ms: 44.732262474523424\n",
      "time_since_restore: 127.6484866142273\n",
      "time_this_iter_s: 2.335458278656006\n",
      "time_total_s: 127.6484866142273\n",
      "timers:\n",
      "  learn_throughput: 1.952\n",
      "  learn_time_ms: 2049.543\n",
      "  load_throughput: 1771.692\n",
      "  load_time_ms: 2.258\n",
      "  sample_throughput: 1.792\n",
      "  sample_time_ms: 2232.481\n",
      "  update_time_ms: 2.473\n",
      "timestamp: 1655985530\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 236\n",
      "training_iteration: 59\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 60 of 500 -------\n",
      "agent_timesteps_total: 240\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-53\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018660214164270616\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 240\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.4694469519536153e-19\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.635585920015971\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.153526742816514e-06\n",
      "        policy_loss: -0.00020055097217361133\n",
      "        total_loss: 0.0007798939322431882\n",
      "        vf_explained_var: 1.0752677917480469e-05\n",
      "        vf_loss: 0.000980444832627351\n",
      "  num_agent_steps_sampled: 240\n",
      "  num_agent_steps_trained: 240\n",
      "  num_steps_sampled: 240\n",
      "  num_steps_trained: 240\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 60\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.75\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12392188815224195\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 71.80985417570334\n",
      "  mean_inference_ms: 9.079899270465454\n",
      "  mean_raw_obs_processing_ms: 44.97409783505915\n",
      "time_since_restore: 130.06205487251282\n",
      "time_this_iter_s: 2.4135682582855225\n",
      "time_total_s: 130.06205487251282\n",
      "timers:\n",
      "  learn_throughput: 1.954\n",
      "  learn_time_ms: 2046.839\n",
      "  load_throughput: 1787.054\n",
      "  load_time_ms: 2.238\n",
      "  sample_throughput: 1.746\n",
      "  sample_time_ms: 2291.108\n",
      "  update_time_ms: 2.429\n",
      "timestamp: 1655985533\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 240\n",
      "training_iteration: 60\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 61 of 500 -------\n",
      "agent_timesteps_total: 244\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-55\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.01866031027568301\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 244\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7347234759768077e-19\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.635488200187683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6376628385235865e-06\n",
      "        policy_loss: -4.312715803583463e-05\n",
      "        total_loss: 0.0008406410304208597\n",
      "        vf_explained_var: -3.218650817871094e-05\n",
      "        vf_loss: 0.0008837683611394217\n",
      "  num_agent_steps_sampled: 244\n",
      "  num_agent_steps_trained: 244\n",
      "  num_steps_sampled: 244\n",
      "  num_steps_trained: 244\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 61\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.6\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12430473704817877\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 72.03900670403047\n",
      "  mean_inference_ms: 9.13113798398351\n",
      "  mean_raw_obs_processing_ms: 45.21640493804074\n",
      "time_since_restore: 132.31113457679749\n",
      "time_this_iter_s: 2.249079704284668\n",
      "time_total_s: 132.31113457679749\n",
      "timers:\n",
      "  learn_throughput: 1.95\n",
      "  learn_time_ms: 2051.734\n",
      "  load_throughput: 1667.069\n",
      "  load_time_ms: 2.399\n",
      "  sample_throughput: 1.779\n",
      "  sample_time_ms: 2247.995\n",
      "  update_time_ms: 2.629\n",
      "timestamp: 1655985535\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 244\n",
      "training_iteration: 61\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 62 of 500 -------\n",
      "agent_timesteps_total: 248\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-57\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018660211328987996\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 248\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 8.673617379884038e-20\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6354652007420856\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.311030761532796e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0007956778494796406\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0007956778494796406\n",
      "  num_agent_steps_sampled: 248\n",
      "  num_agent_steps_trained: 248\n",
      "  num_steps_sampled: 248\n",
      "  num_steps_trained: 248\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 62\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.23333333333334\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12473932412367596\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 72.26501692462107\n",
      "  mean_inference_ms: 9.189879502711628\n",
      "  mean_raw_obs_processing_ms: 45.45833809780122\n",
      "time_since_restore: 134.56524205207825\n",
      "time_this_iter_s: 2.2541074752807617\n",
      "time_total_s: 134.56524205207825\n",
      "timers:\n",
      "  learn_throughput: 1.94\n",
      "  learn_time_ms: 2062.174\n",
      "  load_throughput: 1680.612\n",
      "  load_time_ms: 2.38\n",
      "  sample_throughput: 1.774\n",
      "  sample_time_ms: 2255.085\n",
      "  update_time_ms: 2.999\n",
      "timestamp: 1655985537\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 248\n",
      "training_iteration: 62\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 63 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 252\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-58-59\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.02867380955310577\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 252\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.336808689942019e-20\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6350320180257163\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.0800846811137227e-05\n",
      "        policy_loss: -0.01115752359231313\n",
      "        total_loss: 0.22511454224586486\n",
      "        vf_explained_var: 4.76837158203125e-08\n",
      "        vf_loss: 0.23627206732829412\n",
      "  num_agent_steps_sampled: 252\n",
      "  num_agent_steps_trained: 252\n",
      "  num_steps_sampled: 252\n",
      "  num_steps_trained: 252\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 63\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.36666666666667\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1251915409338168\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 72.48072632414852\n",
      "  mean_inference_ms: 9.249158843675724\n",
      "  mean_raw_obs_processing_ms: 45.61672098983674\n",
      "time_since_restore: 136.69432854652405\n",
      "time_this_iter_s: 2.129086494445801\n",
      "time_total_s: 136.69432854652405\n",
      "timers:\n",
      "  learn_throughput: 1.952\n",
      "  learn_time_ms: 2049.494\n",
      "  load_throughput: 1824.047\n",
      "  load_time_ms: 2.193\n",
      "  sample_throughput: 1.762\n",
      "  sample_time_ms: 2270.2\n",
      "  update_time_ms: 2.951\n",
      "timestamp: 1655985539\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 252\n",
      "training_iteration: 63\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 64 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 256\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-02\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.048701102514835615\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 256\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.1684043449710096e-20\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6335107644399005\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.054387751850299e-05\n",
      "        policy_loss: -0.02083012064297994\n",
      "        total_loss: 0.4373426040013631\n",
      "        vf_explained_var: -2.384185791015625e-08\n",
      "        vf_loss: 0.4581727276245753\n",
      "  num_agent_steps_sampled: 256\n",
      "  num_agent_steps_trained: 256\n",
      "  num_steps_sampled: 256\n",
      "  num_steps_trained: 256\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 64\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.43333333333333\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12573939014881078\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 72.64566166998165\n",
      "  mean_inference_ms: 9.316904556762955\n",
      "  mean_raw_obs_processing_ms: 45.76422081707497\n",
      "time_since_restore: 138.81146144866943\n",
      "time_this_iter_s: 2.1171329021453857\n",
      "time_total_s: 138.81146144866943\n",
      "timers:\n",
      "  learn_throughput: 1.937\n",
      "  learn_time_ms: 2065.042\n",
      "  load_throughput: 1818.866\n",
      "  load_time_ms: 2.199\n",
      "  sample_throughput: 1.774\n",
      "  sample_time_ms: 2254.236\n",
      "  update_time_ms: 2.942\n",
      "timestamp: 1655985542\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 256\n",
      "training_iteration: 64\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 65 of 500 -------\n",
      "agent_timesteps_total: 260\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-04\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.048700916323738956\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 260\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0842021724855048e-20\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6321298281351724\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2108745189228405e-05\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0036363597493618725\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0036363597493618725\n",
      "  num_agent_steps_sampled: 260\n",
      "  num_agent_steps_trained: 260\n",
      "  num_steps_sampled: 260\n",
      "  num_steps_trained: 260\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 65\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.95\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1263561656206243\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 72.7922706376323\n",
      "  mean_inference_ms: 9.390192832354064\n",
      "  mean_raw_obs_processing_ms: 45.90321692706915\n",
      "time_since_restore: 141.2959291934967\n",
      "time_this_iter_s: 2.4844677448272705\n",
      "time_total_s: 141.2959291934967\n",
      "timers:\n",
      "  learn_throughput: 1.921\n",
      "  learn_time_ms: 2081.74\n",
      "  load_throughput: 1675.543\n",
      "  load_time_ms: 2.387\n",
      "  sample_throughput: 1.762\n",
      "  sample_time_ms: 2270.53\n",
      "  update_time_ms: 3.009\n",
      "timestamp: 1655985544\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 260\n",
      "training_iteration: 65\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 66 of 500 -------\n",
      "agent_timesteps_total: 264\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-06\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870092188161013\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 264\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.421010862427524e-21\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6318928082784017\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.86468024973874e-07\n",
      "        policy_loss: -7.735323160886764e-05\n",
      "        total_loss: 0.003474870945016543\n",
      "        vf_explained_var: 0.00033152103424072266\n",
      "        vf_loss: 0.0035522241611033677\n",
      "  num_agent_steps_sampled: 264\n",
      "  num_agent_steps_trained: 264\n",
      "  num_steps_sampled: 264\n",
      "  num_steps_trained: 264\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 66\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.733333333333334\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12697492876682479\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 72.86287000686573\n",
      "  mean_inference_ms: 9.462676313305066\n",
      "  mean_raw_obs_processing_ms: 46.04504453647771\n",
      "time_since_restore: 143.57996249198914\n",
      "time_this_iter_s: 2.2840332984924316\n",
      "time_total_s: 143.57996249198914\n",
      "timers:\n",
      "  learn_throughput: 1.908\n",
      "  learn_time_ms: 2096.761\n",
      "  load_throughput: 1669.126\n",
      "  load_time_ms: 2.396\n",
      "  sample_throughput: 1.747\n",
      "  sample_time_ms: 2290.221\n",
      "  update_time_ms: 2.952\n",
      "timestamp: 1655985546\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 264\n",
      "training_iteration: 66\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 67 of 500 -------\n",
      "agent_timesteps_total: 268\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-09\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.038687138342832876\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 268\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.710505431213762e-21\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6317713340123494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1344681241827932e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0032130676864956815\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0032130676864956815\n",
      "  num_agent_steps_sampled: 268\n",
      "  num_agent_steps_trained: 268\n",
      "  num_steps_sampled: 268\n",
      "  num_steps_trained: 268\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 67\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 36.225\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1274989481790625\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 72.96938105113968\n",
      "  mean_inference_ms: 9.524210005739352\n",
      "  mean_raw_obs_processing_ms: 46.17560840322737\n",
      "time_since_restore: 145.95027494430542\n",
      "time_this_iter_s: 2.370312452316284\n",
      "time_total_s: 145.95027494430542\n",
      "timers:\n",
      "  learn_throughput: 1.905\n",
      "  learn_time_ms: 2099.907\n",
      "  load_throughput: 1585.493\n",
      "  load_time_ms: 2.523\n",
      "  sample_throughput: 1.738\n",
      "  sample_time_ms: 2302.053\n",
      "  update_time_ms: 3.228\n",
      "timestamp: 1655985549\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 268\n",
      "training_iteration: 67\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 68 of 500 -------\n",
      "agent_timesteps_total: 272\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-11\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868704842781126\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 272\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.355252715606881e-21\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6317530234654742\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.355466351242588e-07\n",
      "        policy_loss: -3.448432932297389e-05\n",
      "        total_loss: 0.002861831771830718\n",
      "        vf_explained_var: 0.00036887327829996747\n",
      "        vf_loss: 0.0028963160545875627\n",
      "  num_agent_steps_sampled: 272\n",
      "  num_agent_steps_trained: 272\n",
      "  num_steps_sampled: 272\n",
      "  num_steps_trained: 272\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 68\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.666666666666664\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1279862691581846\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.06662677198923\n",
      "  mean_inference_ms: 9.579763638501943\n",
      "  mean_raw_obs_processing_ms: 46.30652321808594\n",
      "time_since_restore: 148.2762839794159\n",
      "time_this_iter_s: 2.3260090351104736\n",
      "time_total_s: 148.2762839794159\n",
      "timers:\n",
      "  learn_throughput: 1.895\n",
      "  learn_time_ms: 2111.088\n",
      "  load_throughput: 1631.073\n",
      "  load_time_ms: 2.452\n",
      "  sample_throughput: 1.734\n",
      "  sample_time_ms: 2306.222\n",
      "  update_time_ms: 3.235\n",
      "timestamp: 1655985551\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 272\n",
      "training_iteration: 68\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 69 of 500 -------\n",
      "agent_timesteps_total: 276\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-13\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868723831903973\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 276\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 6.776263578034405e-22\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.631674790382385\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1480525441716812e-06\n",
      "        policy_loss: -9.435868511597316e-05\n",
      "        total_loss: 0.0025226719367007415\n",
      "        vf_explained_var: 0.0005183021227518717\n",
      "        vf_loss: 0.0026170306218167147\n",
      "  num_agent_steps_sampled: 276\n",
      "  num_agent_steps_trained: 276\n",
      "  num_steps_sampled: 276\n",
      "  num_steps_trained: 276\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 69\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.6\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12842746841544486\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.16574553453535\n",
      "  mean_inference_ms: 9.638919829283312\n",
      "  mean_raw_obs_processing_ms: 46.35190142624634\n",
      "time_since_restore: 150.56561517715454\n",
      "time_this_iter_s: 2.2893311977386475\n",
      "time_total_s: 150.56561517715454\n",
      "timers:\n",
      "  learn_throughput: 1.909\n",
      "  learn_time_ms: 2094.977\n",
      "  load_throughput: 1600.971\n",
      "  load_time_ms: 2.498\n",
      "  sample_throughput: 1.72\n",
      "  sample_time_ms: 2325.909\n",
      "  update_time_ms: 3.303\n",
      "timestamp: 1655985553\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 276\n",
      "training_iteration: 69\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 70 of 500 -------\n",
      "agent_timesteps_total: 280\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-16\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673636760267324\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 280\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.3881317890172025e-22\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.631536118189494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.015739361619732e-06\n",
      "        policy_loss: -8.759219199419021e-05\n",
      "        total_loss: 0.0022813753535350163\n",
      "        vf_explained_var: 0.0003965854644775391\n",
      "        vf_loss: 0.0023689676464224855\n",
      "  num_agent_steps_sampled: 280\n",
      "  num_agent_steps_trained: 280\n",
      "  num_steps_sampled: 280\n",
      "  num_steps_trained: 280\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 70\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.199999999999996\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12885710073899087\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.29584621402051\n",
      "  mean_inference_ms: 9.695669430554007\n",
      "  mean_raw_obs_processing_ms: 46.399026083348744\n",
      "time_since_restore: 152.84264874458313\n",
      "time_this_iter_s: 2.277033567428589\n",
      "time_total_s: 152.84264874458313\n",
      "timers:\n",
      "  learn_throughput: 1.92\n",
      "  learn_time_ms: 2082.829\n",
      "  load_throughput: 1604.094\n",
      "  load_time_ms: 2.494\n",
      "  sample_throughput: 1.732\n",
      "  sample_time_ms: 2309.703\n",
      "  update_time_ms: 3.312\n",
      "timestamp: 1655985556\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 280\n",
      "training_iteration: 70\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 71 of 500 -------\n",
      "agent_timesteps_total: 284\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-18\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673633624082408\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 284\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6940658945086012e-22\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.631407316525777\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4622084563598036e-06\n",
      "        policy_loss: -9.08497100075086e-05\n",
      "        total_loss: 0.0020548699423670767\n",
      "        vf_explained_var: -2.5494893391927084e-05\n",
      "        vf_loss: 0.002145719473871092\n",
      "  num_agent_steps_sampled: 284\n",
      "  num_agent_steps_trained: 284\n",
      "  num_steps_sampled: 284\n",
      "  num_steps_trained: 284\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 71\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.05\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12927949617011264\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.43461393286658\n",
      "  mean_inference_ms: 9.750650502170465\n",
      "  mean_raw_obs_processing_ms: 46.43863314223344\n",
      "time_since_restore: 155.09191226959229\n",
      "time_this_iter_s: 2.2492635250091553\n",
      "time_total_s: 155.09191226959229\n",
      "timers:\n",
      "  learn_throughput: 1.921\n",
      "  learn_time_ms: 2082.214\n",
      "  load_throughput: 1671.454\n",
      "  load_time_ms: 2.393\n",
      "  sample_throughput: 1.74\n",
      "  sample_time_ms: 2298.752\n",
      "  update_time_ms: 3.451\n",
      "timestamp: 1655985558\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 284\n",
      "training_iteration: 71\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 72 of 500 -------\n",
      "agent_timesteps_total: 288\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-20\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673633623919087\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 288\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 8.470329472543006e-23\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.631349531809489\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.2465907629036034e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0019434450039019187\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0019434450039019187\n",
      "  num_agent_steps_sampled: 288\n",
      "  num_agent_steps_trained: 288\n",
      "  num_steps_sampled: 288\n",
      "  num_steps_trained: 288\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 72\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.9\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12968398853092877\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.55804536301807\n",
      "  mean_inference_ms: 9.798654910031384\n",
      "  mean_raw_obs_processing_ms: 46.473565424106425\n",
      "time_since_restore: 157.2537271976471\n",
      "time_this_iter_s: 2.1618149280548096\n",
      "time_total_s: 157.2537271976471\n",
      "timers:\n",
      "  learn_throughput: 1.927\n",
      "  learn_time_ms: 2075.445\n",
      "  load_throughput: 1746.517\n",
      "  load_time_ms: 2.29\n",
      "  sample_throughput: 1.743\n",
      "  sample_time_ms: 2295.054\n",
      "  update_time_ms: 3.318\n",
      "timestamp: 1655985560\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 288\n",
      "training_iteration: 72\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 73 of 500 -------\n",
      "agent_timesteps_total: 292\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-22\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.02867354631368056\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 292\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.235164736271503e-23\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.63135613600413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -4.9313268550103356e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0017587362322956323\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0017587362322956323\n",
      "  num_agent_steps_sampled: 292\n",
      "  num_agent_steps_trained: 292\n",
      "  num_steps_sampled: 292\n",
      "  num_steps_trained: 292\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 73\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.53333333333333\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13005221964835828\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.65086404810363\n",
      "  mean_inference_ms: 9.844466051088089\n",
      "  mean_raw_obs_processing_ms: 46.50870091761902\n",
      "time_since_restore: 159.3764100074768\n",
      "time_this_iter_s: 2.122682809829712\n",
      "time_total_s: 159.3764100074768\n",
      "timers:\n",
      "  learn_throughput: 1.922\n",
      "  learn_time_ms: 2081.4\n",
      "  load_throughput: 1884.107\n",
      "  load_time_ms: 2.123\n",
      "  sample_throughput: 1.751\n",
      "  sample_time_ms: 2284.073\n",
      "  update_time_ms: 3.266\n",
      "timestamp: 1655985562\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 292\n",
      "training_iteration: 73\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 74 of 500 -------\n",
      "agent_timesteps_total: 296\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-24\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673726243636774\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 296\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.1175823681357516e-23\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.631307681401571\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.739740451346734e-07\n",
      "        policy_loss: -8.433454980452856e-05\n",
      "        total_loss: 0.0015084206437071164\n",
      "        vf_explained_var: 0.00028029878934224446\n",
      "        vf_loss: 0.001592755620367825\n",
      "  num_agent_steps_sampled: 296\n",
      "  num_agent_steps_trained: 296\n",
      "  num_steps_sampled: 296\n",
      "  num_steps_trained: 296\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 74\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.399999999999995\n",
      "  ram_util_percent: 13.333333333333334\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13047426821708386\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.78701895405861\n",
      "  mean_inference_ms: 9.89498243394128\n",
      "  mean_raw_obs_processing_ms: 46.56825036054605\n",
      "time_since_restore: 161.30732464790344\n",
      "time_this_iter_s: 1.9309146404266357\n",
      "time_total_s: 161.30732464790344\n",
      "timers:\n",
      "  learn_throughput: 1.959\n",
      "  learn_time_ms: 2042.259\n",
      "  load_throughput: 1904.1\n",
      "  load_time_ms: 2.101\n",
      "  sample_throughput: 1.732\n",
      "  sample_time_ms: 2309.229\n",
      "  update_time_ms: 3.291\n",
      "timestamp: 1655985564\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 296\n",
      "training_iteration: 74\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 75 of 500 -------\n",
      "agent_timesteps_total: 300\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-26\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.02867372624347345\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 300\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0587911840678758e-23\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.631177608172099\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.8599782429713136e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0014446875196881593\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0014446875196881593\n",
      "  num_agent_steps_sampled: 300\n",
      "  num_agent_steps_trained: 300\n",
      "  num_steps_sampled: 300\n",
      "  num_steps_trained: 300\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 75\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 7.55\n",
      "  ram_util_percent: 13.15\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13088980301307523\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.88492968083024\n",
      "  mean_inference_ms: 9.945084990517335\n",
      "  mean_raw_obs_processing_ms: 46.627871876080896\n",
      "time_since_restore: 162.90454125404358\n",
      "time_this_iter_s: 1.5972166061401367\n",
      "time_total_s: 162.90454125404358\n",
      "timers:\n",
      "  learn_throughput: 2.034\n",
      "  learn_time_ms: 1966.545\n",
      "  load_throughput: 2121.389\n",
      "  load_time_ms: 1.886\n",
      "  sample_throughput: 1.772\n",
      "  sample_time_ms: 2257.603\n",
      "  update_time_ms: 3.17\n",
      "timestamp: 1655985566\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 300\n",
      "training_iteration: 75\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 76 of 500 -------\n",
      "agent_timesteps_total: 304\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-28\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673627266150123\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 304\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.293955920339379e-24\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.631169398625692\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.2345426839081786e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0013076752540655435\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0013076752540655435\n",
      "  num_agent_steps_sampled: 304\n",
      "  num_agent_steps_trained: 304\n",
      "  num_steps_sampled: 304\n",
      "  num_steps_trained: 304\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 76\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.333333333333336\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13120682529190572\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.98534632077465\n",
      "  mean_inference_ms: 9.984081497278815\n",
      "  mean_raw_obs_processing_ms: 46.61605612937625\n",
      "time_since_restore: 165.0356433391571\n",
      "time_this_iter_s: 2.1311020851135254\n",
      "time_total_s: 165.0356433391571\n",
      "timers:\n",
      "  learn_throughput: 2.054\n",
      "  learn_time_ms: 1947.151\n",
      "  load_throughput: 2121.63\n",
      "  load_time_ms: 1.885\n",
      "  sample_throughput: 1.831\n",
      "  sample_time_ms: 2185.186\n",
      "  update_time_ms: 3.217\n",
      "timestamp: 1655985568\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 304\n",
      "training_iteration: 76\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 77 of 500 -------\n",
      "agent_timesteps_total: 308\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-30\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673531109820013\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 308\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.6469779601696894e-24\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6311797857284547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4458402710555067e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.001183082714366416\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.001183082714366416\n",
      "  num_agent_steps_sampled: 308\n",
      "  num_agent_steps_trained: 308\n",
      "  num_steps_sampled: 308\n",
      "  num_steps_trained: 308\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 77\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.4\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1314728229141922\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.08803159834163\n",
      "  mean_inference_ms: 10.02180006046938\n",
      "  mean_raw_obs_processing_ms: 46.614667933450725\n",
      "time_since_restore: 167.1815469264984\n",
      "time_this_iter_s: 2.1459035873413086\n",
      "time_total_s: 167.1815469264984\n",
      "timers:\n",
      "  learn_throughput: 2.079\n",
      "  learn_time_ms: 1924.269\n",
      "  load_throughput: 2146.879\n",
      "  load_time_ms: 1.863\n",
      "  sample_throughput: 1.846\n",
      "  sample_time_ms: 2166.498\n",
      "  update_time_ms: 3.019\n",
      "timestamp: 1655985570\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 308\n",
      "training_iteration: 77\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 78 of 500 -------\n",
      "agent_timesteps_total: 312\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-33\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673531109874452\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 312\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.3234889800848447e-24\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.631189791361491\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.3489622809809892e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0010699842668448886\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0010699842668448886\n",
      "  num_agent_steps_sampled: 312\n",
      "  num_agent_steps_trained: 312\n",
      "  num_steps_sampled: 312\n",
      "  num_steps_trained: 312\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 78\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.025\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1317745481649514\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.19196641663271\n",
      "  mean_inference_ms: 10.06067306632163\n",
      "  mean_raw_obs_processing_ms: 46.61969637968669\n",
      "time_since_restore: 169.64123344421387\n",
      "time_this_iter_s: 2.459686517715454\n",
      "time_total_s: 169.64123344421387\n",
      "timers:\n",
      "  learn_throughput: 2.074\n",
      "  learn_time_ms: 1928.266\n",
      "  load_throughput: 2208.022\n",
      "  load_time_ms: 1.812\n",
      "  sample_throughput: 1.862\n",
      "  sample_time_ms: 2147.979\n",
      "  update_time_ms: 3.328\n",
      "timestamp: 1655985573\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 312\n",
      "training_iteration: 78\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 79 of 500 -------\n",
      "agent_timesteps_total: 316\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-35\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673618291075925\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 316\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 6.617444900424224e-25\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6311492284138995\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.657044842640365e-07\n",
      "        policy_loss: -9.474760542313257e-05\n",
      "        total_loss: 0.0008729775125781695\n",
      "        vf_explained_var: 0.00011616150538126627\n",
      "        vf_loss: 0.0009677251645674308\n",
      "  num_agent_steps_sampled: 316\n",
      "  num_agent_steps_trained: 316\n",
      "  num_steps_sampled: 316\n",
      "  num_steps_trained: 316\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 79\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.833333333333336\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13200117315229354\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.30065369282266\n",
      "  mean_inference_ms: 10.095063922240861\n",
      "  mean_raw_obs_processing_ms: 46.62395335967234\n",
      "time_since_restore: 171.95751929283142\n",
      "time_this_iter_s: 2.3162858486175537\n",
      "time_total_s: 171.95751929283142\n",
      "timers:\n",
      "  learn_throughput: 2.062\n",
      "  learn_time_ms: 1940.09\n",
      "  load_throughput: 2281.776\n",
      "  load_time_ms: 1.753\n",
      "  sample_throughput: 1.862\n",
      "  sample_time_ms: 2147.905\n",
      "  update_time_ms: 3.18\n",
      "timestamp: 1655985575\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 316\n",
      "training_iteration: 79\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 80 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 320\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-37\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868723648619357\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 320\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.308722450212112e-25\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6304284811019896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.926964373211679e-05\n",
      "        policy_loss: -0.01252899666627248\n",
      "        total_loss: 0.2225481390953064\n",
      "        vf_explained_var: -7.947285970052083e-09\n",
      "        vf_loss: 0.23507712930440902\n",
      "  num_agent_steps_sampled: 320\n",
      "  num_agent_steps_trained: 320\n",
      "  num_steps_sampled: 320\n",
      "  num_steps_trained: 320\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 80\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.3\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1320167476252154\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.38902714855904\n",
      "  mean_inference_ms: 10.115248162674556\n",
      "  mean_raw_obs_processing_ms: 46.62398531491031\n",
      "time_since_restore: 174.45482897758484\n",
      "time_this_iter_s: 2.497309684753418\n",
      "time_total_s: 174.45482897758484\n",
      "timers:\n",
      "  learn_throughput: 2.039\n",
      "  learn_time_ms: 1962.215\n",
      "  load_throughput: 2269.399\n",
      "  load_time_ms: 1.763\n",
      "  sample_throughput: 1.854\n",
      "  sample_time_ms: 2157.704\n",
      "  update_time_ms: 3.242\n",
      "timestamp: 1655985577\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 320\n",
      "training_iteration: 80\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 81 of 500 -------\n",
      "agent_timesteps_total: 324\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-40\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868723194727916\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 324\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.654361225106056e-25\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.628810087839762\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6692797374151525e-05\n",
      "        policy_loss: -0.0001686659331123034\n",
      "        total_loss: 0.0014559165885051093\n",
      "        vf_explained_var: 0.0007244348526000977\n",
      "        vf_loss: 0.0016245823935605586\n",
      "  num_agent_steps_sampled: 324\n",
      "  num_agent_steps_trained: 324\n",
      "  num_steps_sampled: 324\n",
      "  num_steps_trained: 324\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 81\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.13333333333333\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1320539249065604\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.4856393985937\n",
      "  mean_inference_ms: 10.136452885637693\n",
      "  mean_raw_obs_processing_ms: 46.627976480691714\n",
      "time_since_restore: 176.75808572769165\n",
      "time_this_iter_s: 2.3032567501068115\n",
      "time_total_s: 176.75808572769165\n",
      "timers:\n",
      "  learn_throughput: 2.034\n",
      "  learn_time_ms: 1966.294\n",
      "  load_throughput: 1744.247\n",
      "  load_time_ms: 2.293\n",
      "  sample_throughput: 1.833\n",
      "  sample_time_ms: 2182.473\n",
      "  update_time_ms: 2.911\n",
      "timestamp: 1655985580\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 324\n",
      "training_iteration: 81\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 82 of 500 -------\n",
      "agent_timesteps_total: 328\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-42\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.038687145579589036\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 328\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 8.27180612553028e-26\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6283276081085205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3598479805902268e-06\n",
      "        policy_loss: -5.9821301450332004e-05\n",
      "        total_loss: 0.0014101192665596804\n",
      "        vf_explained_var: 0.00024962425231933594\n",
      "        vf_loss: 0.0014699403235378364\n",
      "  num_agent_steps_sampled: 328\n",
      "  num_agent_steps_trained: 328\n",
      "  num_steps_sampled: 328\n",
      "  num_steps_trained: 328\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 82\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.8\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1318789269571781\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.58250196928938\n",
      "  mean_inference_ms: 10.14917348655353\n",
      "  mean_raw_obs_processing_ms: 46.6298495528155\n",
      "time_since_restore: 178.9113495349884\n",
      "time_this_iter_s: 2.153263807296753\n",
      "time_total_s: 178.9113495349884\n",
      "timers:\n",
      "  learn_throughput: 2.038\n",
      "  learn_time_ms: 1963.092\n",
      "  load_throughput: 1698.064\n",
      "  load_time_ms: 2.356\n",
      "  sample_throughput: 1.827\n",
      "  sample_time_ms: 2189.096\n",
      "  update_time_ms: 2.623\n",
      "timestamp: 1655985582\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 328\n",
      "training_iteration: 82\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 83 of 500 -------\n",
      "agent_timesteps_total: 332\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-44\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868714557948016\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 332\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.13590306276514e-26\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6282106240590415\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.286840421916736e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0011965656847072144\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0011965656847072144\n",
      "  num_agent_steps_sampled: 332\n",
      "  num_agent_steps_trained: 332\n",
      "  num_steps_sampled: 332\n",
      "  num_steps_trained: 332\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 83\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.733333333333334\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13172776544082074\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.68077383183973\n",
      "  mean_inference_ms: 10.159437515586873\n",
      "  mean_raw_obs_processing_ms: 46.65011936834665\n",
      "time_since_restore: 181.14858174324036\n",
      "time_this_iter_s: 2.237232208251953\n",
      "time_total_s: 181.14858174324036\n",
      "timers:\n",
      "  learn_throughput: 2.027\n",
      "  learn_time_ms: 1972.955\n",
      "  load_throughput: 1692.019\n",
      "  load_time_ms: 2.364\n",
      "  sample_throughput: 1.828\n",
      "  sample_time_ms: 2188.1\n",
      "  update_time_ms: 2.612\n",
      "timestamp: 1655985584\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 332\n",
      "training_iteration: 83\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 84 of 500 -------\n",
      "agent_timesteps_total: 336\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-46\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868705342560004\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 336\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.06795153138257e-26\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.628202756245931\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.684924933452446e-09\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0009421270748134703\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0009421270748134703\n",
      "  num_agent_steps_sampled: 336\n",
      "  num_agent_steps_trained: 336\n",
      "  num_steps_sampled: 336\n",
      "  num_steps_trained: 336\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 84\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.93333333333334\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13165670501706322\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.77655461584828\n",
      "  mean_inference_ms: 10.176782044140403\n",
      "  mean_raw_obs_processing_ms: 46.67700134830768\n",
      "time_since_restore: 183.27592587471008\n",
      "time_this_iter_s: 2.1273441314697266\n",
      "time_total_s: 183.27592587471008\n",
      "timers:\n",
      "  learn_throughput: 1.991\n",
      "  learn_time_ms: 2009.092\n",
      "  load_throughput: 1675.025\n",
      "  load_time_ms: 2.388\n",
      "  sample_throughput: 1.833\n",
      "  sample_time_ms: 2182.736\n",
      "  update_time_ms: 2.593\n",
      "timestamp: 1655985586\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 336\n",
      "training_iteration: 84\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 85 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 340\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-49\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871416741536325\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 340\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.033975765691285e-26\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.627041451136271\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.077020684188104e-05\n",
      "        policy_loss: -0.022202320893605552\n",
      "        total_loss: 0.44372531871000925\n",
      "        vf_explained_var: -1.5894571940104166e-08\n",
      "        vf_loss: 0.46592764258384706\n",
      "  num_agent_steps_sampled: 340\n",
      "  num_agent_steps_trained: 340\n",
      "  num_steps_sampled: 340\n",
      "  num_steps_trained: 340\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 85\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.425\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1316301035754468\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.85275708814164\n",
      "  mean_inference_ms: 10.20114656002379\n",
      "  mean_raw_obs_processing_ms: 46.70617196326357\n",
      "time_since_restore: 185.6582431793213\n",
      "time_this_iter_s: 2.382317304611206\n",
      "time_total_s: 185.6582431793213\n",
      "timers:\n",
      "  learn_throughput: 1.926\n",
      "  learn_time_ms: 2076.98\n",
      "  load_throughput: 1639.921\n",
      "  load_time_ms: 2.439\n",
      "  sample_throughput: 1.794\n",
      "  sample_time_ms: 2229.142\n",
      "  update_time_ms: 2.54\n",
      "timestamp: 1655985589\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 340\n",
      "training_iteration: 85\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 86 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905510)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 344\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-51\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.068727765223492\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 344\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.169878828456425e-27\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6232065757115683\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001369085512124002\n",
      "        policy_loss: -0.02689856042464574\n",
      "        total_loss: 0.19809827307860056\n",
      "        vf_explained_var: -2.384185791015625e-08\n",
      "        vf_loss: 0.22499683499336243\n",
      "  num_agent_steps_sampled: 344\n",
      "  num_agent_steps_trained: 344\n",
      "  num_steps_sampled: 344\n",
      "  num_steps_trained: 344\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 86\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.4\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13165192855538804\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.90285244501817\n",
      "  mean_inference_ms: 10.231323247317434\n",
      "  mean_raw_obs_processing_ms: 46.73904543784719\n",
      "time_since_restore: 187.99565315246582\n",
      "time_this_iter_s: 2.3374099731445312\n",
      "time_total_s: 187.99565315246582\n",
      "timers:\n",
      "  learn_throughput: 1.904\n",
      "  learn_time_ms: 2100.684\n",
      "  load_throughput: 1609.773\n",
      "  load_time_ms: 2.485\n",
      "  sample_throughput: 1.744\n",
      "  sample_time_ms: 2293.749\n",
      "  update_time_ms: 2.907\n",
      "timestamp: 1655985591\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 344\n",
      "training_iteration: 86\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 87 of 500 -------\n",
      "agent_timesteps_total: 348\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-54\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872784938147847\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 348\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.5849394142282123e-27\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6199013630549115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.562921196762602e-05\n",
      "        policy_loss: -0.00011254359657565752\n",
      "        total_loss: 0.0045479723562796915\n",
      "        vf_explained_var: -0.0008116960525512695\n",
      "        vf_loss: 0.0046605159062892195\n",
      "  num_agent_steps_sampled: 348\n",
      "  num_agent_steps_trained: 348\n",
      "  num_steps_sampled: 348\n",
      "  num_steps_trained: 348\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 87\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.166666666666664\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1316256375744476\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.97191423839435\n",
      "  mean_inference_ms: 10.258632041644546\n",
      "  mean_raw_obs_processing_ms: 46.771208825788435\n",
      "time_since_restore: 190.34438276290894\n",
      "time_this_iter_s: 2.3487296104431152\n",
      "time_total_s: 190.34438276290894\n",
      "timers:\n",
      "  learn_throughput: 1.887\n",
      "  learn_time_ms: 2119.232\n",
      "  load_throughput: 1709.797\n",
      "  load_time_ms: 2.339\n",
      "  sample_throughput: 1.722\n",
      "  sample_time_ms: 2322.248\n",
      "  update_time_ms: 3.018\n",
      "timestamp: 1655985594\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 348\n",
      "training_iteration: 87\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 88 of 500 -------\n",
      "agent_timesteps_total: 352\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-56\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.058714155461719555\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 352\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2924697071141062e-27\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.61934715906779\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.799288338593518e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.004262339494501551\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.004262339494501551\n",
      "  num_agent_steps_sampled: 352\n",
      "  num_agent_steps_trained: 352\n",
      "  num_steps_sampled: 352\n",
      "  num_steps_trained: 352\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 88\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.225\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1315972487695136\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.02845226061181\n",
      "  mean_inference_ms: 10.285848881970661\n",
      "  mean_raw_obs_processing_ms: 46.79664566087987\n",
      "time_since_restore: 192.65702104568481\n",
      "time_this_iter_s: 2.312638282775879\n",
      "time_total_s: 192.65702104568481\n",
      "timers:\n",
      "  learn_throughput: 1.892\n",
      "  learn_time_ms: 2114.562\n",
      "  load_throughput: 1699.286\n",
      "  load_time_ms: 2.354\n",
      "  sample_throughput: 1.714\n",
      "  sample_time_ms: 2333.892\n",
      "  update_time_ms: 2.694\n",
      "timestamp: 1655985596\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 352\n",
      "training_iteration: 88\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 89 of 500 -------\n",
      "agent_timesteps_total: 356\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_12-59-58\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868685782946411\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 356\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 6.462348535570531e-28\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6191585143407186\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.7644075818381946e-06\n",
      "        policy_loss: -0.00017541612808903058\n",
      "        total_loss: 0.003515980610003074\n",
      "        vf_explained_var: 6.643732388814291e-05\n",
      "        vf_loss: 0.003691396955400705\n",
      "  num_agent_steps_sampled: 356\n",
      "  num_agent_steps_trained: 356\n",
      "  num_steps_sampled: 356\n",
      "  num_steps_trained: 356\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 89\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.766666666666666\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13153566749207388\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.11178726083743\n",
      "  mean_inference_ms: 10.31007929491849\n",
      "  mean_raw_obs_processing_ms: 46.82864957914453\n",
      "time_since_restore: 195.03897905349731\n",
      "time_this_iter_s: 2.3819580078125\n",
      "time_total_s: 195.03897905349731\n",
      "timers:\n",
      "  learn_throughput: 1.89\n",
      "  learn_time_ms: 2116.875\n",
      "  load_throughput: 1708.961\n",
      "  load_time_ms: 2.341\n",
      "  sample_throughput: 1.716\n",
      "  sample_time_ms: 2331.311\n",
      "  update_time_ms: 2.709\n",
      "timestamp: 1655985598\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 356\n",
      "training_iteration: 89\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 90 of 500 -------\n",
      "agent_timesteps_total: 360\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-01\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0386869503040321\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 360\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.2311742677852654e-28\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6187268495559692\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6239638474265424e-06\n",
      "        policy_loss: -4.252813135584195e-05\n",
      "        total_loss: 0.003153731860220432\n",
      "        vf_explained_var: 0.00026851892471313477\n",
      "        vf_loss: 0.003196260170079768\n",
      "  num_agent_steps_sampled: 360\n",
      "  num_agent_steps_trained: 360\n",
      "  num_steps_sampled: 360\n",
      "  num_steps_trained: 360\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 90\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.6\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13144916791364428\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.18589843364342\n",
      "  mean_inference_ms: 10.333398035234138\n",
      "  mean_raw_obs_processing_ms: 46.8568365489683\n",
      "time_since_restore: 197.3245086669922\n",
      "time_this_iter_s: 2.285529613494873\n",
      "time_total_s: 197.3245086669922\n",
      "timers:\n",
      "  learn_throughput: 1.91\n",
      "  learn_time_ms: 2094.279\n",
      "  load_throughput: 1586.273\n",
      "  load_time_ms: 2.522\n",
      "  sample_throughput: 1.714\n",
      "  sample_time_ms: 2333.992\n",
      "  update_time_ms: 2.613\n",
      "timestamp: 1655985601\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 360\n",
      "training_iteration: 90\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 91 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905511)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 364\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-03\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870045092327166\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 364\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.6155871338926327e-28\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.617017658551534\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.460012282449525e-05\n",
      "        policy_loss: -0.01884345312913259\n",
      "        total_loss: 0.20622047086556752\n",
      "        vf_explained_var: -3.178914388020833e-08\n",
      "        vf_loss: 0.22506392995516458\n",
      "  num_agent_steps_sampled: 364\n",
      "  num_agent_steps_trained: 364\n",
      "  num_steps_sampled: 364\n",
      "  num_steps_trained: 364\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 91\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.324999999999996\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13136658124953976\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.24341709493635\n",
      "  mean_inference_ms: 10.356630903863163\n",
      "  mean_raw_obs_processing_ms: 46.88167079984608\n",
      "time_since_restore: 199.44094920158386\n",
      "time_this_iter_s: 2.116440534591675\n",
      "time_total_s: 199.44094920158386\n",
      "timers:\n",
      "  learn_throughput: 1.919\n",
      "  learn_time_ms: 2084.017\n",
      "  load_throughput: 2048.375\n",
      "  load_time_ms: 1.953\n",
      "  sample_throughput: 1.737\n",
      "  sample_time_ms: 2303.011\n",
      "  update_time_ms: 2.633\n",
      "timestamp: 1655985603\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 364\n",
      "training_iteration: 91\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 92 of 500 -------\n",
      "agent_timesteps_total: 368\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-05\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.04870045092327166\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 368\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 8.077935669463164e-29\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.613372484842936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.7014167730735987e-05\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.004341224953532219\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.004341224953532219\n",
      "  num_agent_steps_sampled: 368\n",
      "  num_agent_steps_trained: 368\n",
      "  num_steps_sampled: 368\n",
      "  num_steps_trained: 368\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 92\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.666666666666664\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1313359446129276\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28947273759456\n",
      "  mean_inference_ms: 10.385001288185636\n",
      "  mean_raw_obs_processing_ms: 46.91135802909546\n",
      "time_since_restore: 201.78827023506165\n",
      "time_this_iter_s: 2.347321033477783\n",
      "time_total_s: 201.78827023506165\n",
      "timers:\n",
      "  learn_throughput: 1.905\n",
      "  learn_time_ms: 2100.196\n",
      "  load_throughput: 2144.93\n",
      "  load_time_ms: 1.865\n",
      "  sample_throughput: 1.744\n",
      "  sample_time_ms: 2293.119\n",
      "  update_time_ms: 2.821\n",
      "timestamp: 1655985605\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 368\n",
      "training_iteration: 92\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 93 of 500 -------\n",
      "agent_timesteps_total: 372\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-08\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.048700530670948254\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 372\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.038967834731582e-29\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6126329342524213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.8193437932156184e-06\n",
      "        policy_loss: -0.0002734653030832609\n",
      "        total_loss: 0.003647390939295292\n",
      "        vf_explained_var: -8.353789647420247e-05\n",
      "        vf_loss: 0.003920856358793874\n",
      "  num_agent_steps_sampled: 372\n",
      "  num_agent_steps_trained: 372\n",
      "  num_steps_sampled: 372\n",
      "  num_steps_trained: 372\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 93\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.4\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13134605148747647\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.33966014550779\n",
      "  mean_inference_ms: 10.421350960799565\n",
      "  mean_raw_obs_processing_ms: 46.9388180852363\n",
      "time_since_restore: 204.31028819084167\n",
      "time_this_iter_s: 2.5220179557800293\n",
      "time_total_s: 204.31028819084167\n",
      "timers:\n",
      "  learn_throughput: 1.877\n",
      "  learn_time_ms: 2130.667\n",
      "  load_throughput: 2131.739\n",
      "  load_time_ms: 1.876\n",
      "  sample_throughput: 1.733\n",
      "  sample_time_ms: 2307.873\n",
      "  update_time_ms: 2.757\n",
      "timestamp: 1655985608\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 372\n",
      "training_iteration: 93\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 94 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 376\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-10\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871403469947869\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 376\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.019483917365791e-29\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.610150424639384\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.809704760431378e-05\n",
      "        policy_loss: -0.019456152121225992\n",
      "        total_loss: 0.20336321691672007\n",
      "        vf_explained_var: 3.5762786865234374e-08\n",
      "        vf_loss: 0.2228193720181783\n",
      "  num_agent_steps_sampled: 376\n",
      "  num_agent_steps_trained: 376\n",
      "  num_steps_sampled: 376\n",
      "  num_steps_trained: 376\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 94\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.2\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13123154424253458\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.38865290481084\n",
      "  mean_inference_ms: 10.45062586902311\n",
      "  mean_raw_obs_processing_ms: 46.97004808317235\n",
      "time_since_restore: 206.4020688533783\n",
      "time_this_iter_s: 2.091780662536621\n",
      "time_total_s: 206.4020688533783\n",
      "timers:\n",
      "  learn_throughput: 1.885\n",
      "  learn_time_ms: 2122.373\n",
      "  load_throughput: 1993.633\n",
      "  load_time_ms: 2.006\n",
      "  sample_throughput: 1.709\n",
      "  sample_time_ms: 2340.261\n",
      "  update_time_ms: 2.801\n",
      "timestamp: 1655985610\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 376\n",
      "training_iteration: 94\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 95 of 500 -------\n",
      "agent_timesteps_total: 380\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-12\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871394233843775\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 380\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0097419586828954e-29\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6054229497909547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.070476409590128e-05\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.005244083687042197\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.005244083687042197\n",
      "  num_agent_steps_sampled: 380\n",
      "  num_agent_steps_trained: 380\n",
      "  num_steps_sampled: 380\n",
      "  num_steps_trained: 380\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 95\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 41.36666666666667\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1311397838713219\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.41492410602626\n",
      "  mean_inference_ms: 10.481237706087267\n",
      "  mean_raw_obs_processing_ms: 47.01515957628805\n",
      "time_since_restore: 208.5319676399231\n",
      "time_this_iter_s: 2.1298987865448\n",
      "time_total_s: 208.5319676399231\n",
      "timers:\n",
      "  learn_throughput: 1.906\n",
      "  learn_time_ms: 2098.202\n",
      "  load_throughput: 2006.652\n",
      "  load_time_ms: 1.993\n",
      "  sample_throughput: 1.714\n",
      "  sample_time_ms: 2333.162\n",
      "  update_time_ms: 3.011\n",
      "timestamp: 1655985612\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 380\n",
      "training_iteration: 95\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 96 of 500 -------\n",
      "agent_timesteps_total: 384\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-14\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871394997163084\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 384\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.048709793414477e-30\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.60461433728536\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.977360433457458e-06\n",
      "        policy_loss: -0.00017799343913793564\n",
      "        total_loss: 0.004480633077522119\n",
      "        vf_explained_var: 0.00016868114471435547\n",
      "        vf_loss: 0.004658626547704141\n",
      "  num_agent_steps_sampled: 384\n",
      "  num_agent_steps_trained: 384\n",
      "  num_steps_sampled: 384\n",
      "  num_steps_trained: 384\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 96\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.233333333333334\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13108597446394463\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.4251405582332\n",
      "  mean_inference_ms: 10.51559007363696\n",
      "  mean_raw_obs_processing_ms: 47.06200919611141\n",
      "time_since_restore: 210.8642828464508\n",
      "time_this_iter_s: 2.33231520652771\n",
      "time_total_s: 210.8642828464508\n",
      "timers:\n",
      "  learn_throughput: 1.906\n",
      "  learn_time_ms: 2098.947\n",
      "  load_throughput: 2002.126\n",
      "  load_time_ms: 1.998\n",
      "  sample_throughput: 1.733\n",
      "  sample_time_ms: 2308.365\n",
      "  update_time_ms: 2.721\n",
      "timestamp: 1655985614\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 384\n",
      "training_iteration: 96\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 97 of 500 -------\n",
      "agent_timesteps_total: 388\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-16\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871404436558704\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 388\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.5243548967072386e-30\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.604156748453776\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.7884141180637795e-06\n",
      "        policy_loss: -0.00013980241492390634\n",
      "        total_loss: 0.0037678638473153114\n",
      "        vf_explained_var: -6.151398022969564e-05\n",
      "        vf_loss: 0.00390766616910696\n",
      "  num_agent_steps_sampled: 388\n",
      "  num_agent_steps_trained: 388\n",
      "  num_steps_sampled: 388\n",
      "  num_steps_trained: 388\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 97\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.800000000000004\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1310882493384497\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.4475222219889\n",
      "  mean_inference_ms: 10.551114816180938\n",
      "  mean_raw_obs_processing_ms: 47.11252816441201\n",
      "time_since_restore: 212.96482825279236\n",
      "time_this_iter_s: 2.1005454063415527\n",
      "time_total_s: 212.96482825279236\n",
      "timers:\n",
      "  learn_throughput: 1.929\n",
      "  learn_time_ms: 2073.229\n",
      "  load_throughput: 1984.906\n",
      "  load_time_ms: 2.015\n",
      "  sample_throughput: 1.732\n",
      "  sample_time_ms: 2309.778\n",
      "  update_time_ms: 2.584\n",
      "timestamp: 1655985616\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 388\n",
      "training_iteration: 97\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 98 of 500 -------\n",
      "agent_timesteps_total: 392\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-18\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0587141386803335\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 392\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2621774483536193e-30\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.603720450401306\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.9512284124066922e-06\n",
      "        policy_loss: -0.0001970066378513972\n",
      "        total_loss: 0.0030745381799836954\n",
      "        vf_explained_var: 0.000647286574045817\n",
      "        vf_loss: 0.0032715448566401997\n",
      "  num_agent_steps_sampled: 392\n",
      "  num_agent_steps_trained: 392\n",
      "  num_steps_sampled: 392\n",
      "  num_steps_trained: 392\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 98\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.199999999999996\n",
      "  ram_util_percent: 13.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13113190929326538\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.4774397061479\n",
      "  mean_inference_ms: 10.590377579976781\n",
      "  mean_raw_obs_processing_ms: 47.14999956503927\n",
      "time_since_restore: 214.96582651138306\n",
      "time_this_iter_s: 2.0009982585906982\n",
      "time_total_s: 214.96582651138306\n",
      "timers:\n",
      "  learn_throughput: 1.965\n",
      "  learn_time_ms: 2035.874\n",
      "  load_throughput: 1871.998\n",
      "  load_time_ms: 2.137\n",
      "  sample_throughput: 1.748\n",
      "  sample_time_ms: 2288.371\n",
      "  update_time_ms: 2.612\n",
      "timestamp: 1655985618\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 392\n",
      "training_iteration: 98\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 99 of 500 -------\n",
      "agent_timesteps_total: 396\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-20\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871395875032284\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 396\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 6.3108872417680965e-31\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6033557812372843\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.090421956391386e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0027376546679685515\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0027376546679685515\n",
      "  num_agent_steps_sampled: 396\n",
      "  num_agent_steps_trained: 396\n",
      "  num_steps_sampled: 396\n",
      "  num_steps_trained: 396\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 99\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 10.9\n",
      "  ram_util_percent: 13.0\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1311420182410193\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.4707547909232\n",
      "  mean_inference_ms: 10.625106877932614\n",
      "  mean_raw_obs_processing_ms: 47.166442634815155\n",
      "time_since_restore: 216.62646865844727\n",
      "time_this_iter_s: 1.660642147064209\n",
      "time_total_s: 216.62646865844727\n",
      "timers:\n",
      "  learn_throughput: 2.026\n",
      "  learn_time_ms: 1974.333\n",
      "  load_throughput: 1889.028\n",
      "  load_time_ms: 2.117\n",
      "  sample_throughput: 1.785\n",
      "  sample_time_ms: 2240.696\n",
      "  update_time_ms: 2.704\n",
      "timestamp: 1655985620\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 396\n",
      "training_iteration: 99\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 100 of 500 -------\n",
      "agent_timesteps_total: 400\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-22\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.058713958750322856\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 400\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.1554436208840483e-31\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6033301035563152\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.6572386745868547e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0022853842781235772\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0022853842781235772\n",
      "  num_agent_steps_sampled: 400\n",
      "  num_agent_steps_trained: 400\n",
      "  num_steps_sampled: 400\n",
      "  num_steps_trained: 400\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 100\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.03333333333333\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13120558120571657\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.4715258389686\n",
      "  mean_inference_ms: 10.664368995720523\n",
      "  mean_raw_obs_processing_ms: 47.18696518398121\n",
      "time_since_restore: 218.61885738372803\n",
      "time_this_iter_s: 1.9923887252807617\n",
      "time_total_s: 218.61885738372803\n",
      "timers:\n",
      "  learn_throughput: 2.052\n",
      "  learn_time_ms: 1949.4\n",
      "  load_throughput: 2012.091\n",
      "  load_time_ms: 1.988\n",
      "  sample_throughput: 1.839\n",
      "  sample_time_ms: 2175.246\n",
      "  update_time_ms: 2.68\n",
      "timestamp: 1655985622\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 400\n",
      "training_iteration: 100\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 101 of 500 -------\n",
      "agent_timesteps_total: 404\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-24\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.058714049311653825\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 404\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5777218104420241e-31\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6030060450236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.50450735244586e-06\n",
      "        policy_loss: -0.0001943148672580719\n",
      "        total_loss: 0.0017104214678208033\n",
      "        vf_explained_var: 3.885825475056966e-05\n",
      "        vf_loss: 0.0019047364165696004\n",
      "  num_agent_steps_sampled: 404\n",
      "  num_agent_steps_trained: 404\n",
      "  num_steps_sampled: 404\n",
      "  num_steps_trained: 404\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 101\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.1\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13132568752468074\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.46798570070162\n",
      "  mean_inference_ms: 10.706805554835203\n",
      "  mean_raw_obs_processing_ms: 47.200172896924485\n",
      "time_since_restore: 220.8039870262146\n",
      "time_this_iter_s: 2.1851296424865723\n",
      "time_total_s: 220.8039870262146\n",
      "timers:\n",
      "  learn_throughput: 2.053\n",
      "  learn_time_ms: 1948.601\n",
      "  load_throughput: 2021.01\n",
      "  load_time_ms: 1.979\n",
      "  sample_throughput: 1.854\n",
      "  sample_time_ms: 2157.353\n",
      "  update_time_ms: 2.833\n",
      "timestamp: 1655985624\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 404\n",
      "training_iteration: 101\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 102 of 500 -------\n",
      "agent_timesteps_total: 408\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-26\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871404931181713\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 408\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.888609052210121e-32\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6022862831751508\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.383075817310934e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0015842283338618776\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0015842283338618776\n",
      "  num_agent_steps_sampled: 408\n",
      "  num_agent_steps_trained: 408\n",
      "  num_steps_sampled: 408\n",
      "  num_steps_trained: 408\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 102\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.53333333333333\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13145802319037145\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.45255748013172\n",
      "  mean_inference_ms: 10.749537236641842\n",
      "  mean_raw_obs_processing_ms: 47.209897797870774\n",
      "time_since_restore: 222.9517662525177\n",
      "time_this_iter_s: 2.1477792263031006\n",
      "time_total_s: 222.9517662525177\n",
      "timers:\n",
      "  learn_throughput: 2.071\n",
      "  learn_time_ms: 1931.069\n",
      "  load_throughput: 2013.467\n",
      "  load_time_ms: 1.987\n",
      "  sample_throughput: 1.855\n",
      "  sample_time_ms: 2156.292\n",
      "  update_time_ms: 2.961\n",
      "timestamp: 1655985626\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 408\n",
      "training_iteration: 102\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 103 of 500 -------\n",
      "agent_timesteps_total: 412\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-29\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871422623408956\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 412\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.9443045261050603e-32\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.6018882830937704\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.6356994314701295e-06\n",
      "        policy_loss: -0.00021419922510782877\n",
      "        total_loss: 0.0010981796930233636\n",
      "        vf_explained_var: -3.326535224914551e-05\n",
      "        vf_loss: 0.0013123787318666776\n",
      "  num_agent_steps_sampled: 412\n",
      "  num_agent_steps_trained: 412\n",
      "  num_steps_sampled: 412\n",
      "  num_steps_trained: 412\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 103\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.35\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13156838000040166\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.43001775010845\n",
      "  mean_inference_ms: 10.791517638498599\n",
      "  mean_raw_obs_processing_ms: 47.214422053900044\n",
      "time_since_restore: 225.297833442688\n",
      "time_this_iter_s: 2.346067190170288\n",
      "time_total_s: 225.297833442688\n",
      "timers:\n",
      "  learn_throughput: 2.09\n",
      "  learn_time_ms: 1914.081\n",
      "  load_throughput: 1898.326\n",
      "  load_time_ms: 2.107\n",
      "  sample_throughput: 1.872\n",
      "  sample_time_ms: 2136.434\n",
      "  update_time_ms: 2.908\n",
      "timestamp: 1655985629\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 412\n",
      "training_iteration: 103\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 104 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 416\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-31\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.06872782069457087\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 416\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9721522630525302e-32\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5991150617599486\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010450263730354218\n",
      "        policy_loss: -0.02168356776237488\n",
      "        total_loss: 0.21092533071835837\n",
      "        vf_explained_var: -3.9736429850260414e-08\n",
      "        vf_loss: 0.23260890146096547\n",
      "  num_agent_steps_sampled: 416\n",
      "  num_agent_steps_trained: 416\n",
      "  num_steps_sampled: 416\n",
      "  num_steps_trained: 416\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 104\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.333333333333336\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1316692409934691\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.39925008508555\n",
      "  mean_inference_ms: 10.828500841818874\n",
      "  mean_raw_obs_processing_ms: 47.21595437862289\n",
      "time_since_restore: 227.56466436386108\n",
      "time_this_iter_s: 2.2668309211730957\n",
      "time_total_s: 227.56466436386108\n",
      "timers:\n",
      "  learn_throughput: 2.066\n",
      "  learn_time_ms: 1935.961\n",
      "  load_throughput: 2000.217\n",
      "  load_time_ms: 2.0\n",
      "  sample_throughput: 1.891\n",
      "  sample_time_ms: 2115.156\n",
      "  update_time_ms: 3.098\n",
      "timestamp: 1655985631\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 416\n",
      "training_iteration: 104\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 105 of 500 -------\n",
      "agent_timesteps_total: 420\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-33\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871412439099233\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 420\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.860761315262651e-33\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5935931762059528\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.8817455932711104e-05\n",
      "        policy_loss: -0.0004266873312493165\n",
      "        total_loss: 0.0020562966043750444\n",
      "        vf_explained_var: 0.00032169818878173827\n",
      "        vf_loss: 0.0024829838735361894\n",
      "  num_agent_steps_sampled: 420\n",
      "  num_agent_steps_trained: 420\n",
      "  num_steps_sampled: 420\n",
      "  num_steps_trained: 420\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 105\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.699999999999996\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13176309727450705\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.37821260946764\n",
      "  mean_inference_ms: 10.864587147090168\n",
      "  mean_raw_obs_processing_ms: 47.21674841523398\n",
      "time_since_restore: 229.74825263023376\n",
      "time_this_iter_s: 2.1835882663726807\n",
      "time_total_s: 229.74825263023376\n",
      "timers:\n",
      "  learn_throughput: 2.061\n",
      "  learn_time_ms: 1941.051\n",
      "  load_throughput: 1988.364\n",
      "  load_time_ms: 2.012\n",
      "  sample_throughput: 1.873\n",
      "  sample_time_ms: 2136.045\n",
      "  update_time_ms: 3.283\n",
      "timestamp: 1655985633\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 420\n",
      "training_iteration: 105\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 106 of 500 -------\n",
      "agent_timesteps_total: 424\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-35\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.058714040838790574\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 424\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.9303806576313254e-33\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.592059620221456\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.3157122389723856e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0022308084492882094\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0022308084492882094\n",
      "  num_agent_steps_sampled: 424\n",
      "  num_agent_steps_trained: 424\n",
      "  num_steps_sampled: 424\n",
      "  num_steps_trained: 424\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 106\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.6\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1318559650338099\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.3410709952976\n",
      "  mean_inference_ms: 10.900997799896015\n",
      "  mean_raw_obs_processing_ms: 47.215215070190425\n",
      "time_since_restore: 231.83762502670288\n",
      "time_this_iter_s: 2.089372396469116\n",
      "time_total_s: 231.83762502670288\n",
      "timers:\n",
      "  learn_throughput: 2.087\n",
      "  learn_time_ms: 1916.496\n",
      "  load_throughput: 2032.469\n",
      "  load_time_ms: 1.968\n",
      "  sample_throughput: 1.867\n",
      "  sample_time_ms: 2143.016\n",
      "  update_time_ms: 3.218\n",
      "timestamp: 1655985635\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 424\n",
      "training_iteration: 106\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 107 of 500 -------\n",
      "agent_timesteps_total: 428\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-38\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.05871395380523911\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 428\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4651903288156627e-33\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5919349590937295\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.4428554209189315e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0018279642371150354\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0018279642371150354\n",
      "  num_agent_steps_sampled: 428\n",
      "  num_agent_steps_trained: 428\n",
      "  num_steps_sampled: 428\n",
      "  num_steps_trained: 428\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 107\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.86666666666667\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13195890073928845\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.30903935307191\n",
      "  mean_inference_ms: 10.938026911510098\n",
      "  mean_raw_obs_processing_ms: 47.21568014440788\n",
      "time_since_restore: 234.0822947025299\n",
      "time_this_iter_s: 2.2446696758270264\n",
      "time_total_s: 234.0822947025299\n",
      "timers:\n",
      "  learn_throughput: 2.07\n",
      "  learn_time_ms: 1932.354\n",
      "  load_throughput: 1914.311\n",
      "  load_time_ms: 2.09\n",
      "  sample_throughput: 1.892\n",
      "  sample_time_ms: 2114.468\n",
      "  update_time_ms: 3.207\n",
      "timestamp: 1655985638\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 428\n",
      "training_iteration: 107\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 108 of 500 -------\n",
      "agent_timesteps_total: 432\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-40\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.058714047777788034\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 432\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2325951644078314e-33\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.591530505816142\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.6511741988174435e-06\n",
      "        policy_loss: -0.00024123319114247957\n",
      "        total_loss: 0.0012457116196552913\n",
      "        vf_explained_var: 0.00042939186096191406\n",
      "        vf_loss: 0.0014869451328801612\n",
      "  num_agent_steps_sampled: 432\n",
      "  num_agent_steps_trained: 432\n",
      "  num_steps_sampled: 432\n",
      "  num_steps_trained: 432\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 108\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.4\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1320075096421026\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28239520609307\n",
      "  mean_inference_ms: 10.97273824405833\n",
      "  mean_raw_obs_processing_ms: 47.20545767076756\n",
      "time_since_restore: 236.15891242027283\n",
      "time_this_iter_s: 2.07661771774292\n",
      "time_total_s: 236.15891242027283\n",
      "timers:\n",
      "  learn_throughput: 2.062\n",
      "  learn_time_ms: 1939.875\n",
      "  load_throughput: 2017.001\n",
      "  load_time_ms: 1.983\n",
      "  sample_throughput: 1.875\n",
      "  sample_time_ms: 2133.199\n",
      "  update_time_ms: 3.261\n",
      "timestamp: 1655985640\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 432\n",
      "training_iteration: 108\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 109 of 500 -------\n",
      "agent_timesteps_total: 436\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-42\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0587141356640148\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 436\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 6.162975822039157e-34\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5905375719070434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.7354122721590103e-06\n",
      "        policy_loss: -7.960392783085505e-05\n",
      "        total_loss: 0.0011260322916011015\n",
      "        vf_explained_var: 0.00027315219243367514\n",
      "        vf_loss: 0.0012056362194319567\n",
      "  num_agent_steps_sampled: 436\n",
      "  num_agent_steps_trained: 436\n",
      "  num_steps_sampled: 436\n",
      "  num_steps_trained: 436\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 109\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.474999999999994\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1319736478290419\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.26222814474123\n",
      "  mean_inference_ms: 10.998346769615011\n",
      "  mean_raw_obs_processing_ms: 47.196637834421324\n",
      "time_since_restore: 238.31373071670532\n",
      "time_this_iter_s: 2.154818296432495\n",
      "time_total_s: 238.31373071670532\n",
      "timers:\n",
      "  learn_throughput: 2.02\n",
      "  learn_time_ms: 1980.618\n",
      "  load_throughput: 1969.203\n",
      "  load_time_ms: 2.031\n",
      "  sample_throughput: 1.86\n",
      "  sample_time_ms: 2150.055\n",
      "  update_time_ms: 3.964\n",
      "timestamp: 1655985642\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 436\n",
      "training_iteration: 109\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 110 of 500 -------\n",
      "agent_timesteps_total: 440\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-44\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.038686933768894975\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 440\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.0814879110195784e-34\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5899664481480915\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.998630902264267e-06\n",
      "        policy_loss: -0.00021545182292660077\n",
      "        total_loss: 0.0007597338408231735\n",
      "        vf_explained_var: -6.632804870605468e-05\n",
      "        vf_loss: 0.0009751856637497743\n",
      "  num_agent_steps_sampled: 440\n",
      "  num_agent_steps_trained: 440\n",
      "  num_steps_sampled: 440\n",
      "  num_steps_trained: 440\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 110\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.13333333333333\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13192307291955752\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.25917345802166\n",
      "  mean_inference_ms: 11.0167278827449\n",
      "  mean_raw_obs_processing_ms: 47.182265309606784\n",
      "time_since_restore: 240.58064150810242\n",
      "time_this_iter_s: 2.2669107913970947\n",
      "time_total_s: 240.58064150810242\n",
      "timers:\n",
      "  learn_throughput: 1.995\n",
      "  learn_time_ms: 2005.308\n",
      "  load_throughput: 2048.725\n",
      "  load_time_ms: 1.952\n",
      "  sample_throughput: 1.821\n",
      "  sample_time_ms: 2196.986\n",
      "  update_time_ms: 4.01\n",
      "timestamp: 1655985644\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 440\n",
      "training_iteration: 110\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 111 of 500 -------\n",
      "agent_timesteps_total: 444\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-46\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673329540539916\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 444\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5407439555097892e-34\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5892837206522623\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.96084252835279e-06\n",
      "        policy_loss: -0.00018096774195631345\n",
      "        total_loss: 0.0006050131283700466\n",
      "        vf_explained_var: -0.0003116130828857422\n",
      "        vf_loss: 0.0007859809091314673\n",
      "  num_agent_steps_sampled: 444\n",
      "  num_agent_steps_trained: 444\n",
      "  num_steps_sampled: 444\n",
      "  num_steps_trained: 444\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 111\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.46666666666667\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1318477912255399\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.27094081913292\n",
      "  mean_inference_ms: 11.030063980116893\n",
      "  mean_raw_obs_processing_ms: 47.16871337075002\n",
      "time_since_restore: 242.6455099582672\n",
      "time_this_iter_s: 2.064868450164795\n",
      "time_total_s: 242.6455099582672\n",
      "timers:\n",
      "  learn_throughput: 2.007\n",
      "  learn_time_ms: 1992.633\n",
      "  load_throughput: 1836.546\n",
      "  load_time_ms: 2.178\n",
      "  sample_throughput: 1.8\n",
      "  sample_time_ms: 2222.383\n",
      "  update_time_ms: 4.035\n",
      "timestamp: 1655985646\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 444\n",
      "training_iteration: 111\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 112 of 500 -------\n",
      "agent_timesteps_total: 448\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-49\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673245382662332\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 448\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.703719777548946e-35\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.588806732495626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.308368116629935e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0006290872755926102\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0006290872755926102\n",
      "  num_agent_steps_sampled: 448\n",
      "  num_agent_steps_trained: 448\n",
      "  num_steps_sampled: 448\n",
      "  num_steps_trained: 448\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 112\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.666666666666664\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13175882278168854\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.2752654111175\n",
      "  mean_inference_ms: 11.037670560886433\n",
      "  mean_raw_obs_processing_ms: 47.15454757406886\n",
      "time_since_restore: 244.89160585403442\n",
      "time_this_iter_s: 2.246095895767212\n",
      "time_total_s: 244.89160585403442\n",
      "timers:\n",
      "  learn_throughput: 1.999\n",
      "  learn_time_ms: 2001.22\n",
      "  load_throughput: 1820.721\n",
      "  load_time_ms: 2.197\n",
      "  sample_throughput: 1.809\n",
      "  sample_time_ms: 2211.612\n",
      "  update_time_ms: 3.733\n",
      "timestamp: 1655985649\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 448\n",
      "training_iteration: 112\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 113 of 500 -------\n",
      "agent_timesteps_total: 452\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-51\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.02867324538271678\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 452\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.851859888774473e-35\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5887397209803265\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.114737301281347e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0005003886297345161\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0005003886297345161\n",
      "  num_agent_steps_sampled: 452\n",
      "  num_agent_steps_trained: 452\n",
      "  num_steps_sampled: 452\n",
      "  num_steps_trained: 452\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 113\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.73333333333333\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1316988405115863\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28507328174665\n",
      "  mean_inference_ms: 11.04929253583113\n",
      "  mean_raw_obs_processing_ms: 47.138382184612546\n",
      "time_since_restore: 246.88308548927307\n",
      "time_this_iter_s: 1.9914796352386475\n",
      "time_total_s: 246.88308548927307\n",
      "timers:\n",
      "  learn_throughput: 2.037\n",
      "  learn_time_ms: 1964.066\n",
      "  load_throughput: 1792.974\n",
      "  load_time_ms: 2.231\n",
      "  sample_throughput: 1.801\n",
      "  sample_time_ms: 2221.125\n",
      "  update_time_ms: 3.741\n",
      "timestamp: 1655985651\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 452\n",
      "training_iteration: 113\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 114 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 456\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-53\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868694343539023\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 456\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.9259299443872365e-35\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5865396579106648\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00012971680698683485\n",
      "        policy_loss: -0.02570931166410446\n",
      "        total_loss: 0.21219313144683838\n",
      "        vf_explained_var: -7.152557373046875e-08\n",
      "        vf_loss: 0.23790244509776434\n",
      "  num_agent_steps_sampled: 456\n",
      "  num_agent_steps_trained: 456\n",
      "  num_steps_sampled: 456\n",
      "  num_steps_trained: 456\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 114\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.53333333333333\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13161722073289334\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28500130237137\n",
      "  mean_inference_ms: 11.058688117919658\n",
      "  mean_raw_obs_processing_ms: 47.12154301323519\n",
      "time_since_restore: 249.01521492004395\n",
      "time_this_iter_s: 2.132129430770874\n",
      "time_total_s: 249.01521492004395\n",
      "timers:\n",
      "  learn_throughput: 2.047\n",
      "  learn_time_ms: 1954.307\n",
      "  load_throughput: 1827.086\n",
      "  load_time_ms: 2.189\n",
      "  sample_throughput: 1.833\n",
      "  sample_time_ms: 2181.65\n",
      "  update_time_ms: 3.598\n",
      "timestamp: 1655985653\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 456\n",
      "training_iteration: 114\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 115 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905512)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 460\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-55\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.048700647030322335\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 460\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.629649721936182e-36\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5779552380243937\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0002563496275494496\n",
      "        policy_loss: -0.038134388128916424\n",
      "        total_loss: 0.19116338690121967\n",
      "        vf_explained_var: 7.947285970052083e-09\n",
      "        vf_loss: 0.22929777006308238\n",
      "  num_agent_steps_sampled: 460\n",
      "  num_agent_steps_trained: 460\n",
      "  num_steps_sampled: 460\n",
      "  num_steps_trained: 460\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 115\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.699999999999996\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13152599445760718\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.27593193370465\n",
      "  mean_inference_ms: 11.06546347749585\n",
      "  mean_raw_obs_processing_ms: 47.105455558345746\n",
      "time_since_restore: 251.13430047035217\n",
      "time_this_iter_s: 2.1190855503082275\n",
      "time_total_s: 251.13430047035217\n",
      "timers:\n",
      "  learn_throughput: 2.05\n",
      "  learn_time_ms: 1951.389\n",
      "  load_throughput: 1744.374\n",
      "  load_time_ms: 2.293\n",
      "  sample_throughput: 1.843\n",
      "  sample_time_ms: 2170.169\n",
      "  update_time_ms: 3.271\n",
      "timestamp: 1655985655\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 460\n",
      "training_iteration: 115\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 116 of 500 -------\n",
      "agent_timesteps_total: 464\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-00-57\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868704443514179\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 464\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.814824860968091e-36\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5690036455790204\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 7.590016502329414e-05\n",
      "        policy_loss: -0.0006095594105621179\n",
      "        total_loss: 0.0031566782233615714\n",
      "        vf_explained_var: 0.0005006909370422364\n",
      "        vf_loss: 0.003766237680489818\n",
      "  num_agent_steps_sampled: 464\n",
      "  num_agent_steps_trained: 464\n",
      "  num_steps_sampled: 464\n",
      "  num_steps_trained: 464\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 116\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.9\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13143725308671894\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28086754537671\n",
      "  mean_inference_ms: 11.072477839362548\n",
      "  mean_raw_obs_processing_ms: 47.09025291105542\n",
      "time_since_restore: 253.3257029056549\n",
      "time_this_iter_s: 2.1914024353027344\n",
      "time_total_s: 253.3257029056549\n",
      "timers:\n",
      "  learn_throughput: 2.039\n",
      "  learn_time_ms: 1961.3\n",
      "  load_throughput: 1621.206\n",
      "  load_time_ms: 2.467\n",
      "  sample_throughput: 1.847\n",
      "  sample_time_ms: 2165.617\n",
      "  update_time_ms: 3.249\n",
      "timestamp: 1655985657\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 464\n",
      "training_iteration: 116\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 117 of 500 -------\n",
      "agent_timesteps_total: 468\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-00\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.038687131371239775\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 468\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.4074124304840456e-36\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.566453949610392\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.982556977968973e-06\n",
      "        policy_loss: -0.0001397335280974706\n",
      "        total_loss: 0.0031146354352434478\n",
      "        vf_explained_var: 0.00036177635192871096\n",
      "        vf_loss: 0.003254368893491725\n",
      "  num_agent_steps_sampled: 468\n",
      "  num_agent_steps_trained: 468\n",
      "  num_steps_sampled: 468\n",
      "  num_steps_trained: 468\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 117\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.4\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1313246347434359\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28573185599534\n",
      "  mean_inference_ms: 11.07568033250943\n",
      "  mean_raw_obs_processing_ms: 47.0781707747285\n",
      "time_since_restore: 255.65851616859436\n",
      "time_this_iter_s: 2.332813262939453\n",
      "time_total_s: 255.65851616859436\n",
      "timers:\n",
      "  learn_throughput: 2.032\n",
      "  learn_time_ms: 1968.347\n",
      "  load_throughput: 1597.161\n",
      "  load_time_ms: 2.504\n",
      "  sample_throughput: 1.837\n",
      "  sample_time_ms: 2177.252\n",
      "  update_time_ms: 3.287\n",
      "timestamp: 1655985660\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 468\n",
      "training_iteration: 117\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 118 of 500 -------\n",
      "agent_timesteps_total: 472\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-02\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.03868695344727108\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 472\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.2037062152420228e-36\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.565834816296895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 8.043425016997693e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.00255789573614796\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.00255789573614796\n",
      "  num_agent_steps_sampled: 472\n",
      "  num_agent_steps_trained: 472\n",
      "  num_steps_sampled: 472\n",
      "  num_steps_trained: 472\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 118\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.5\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13120030646674558\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28991022365307\n",
      "  mean_inference_ms: 11.073912887157103\n",
      "  mean_raw_obs_processing_ms: 47.06230253930974\n",
      "time_since_restore: 257.8989305496216\n",
      "time_this_iter_s: 2.2404143810272217\n",
      "time_total_s: 257.8989305496216\n",
      "timers:\n",
      "  learn_throughput: 2.01\n",
      "  learn_time_ms: 1990.108\n",
      "  load_throughput: 1525.437\n",
      "  load_time_ms: 2.622\n",
      "  sample_throughput: 1.838\n",
      "  sample_time_ms: 2176.716\n",
      "  update_time_ms: 3.202\n",
      "timestamp: 1655985662\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 472\n",
      "training_iteration: 118\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 119 of 500 -------\n",
      "agent_timesteps_total: 476\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-04\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.02867335172868465\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 476\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 6.018531076210114e-37\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5655917167663573\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.545812686232817e-07\n",
      "        policy_loss: -9.985659271478653e-05\n",
      "        total_loss: 0.0018976046393314996\n",
      "        vf_explained_var: 5.384683609008789e-05\n",
      "        vf_loss: 0.001997461309656501\n",
      "  num_agent_steps_sampled: 476\n",
      "  num_agent_steps_trained: 476\n",
      "  num_steps_sampled: 476\n",
      "  num_steps_trained: 476\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 119\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 40.03333333333333\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1311091720105795\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.291409830505\n",
      "  mean_inference_ms: 11.077680179907615\n",
      "  mean_raw_obs_processing_ms: 47.03969967792205\n",
      "time_since_restore: 260.3871822357178\n",
      "time_this_iter_s: 2.4882516860961914\n",
      "time_total_s: 260.3871822357178\n",
      "timers:\n",
      "  learn_throughput: 1.979\n",
      "  learn_time_ms: 2020.713\n",
      "  load_throughput: 1524.675\n",
      "  load_time_ms: 2.624\n",
      "  sample_throughput: 1.818\n",
      "  sample_time_ms: 2200.226\n",
      "  update_time_ms: 2.467\n",
      "timestamp: 1655985664\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 476\n",
      "training_iteration: 119\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 120 of 500 -------\n",
      "agent_timesteps_total: 480\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-07\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673351728847968\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 480\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.009265538105057e-37\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.565161617596944\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.981282248905927e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0015570580416048565\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0015570580416048565\n",
      "  num_agent_steps_sampled: 480\n",
      "  num_agent_steps_trained: 480\n",
      "  num_steps_sampled: 480\n",
      "  num_steps_trained: 480\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 120\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.93333333333334\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13101249261995335\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28778562713896\n",
      "  mean_inference_ms: 11.080650422005629\n",
      "  mean_raw_obs_processing_ms: 47.00790652776408\n",
      "time_since_restore: 262.5988097190857\n",
      "time_this_iter_s: 2.21162748336792\n",
      "time_total_s: 262.5988097190857\n",
      "timers:\n",
      "  learn_throughput: 1.983\n",
      "  learn_time_ms: 2016.7\n",
      "  load_throughput: 1510.794\n",
      "  load_time_ms: 2.648\n",
      "  sample_throughput: 1.799\n",
      "  sample_time_ms: 2223.432\n",
      "  update_time_ms: 2.49\n",
      "timestamp: 1655985667\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 480\n",
      "training_iteration: 120\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 121 of 500 -------\n",
      "agent_timesteps_total: 484\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-09\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673345792501297\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 484\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5046327690525285e-37\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5649922609329225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 9.587411843388813e-07\n",
      "        policy_loss: -0.00010244973624746005\n",
      "        total_loss: 0.0011071656520167986\n",
      "        vf_explained_var: 0.0005467534065246582\n",
      "        vf_loss: 0.001209615454232941\n",
      "  num_agent_steps_sampled: 484\n",
      "  num_agent_steps_trained: 484\n",
      "  num_steps_sampled: 484\n",
      "  num_steps_trained: 484\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 121\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.25\n",
      "  ram_util_percent: 13.3\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1309003579192188\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.29000631105316\n",
      "  mean_inference_ms: 11.080647681420928\n",
      "  mean_raw_obs_processing_ms: 46.97753855034677\n",
      "time_since_restore: 264.8326599597931\n",
      "time_this_iter_s: 2.2338502407073975\n",
      "time_total_s: 264.8326599597931\n",
      "timers:\n",
      "  learn_throughput: 1.966\n",
      "  learn_time_ms: 2034.475\n",
      "  load_throughput: 1625.054\n",
      "  load_time_ms: 2.461\n",
      "  sample_throughput: 1.8\n",
      "  sample_time_ms: 2222.216\n",
      "  update_time_ms: 2.277\n",
      "timestamp: 1655985669\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 484\n",
      "training_iteration: 121\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 122 of 500 -------\n",
      "agent_timesteps_total: 488\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-11\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673251398708417\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 488\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.523163845262643e-38\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5647356589635213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4330870490297798e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0009368983213789761\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0009368983213789761\n",
      "  num_agent_steps_sampled: 488\n",
      "  num_agent_steps_trained: 488\n",
      "  num_steps_sampled: 488\n",
      "  num_steps_trained: 488\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 122\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.56666666666666\n",
      "  ram_util_percent: 13.300000000000002\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1307702676914952\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.2867032864833\n",
      "  mean_inference_ms: 11.083657695807297\n",
      "  mean_raw_obs_processing_ms: 46.93951016571515\n",
      "time_since_restore: 266.93855261802673\n",
      "time_this_iter_s: 2.1058926582336426\n",
      "time_total_s: 266.93855261802673\n",
      "timers:\n",
      "  learn_throughput: 1.977\n",
      "  learn_time_ms: 2023.548\n",
      "  load_throughput: 1649.288\n",
      "  load_time_ms: 2.425\n",
      "  sample_throughput: 1.789\n",
      "  sample_time_ms: 2235.791\n",
      "  update_time_ms: 2.693\n",
      "timestamp: 1655985671\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 488\n",
      "training_iteration: 122\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 123 of 500 -------\n",
      "agent_timesteps_total: 492\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-13\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0286732437987376\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 492\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.7615819226313213e-38\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.564558720588684\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.2900559496209496e-06\n",
      "        policy_loss: -8.157708992560705e-05\n",
      "        total_loss: 0.0006400706867376963\n",
      "        vf_explained_var: -0.0002505779266357422\n",
      "        vf_loss: 0.0007216476602479816\n",
      "  num_agent_steps_sampled: 492\n",
      "  num_agent_steps_trained: 492\n",
      "  num_steps_sampled: 492\n",
      "  num_steps_trained: 492\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 123\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 35.93333333333333\n",
      "  ram_util_percent: 13.466666666666667\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13060101186400508\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28215861255089\n",
      "  mean_inference_ms: 11.082736472520171\n",
      "  mean_raw_obs_processing_ms: 46.90532262764834\n",
      "time_since_restore: 268.96991324424744\n",
      "time_this_iter_s: 2.031360626220703\n",
      "time_total_s: 268.96991324424744\n",
      "timers:\n",
      "  learn_throughput: 1.973\n",
      "  learn_time_ms: 2027.288\n",
      "  load_throughput: 1682.871\n",
      "  load_time_ms: 2.377\n",
      "  sample_throughput: 1.796\n",
      "  sample_time_ms: 2227.694\n",
      "  update_time_ms: 2.772\n",
      "timestamp: 1655985673\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 492\n",
      "training_iteration: 123\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 124 of 500 -------\n",
      "agent_timesteps_total: 496\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-15\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.02867324379879204\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 496\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8807909613156606e-38\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.564241576194763\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.350498219608274e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0005531805004769315\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0005531805004769315\n",
      "  num_agent_steps_sampled: 496\n",
      "  num_agent_steps_trained: 496\n",
      "  num_steps_sampled: 496\n",
      "  num_steps_trained: 496\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 124\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 14.05\n",
      "  ram_util_percent: 13.75\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13043804799681755\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28317758971612\n",
      "  mean_inference_ms: 11.082084812554653\n",
      "  mean_raw_obs_processing_ms: 46.881491970643026\n",
      "time_since_restore: 270.689208984375\n",
      "time_this_iter_s: 1.7192957401275635\n",
      "time_total_s: 270.689208984375\n",
      "timers:\n",
      "  learn_throughput: 2.017\n",
      "  learn_time_ms: 1983.022\n",
      "  load_throughput: 1698.838\n",
      "  load_time_ms: 2.355\n",
      "  sample_throughput: 1.791\n",
      "  sample_time_ms: 2233.623\n",
      "  update_time_ms: 2.731\n",
      "timestamp: 1655985675\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 496\n",
      "training_iteration: 124\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 125 of 500 -------\n",
      "agent_timesteps_total: 500\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-16\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673339770455844\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 500\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.403954806578303e-39\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5639968236287434\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5057945285927114e-06\n",
      "        policy_loss: -0.00010812490557630857\n",
      "        total_loss: 0.0003130910297234853\n",
      "        vf_explained_var: -0.00029536088307698566\n",
      "        vf_loss: 0.0004212158887336651\n",
      "  num_agent_steps_sampled: 500\n",
      "  num_agent_steps_trained: 500\n",
      "  num_steps_sampled: 500\n",
      "  num_steps_trained: 500\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 125\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.25\n",
      "  ram_util_percent: 12.600000000000001\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13024519722953473\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28702559888707\n",
      "  mean_inference_ms: 11.078048616073339\n",
      "  mean_raw_obs_processing_ms: 46.85939528671009\n",
      "time_since_restore: 272.1880261898041\n",
      "time_this_iter_s: 1.4988172054290771\n",
      "time_total_s: 272.1880261898041\n",
      "timers:\n",
      "  learn_throughput: 2.081\n",
      "  learn_time_ms: 1922.126\n",
      "  load_throughput: 1811.071\n",
      "  load_time_ms: 2.209\n",
      "  sample_throughput: 1.83\n",
      "  sample_time_ms: 2186.064\n",
      "  update_time_ms: 2.657\n",
      "timestamp: 1655985676\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 500\n",
      "training_iteration: 125\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 126 of 500 -------\n",
      "agent_timesteps_total: 504\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-18\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673430112449036\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 504\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.7019774032891516e-39\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5632644097010293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.414171192020551e-06\n",
      "        policy_loss: -0.00030654010673364005\n",
      "        total_loss: 1.1323268214861552e-05\n",
      "        vf_explained_var: 2.6547908782958983e-05\n",
      "        vf_loss: 0.000317863337113522\n",
      "  num_agent_steps_sampled: 504\n",
      "  num_agent_steps_trained: 504\n",
      "  num_steps_sampled: 504\n",
      "  num_steps_trained: 504\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 126\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.0\n",
      "  ram_util_percent: 12.4\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.13000758534229068\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.28437457569964\n",
      "  mean_inference_ms: 11.071601216157392\n",
      "  mean_raw_obs_processing_ms: 46.83414158074789\n",
      "time_since_restore: 273.7726352214813\n",
      "time_this_iter_s: 1.584609031677246\n",
      "time_total_s: 273.7726352214813\n",
      "timers:\n",
      "  learn_throughput: 2.142\n",
      "  learn_time_ms: 1867.409\n",
      "  load_throughput: 1990.038\n",
      "  load_time_ms: 2.01\n",
      "  sample_throughput: 1.888\n",
      "  sample_time_ms: 2118.914\n",
      "  update_time_ms: 2.646\n",
      "timestamp: 1655985678\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 504\n",
      "training_iteration: 126\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 127 of 500 -------\n",
      "agent_timesteps_total: 508\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-19\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.028673430112285715\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 508\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.3509887016445758e-39\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.562530199686686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0196936789460173e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0002378998634715875\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0002378998634715875\n",
      "  num_agent_steps_sampled: 508\n",
      "  num_agent_steps_trained: 508\n",
      "  num_steps_sampled: 508\n",
      "  num_steps_trained: 508\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 127\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 12.3\n",
      "  ram_util_percent: 12.4\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1297727698060972\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.27602601297205\n",
      "  mean_inference_ms: 11.065602462748318\n",
      "  mean_raw_obs_processing_ms: 46.808405630451524\n",
      "time_since_restore: 275.3625509738922\n",
      "time_this_iter_s: 1.5899157524108887\n",
      "time_total_s: 275.3625509738922\n",
      "timers:\n",
      "  learn_throughput: 2.218\n",
      "  learn_time_ms: 1803.187\n",
      "  load_throughput: 2113.372\n",
      "  load_time_ms: 1.893\n",
      "  sample_throughput: 1.947\n",
      "  sample_time_ms: 2054.397\n",
      "  update_time_ms: 2.679\n",
      "timestamp: 1655985679\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 508\n",
      "training_iteration: 127\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 128 of 500 -------\n",
      "agent_timesteps_total: 512\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-21\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.02867325318995884\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 512\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1754943508222879e-39\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5624190966288247\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -6.4936071926998315e-09\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.00017751236397695418\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.00017751236397695418\n",
      "  num_agent_steps_sampled: 512\n",
      "  num_agent_steps_trained: 512\n",
      "  num_steps_sampled: 512\n",
      "  num_steps_trained: 512\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 128\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 13.899999999999999\n",
      "  ram_util_percent: 12.4\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12955190314215928\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.26329856404315\n",
      "  mean_inference_ms: 11.060505806945043\n",
      "  mean_raw_obs_processing_ms: 46.781639785477466\n",
      "time_since_restore: 276.99522399902344\n",
      "time_this_iter_s: 1.6326730251312256\n",
      "time_total_s: 276.99522399902344\n",
      "timers:\n",
      "  learn_throughput: 2.296\n",
      "  learn_time_ms: 1742.138\n",
      "  load_throughput: 2291.782\n",
      "  load_time_ms: 1.745\n",
      "  sample_throughput: 2.01\n",
      "  sample_time_ms: 1990.326\n",
      "  update_time_ms: 2.704\n",
      "timestamp: 1655985681\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 512\n",
      "training_iteration: 128\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 129 of 500 -------\n",
      "agent_timesteps_total: 516\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-23\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.01865956405759652\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 516\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.8774717541114395e-40\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.562231095631917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.402464083990253e-06\n",
      "        policy_loss: -0.0001256200484931469\n",
      "        total_loss: 6.0641206800937654e-06\n",
      "        vf_explained_var: 6.372531255086264e-05\n",
      "        vf_loss: 0.00013168404645208892\n",
      "  num_agent_steps_sampled: 516\n",
      "  num_agent_steps_trained: 516\n",
      "  num_steps_sampled: 516\n",
      "  num_steps_trained: 516\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 129\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 15.833333333333334\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12932719057470762\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.25511938500178\n",
      "  mean_inference_ms: 11.052928578049569\n",
      "  mean_raw_obs_processing_ms: 46.75318225915522\n",
      "time_since_restore: 278.6895914077759\n",
      "time_this_iter_s: 1.6943674087524414\n",
      "time_total_s: 278.6895914077759\n",
      "timers:\n",
      "  learn_throughput: 2.389\n",
      "  learn_time_ms: 1674.013\n",
      "  load_throughput: 2321.977\n",
      "  load_time_ms: 1.723\n",
      "  sample_throughput: 2.085\n",
      "  sample_time_ms: 1918.005\n",
      "  update_time_ms: 2.591\n",
      "timestamp: 1655985683\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 516\n",
      "training_iteration: 129\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 130 of 500 -------\n",
      "agent_timesteps_total: 520\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-24\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659465649396113\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 520\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.9387358770557197e-40\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.561821397145589\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.121551102833962e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 9.711113404288577e-05\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 9.711113404288577e-05\n",
      "  num_agent_steps_sampled: 520\n",
      "  num_agent_steps_trained: 520\n",
      "  num_steps_sampled: 520\n",
      "  num_steps_trained: 520\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 130\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 16.950000000000003\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12910718410474367\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.241389690732\n",
      "  mean_inference_ms: 11.04569050247675\n",
      "  mean_raw_obs_processing_ms: 46.72166125713859\n",
      "time_since_restore: 280.33043789863586\n",
      "time_this_iter_s: 1.6408464908599854\n",
      "time_total_s: 280.33043789863586\n",
      "timers:\n",
      "  learn_throughput: 2.47\n",
      "  learn_time_ms: 1619.618\n",
      "  load_throughput: 2090.645\n",
      "  load_time_ms: 1.913\n",
      "  sample_throughput: 2.163\n",
      "  sample_time_ms: 1849.263\n",
      "  update_time_ms: 2.533\n",
      "timestamp: 1655985684\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 520\n",
      "training_iteration: 130\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 131 of 500 -------\n",
      "agent_timesteps_total: 524\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-26\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0186595583778734\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 524\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.4693679385278599e-40\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.561497934659322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.5859273238068758e-06\n",
      "        policy_loss: -0.00011897077783942222\n",
      "        total_loss: -4.8014521598815915e-05\n",
      "        vf_explained_var: 0.0001293520132700602\n",
      "        vf_loss: 7.095642043471646e-05\n",
      "  num_agent_steps_sampled: 524\n",
      "  num_agent_steps_trained: 524\n",
      "  num_steps_sampled: 524\n",
      "  num_steps_trained: 524\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 131\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.6\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.128891180990892\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.22399170160318\n",
      "  mean_inference_ms: 11.038149760696992\n",
      "  mean_raw_obs_processing_ms: 46.691413439068725\n",
      "time_since_restore: 281.99794244766235\n",
      "time_this_iter_s: 1.6675045490264893\n",
      "time_total_s: 281.99794244766235\n",
      "timers:\n",
      "  learn_throughput: 2.546\n",
      "  learn_time_ms: 1570.909\n",
      "  load_throughput: 2170.066\n",
      "  load_time_ms: 1.843\n",
      "  sample_throughput: 2.242\n",
      "  sample_time_ms: 1784.032\n",
      "  update_time_ms: 2.605\n",
      "timestamp: 1655985686\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 524\n",
      "training_iteration: 131\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 132 of 500 -------\n",
      "agent_timesteps_total: 528\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-28\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.01865955837798228\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 528\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.346839692639299e-41\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5609052340189615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.158483717507342e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 5.1387912996384937e-05\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 5.1387912996384937e-05\n",
      "  num_agent_steps_sampled: 528\n",
      "  num_agent_steps_trained: 528\n",
      "  num_steps_sampled: 528\n",
      "  num_steps_trained: 528\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 132\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.53333333333333\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1286931227733024\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.19719107460382\n",
      "  mean_inference_ms: 11.031718790830627\n",
      "  mean_raw_obs_processing_ms: 46.65958311569886\n",
      "time_since_restore: 283.7085325717926\n",
      "time_this_iter_s: 1.710590124130249\n",
      "time_total_s: 283.7085325717926\n",
      "timers:\n",
      "  learn_throughput: 2.608\n",
      "  learn_time_ms: 1533.561\n",
      "  load_throughput: 2195.109\n",
      "  load_time_ms: 1.822\n",
      "  sample_throughput: 2.307\n",
      "  sample_time_ms: 1733.805\n",
      "  update_time_ms: 2.195\n",
      "timestamp: 1655985688\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 528\n",
      "training_iteration: 132\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 133 of 500 -------\n",
      "agent_timesteps_total: 532\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-30\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659559853115668\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 532\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.6734198463196497e-41\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.560569492975871\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.8380861822227719e-06\n",
      "        policy_loss: -0.00012490209192037583\n",
      "        total_loss: -8.811186999082565e-05\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 3.679030354154141e-05\n",
      "  num_agent_steps_sampled: 532\n",
      "  num_agent_steps_trained: 532\n",
      "  num_steps_sampled: 532\n",
      "  num_steps_trained: 532\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 133\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.15\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12849864431718724\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.16146366480514\n",
      "  mean_inference_ms: 11.02514911350619\n",
      "  mean_raw_obs_processing_ms: 46.62485152901716\n",
      "time_since_restore: 285.4169936180115\n",
      "time_this_iter_s: 1.708461046218872\n",
      "time_total_s: 285.4169936180115\n",
      "timers:\n",
      "  learn_throughput: 2.651\n",
      "  learn_time_ms: 1508.889\n",
      "  load_throughput: 2366.421\n",
      "  load_time_ms: 1.69\n",
      "  sample_throughput: 2.372\n",
      "  sample_time_ms: 1686.607\n",
      "  update_time_ms: 2.137\n",
      "timestamp: 1655985690\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 532\n",
      "training_iteration: 133\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 134 of 500 -------\n",
      "agent_timesteps_total: 536\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-31\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.01865956150436904\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 536\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.8367099231598248e-41\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.559822138150533\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.339295001462498e-06\n",
      "        policy_loss: -0.00012168223038315773\n",
      "        total_loss: -9.588425358136495e-05\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 2.5798163308839626e-05\n",
      "  num_agent_steps_sampled: 536\n",
      "  num_agent_steps_trained: 536\n",
      "  num_steps_sampled: 536\n",
      "  num_steps_trained: 536\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 134\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 21.933333333333334\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1282816264145176\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.12294227463518\n",
      "  mean_inference_ms: 11.018009051032752\n",
      "  mean_raw_obs_processing_ms: 46.58331291721016\n",
      "time_since_restore: 287.2130796909332\n",
      "time_this_iter_s: 1.796086072921753\n",
      "time_total_s: 287.2130796909332\n",
      "timers:\n",
      "  learn_throughput: 2.632\n",
      "  learn_time_ms: 1519.763\n",
      "  load_throughput: 2375.972\n",
      "  load_time_ms: 1.684\n",
      "  sample_throughput: 2.412\n",
      "  sample_time_ms: 1658.36\n",
      "  update_time_ms: 2.24\n",
      "timestamp: 1655985691\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 536\n",
      "training_iteration: 134\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 135 of 500 -------\n",
      "agent_timesteps_total: 540\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-33\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.01865955118118467\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 540\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 9.183549615799124e-42\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.559020241101583\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.533175610452114e-06\n",
      "        policy_loss: -0.0001755845732986927\n",
      "        total_loss: -0.00015752045437693595\n",
      "        vf_explained_var: -5.0520896911621095e-05\n",
      "        vf_loss: 1.8064227697323075e-05\n",
      "  num_agent_steps_sampled: 540\n",
      "  num_agent_steps_trained: 540\n",
      "  num_steps_sampled: 540\n",
      "  num_steps_trained: 540\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 135\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 22.8\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1280548958103408\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.08184007478887\n",
      "  mean_inference_ms: 11.010278335554526\n",
      "  mean_raw_obs_processing_ms: 46.54029018732338\n",
      "time_since_restore: 289.065575838089\n",
      "time_this_iter_s: 1.8524961471557617\n",
      "time_total_s: 289.065575838089\n",
      "timers:\n",
      "  learn_throughput: 2.57\n",
      "  learn_time_ms: 1556.305\n",
      "  load_throughput: 2369.663\n",
      "  load_time_ms: 1.688\n",
      "  sample_throughput: 2.399\n",
      "  sample_time_ms: 1667.654\n",
      "  update_time_ms: 2.402\n",
      "timestamp: 1655985693\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 540\n",
      "training_iteration: 135\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 136 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 544\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-35\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.02867315540948528\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 544\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.591774807899562e-42\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.552209663391113\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0004128053562453715\n",
      "        policy_loss: -0.04450285981098811\n",
      "        total_loss: 0.199954949816068\n",
      "        vf_explained_var: 5.5631001790364584e-08\n",
      "        vf_loss: 0.24445781807104747\n",
      "  num_agent_steps_sampled: 544\n",
      "  num_agent_steps_trained: 544\n",
      "  num_steps_sampled: 544\n",
      "  num_steps_trained: 544\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 136\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 23.1\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12782824044717436\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 75.03199333235848\n",
      "  mean_inference_ms: 11.002578026723452\n",
      "  mean_raw_obs_processing_ms: 46.4958031725531\n",
      "time_since_restore: 290.8363313674927\n",
      "time_this_iter_s: 1.7707555294036865\n",
      "time_total_s: 290.8363313674927\n",
      "timers:\n",
      "  learn_throughput: 2.541\n",
      "  learn_time_ms: 1573.992\n",
      "  load_throughput: 2348.829\n",
      "  load_time_ms: 1.703\n",
      "  sample_throughput: 2.346\n",
      "  sample_time_ms: 1704.966\n",
      "  update_time_ms: 2.375\n",
      "timestamp: 1655985695\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 544\n",
      "training_iteration: 136\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 137 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m /scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2905509)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 548\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-37\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.038686849329026426\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 548\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.295887403949781e-42\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5335166613260904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0005620851514355726\n",
      "        policy_loss: -0.058222938080628714\n",
      "        total_loss: 0.17377656896909077\n",
      "        vf_explained_var: 4.76837158203125e-08\n",
      "        vf_loss: 0.2319995159904162\n",
      "  num_agent_steps_sampled: 548\n",
      "  num_agent_steps_trained: 548\n",
      "  num_steps_sampled: 548\n",
      "  num_steps_trained: 548\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 137\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 24.033333333333335\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1276134250627779\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.97273576198194\n",
      "  mean_inference_ms: 10.99557396174374\n",
      "  mean_raw_obs_processing_ms: 46.45234385753627\n",
      "time_since_restore: 292.71038269996643\n",
      "time_this_iter_s: 1.8740513324737549\n",
      "time_total_s: 292.71038269996643\n",
      "timers:\n",
      "  learn_throughput: 2.5\n",
      "  learn_time_ms: 1599.727\n",
      "  load_throughput: 2477.805\n",
      "  load_time_ms: 1.614\n",
      "  sample_throughput: 2.317\n",
      "  sample_time_ms: 1726.167\n",
      "  update_time_ms: 2.316\n",
      "timestamp: 1655985697\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 548\n",
      "training_iteration: 137\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 138 of 500 -------\n",
      "agent_timesteps_total: 552\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-39\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.0386870309672567\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 552\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.1479437019748905e-42\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.518899377187093\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00011799686762212028\n",
      "        policy_loss: -0.0007442604129513105\n",
      "        total_loss: 0.002832953135172526\n",
      "        vf_explained_var: 0.000320357084274292\n",
      "        vf_loss: 0.0035772134317085145\n",
      "  num_agent_steps_sampled: 552\n",
      "  num_agent_steps_trained: 552\n",
      "  num_steps_sampled: 552\n",
      "  num_steps_trained: 552\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 138\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.2\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1273928776458434\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.91273213486349\n",
      "  mean_inference_ms: 10.98593367380905\n",
      "  mean_raw_obs_processing_ms: 46.40895876414992\n",
      "time_since_restore: 294.57895827293396\n",
      "time_this_iter_s: 1.8685755729675293\n",
      "time_total_s: 294.57895827293396\n",
      "timers:\n",
      "  learn_throughput: 2.463\n",
      "  learn_time_ms: 1623.815\n",
      "  load_throughput: 2481.8\n",
      "  load_time_ms: 1.612\n",
      "  sample_throughput: 2.285\n",
      "  sample_time_ms: 1750.766\n",
      "  update_time_ms: 2.561\n",
      "timestamp: 1655985699\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 552\n",
      "training_iteration: 138\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 139 of 500 -------\n",
      "agent_timesteps_total: 556\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-41\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.02867333113537347\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 556\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.739718509874453e-43\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.515657583872477\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.693673652729103e-06\n",
      "        policy_loss: -0.00022613791127999624\n",
      "        total_loss: 0.0027312174749871093\n",
      "        vf_explained_var: 0.000503083070119222\n",
      "        vf_loss: 0.002957355153436462\n",
      "  num_agent_steps_sampled: 556\n",
      "  num_agent_steps_trained: 556\n",
      "  num_steps_sampled: 556\n",
      "  num_steps_trained: 556\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 139\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 25.4\n",
      "  ram_util_percent: 12.5\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12715909045870546\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.85810261944661\n",
      "  mean_inference_ms: 10.973668883066757\n",
      "  mean_raw_obs_processing_ms: 46.36314715469811\n",
      "time_since_restore: 296.44822001457214\n",
      "time_this_iter_s: 1.8692617416381836\n",
      "time_total_s: 296.44822001457214\n",
      "timers:\n",
      "  learn_throughput: 2.439\n",
      "  learn_time_ms: 1639.688\n",
      "  load_throughput: 2500.069\n",
      "  load_time_ms: 1.6\n",
      "  sample_throughput: 2.252\n",
      "  sample_time_ms: 1775.904\n",
      "  update_time_ms: 2.953\n",
      "timestamp: 1655985701\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 556\n",
      "training_iteration: 139\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 140 of 500 -------\n",
      "agent_timesteps_total: 560\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-43\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659535065655604\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 560\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.8698592549372263e-43\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5145256916681924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.016557029842602e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.002145408276313295\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.002145408276313295\n",
      "  num_agent_steps_sampled: 560\n",
      "  num_agent_steps_trained: 560\n",
      "  num_steps_sampled: 560\n",
      "  num_steps_trained: 560\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 140\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.2\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12689595176787916\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.81169954666309\n",
      "  mean_inference_ms: 10.959054112497467\n",
      "  mean_raw_obs_processing_ms: 46.316750803580945\n",
      "time_since_restore: 298.3772542476654\n",
      "time_this_iter_s: 1.9290342330932617\n",
      "time_total_s: 298.3772542476654\n",
      "timers:\n",
      "  learn_throughput: 2.4\n",
      "  learn_time_ms: 1666.432\n",
      "  load_throughput: 2865.549\n",
      "  load_time_ms: 1.396\n",
      "  sample_throughput: 2.229\n",
      "  sample_time_ms: 1794.854\n",
      "  update_time_ms: 3.033\n",
      "timestamp: 1655985703\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 560\n",
      "training_iteration: 140\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 141 of 500 -------\n",
      "agent_timesteps_total: 564\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-45\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659443741295013\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 564\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.4349296274686132e-43\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.514388108253479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3800518937993427e-07\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.00154291819781065\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.00154291819781065\n",
      "  num_agent_steps_sampled: 564\n",
      "  num_agent_steps_trained: 564\n",
      "  num_steps_sampled: 564\n",
      "  num_steps_trained: 564\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 141\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 26.833333333333332\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12663986475620415\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.76144806934707\n",
      "  mean_inference_ms: 10.944892155809777\n",
      "  mean_raw_obs_processing_ms: 46.2713517485041\n",
      "time_since_restore: 300.30349707603455\n",
      "time_this_iter_s: 1.9262428283691406\n",
      "time_total_s: 300.30349707603455\n",
      "timers:\n",
      "  learn_throughput: 2.366\n",
      "  learn_time_ms: 1690.604\n",
      "  load_throughput: 2860.712\n",
      "  load_time_ms: 1.398\n",
      "  sample_throughput: 2.194\n",
      "  sample_time_ms: 1822.788\n",
      "  update_time_ms: 3.021\n",
      "timestamp: 1655985705\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 564\n",
      "training_iteration: 141\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 142 of 500 -------\n",
      "agent_timesteps_total: 568\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-47\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659450551074694\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 568\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.174648137343066e-44\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.513956602414449\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.967601763733304e-06\n",
      "        policy_loss: -0.0001797082523504893\n",
      "        total_loss: 0.0009267458692193032\n",
      "        vf_explained_var: 0.0002229611078898112\n",
      "        vf_loss: 0.0011064543835042665\n",
      "  num_agent_steps_sampled: 568\n",
      "  num_agent_steps_trained: 568\n",
      "  num_steps_sampled: 568\n",
      "  num_steps_trained: 568\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 142\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 27.5\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12637980666030446\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.70179164433466\n",
      "  mean_inference_ms: 10.93093427750505\n",
      "  mean_raw_obs_processing_ms: 46.22322553746545\n",
      "time_since_restore: 302.30503034591675\n",
      "time_this_iter_s: 2.001533269882202\n",
      "time_total_s: 302.30503034591675\n",
      "timers:\n",
      "  learn_throughput: 2.329\n",
      "  learn_time_ms: 1717.554\n",
      "  load_throughput: 2844.464\n",
      "  load_time_ms: 1.406\n",
      "  sample_throughput: 2.162\n",
      "  sample_time_ms: 1849.933\n",
      "  update_time_ms: 3.022\n",
      "timestamp: 1655985707\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 568\n",
      "training_iteration: 142\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 143 of 500 -------\n",
      "agent_timesteps_total: 572\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-49\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.01865954659645153\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 572\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.587324068671533e-44\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5125537474950153\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.257303443369684e-06\n",
      "        policy_loss: -0.00021321354433894156\n",
      "        total_loss: 0.0005765960241357486\n",
      "        vf_explained_var: -0.00031136274337768556\n",
      "        vf_loss: 0.0007898096325031171\n",
      "  num_agent_steps_sampled: 572\n",
      "  num_agent_steps_trained: 572\n",
      "  num_steps_sampled: 572\n",
      "  num_steps_trained: 572\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 143\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.0\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1260926310922038\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.64262246505562\n",
      "  mean_inference_ms: 10.914040382216019\n",
      "  mean_raw_obs_processing_ms: 46.17759026256967\n",
      "time_since_restore: 304.31385374069214\n",
      "time_this_iter_s: 2.0088233947753906\n",
      "time_total_s: 304.31385374069214\n",
      "timers:\n",
      "  learn_throughput: 2.294\n",
      "  learn_time_ms: 1743.456\n",
      "  load_throughput: 2831.072\n",
      "  load_time_ms: 1.413\n",
      "  sample_throughput: 2.127\n",
      "  sample_time_ms: 1880.882\n",
      "  update_time_ms: 3.233\n",
      "timestamp: 1655985709\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 572\n",
      "training_iteration: 143\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 144 of 500 -------\n",
      "agent_timesteps_total: 576\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-51\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659454395279045\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 576\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.7936620343357664e-44\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.511403719584147\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.3826736171414685e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0005608813875975708\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0005608813875975708\n",
      "  num_agent_steps_sampled: 576\n",
      "  num_agent_steps_trained: 576\n",
      "  num_steps_sampled: 576\n",
      "  num_steps_trained: 576\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 144\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.95\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1257904752796224\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.58186871124005\n",
      "  mean_inference_ms: 10.892641330237348\n",
      "  mean_raw_obs_processing_ms: 46.129025251654824\n",
      "time_since_restore: 306.3189871311188\n",
      "time_this_iter_s: 2.0051333904266357\n",
      "time_total_s: 306.3189871311188\n",
      "timers:\n",
      "  learn_throughput: 2.268\n",
      "  learn_time_ms: 1763.615\n",
      "  load_throughput: 2847.022\n",
      "  load_time_ms: 1.405\n",
      "  sample_throughput: 2.095\n",
      "  sample_time_ms: 1909.286\n",
      "  update_time_ms: 3.091\n",
      "timestamp: 1655985711\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 576\n",
      "training_iteration: 144\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 145 of 500 -------\n",
      "agent_timesteps_total: 580\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-53\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659547222260355\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 580\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 8.968310171678832e-45\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.510739477475484\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.233993419688886e-06\n",
      "        policy_loss: -0.00021701486160357792\n",
      "        total_loss: 0.00017817442615826926\n",
      "        vf_explained_var: -0.0001671314239501953\n",
      "        vf_loss: 0.0003951894827575112\n",
      "  num_agent_steps_sampled: 580\n",
      "  num_agent_steps_trained: 580\n",
      "  num_steps_sampled: 580\n",
      "  num_steps_trained: 580\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 145\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.2\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12553241449694358\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.52220570776143\n",
      "  mean_inference_ms: 10.87308718133801\n",
      "  mean_raw_obs_processing_ms: 46.081742694460466\n",
      "time_since_restore: 308.22669410705566\n",
      "time_this_iter_s: 1.9077069759368896\n",
      "time_total_s: 308.22669410705566\n",
      "timers:\n",
      "  learn_throughput: 2.265\n",
      "  learn_time_ms: 1766.343\n",
      "  load_throughput: 2596.57\n",
      "  load_time_ms: 1.54\n",
      "  sample_throughput: 2.072\n",
      "  sample_time_ms: 1930.45\n",
      "  update_time_ms: 3.474\n",
      "timestamp: 1655985713\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 580\n",
      "training_iteration: 145\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 146 of 500 -------\n",
      "agent_timesteps_total: 584\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-55\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659456374312405\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 584\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.484155085839416e-45\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5095835367838544\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4925686211123927e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.0002788340469123796\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.0002788340469123796\n",
      "  num_agent_steps_sampled: 584\n",
      "  num_agent_steps_trained: 584\n",
      "  num_steps_sampled: 584\n",
      "  num_steps_trained: 584\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 146\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.066666666666663\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12527941228914297\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.4617369005282\n",
      "  mean_inference_ms: 10.853905036147932\n",
      "  mean_raw_obs_processing_ms: 46.03394092847384\n",
      "time_since_restore: 310.1754205226898\n",
      "time_this_iter_s: 1.9487264156341553\n",
      "time_total_s: 310.1754205226898\n",
      "timers:\n",
      "  learn_throughput: 2.245\n",
      "  learn_time_ms: 1781.881\n",
      "  load_throughput: 2625.378\n",
      "  load_time_ms: 1.524\n",
      "  sample_throughput: 2.065\n",
      "  sample_time_ms: 1937.078\n",
      "  update_time_ms: 3.59\n",
      "timestamp: 1655985715\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 584\n",
      "training_iteration: 146\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 147 of 500 -------\n",
      "agent_timesteps_total: 588\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-57\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659456374257963\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 588\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.242077542919708e-45\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5094095945358275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.0309046890218573e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.000197628271416761\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.000197628271416761\n",
      "  num_agent_steps_sampled: 588\n",
      "  num_agent_steps_trained: 588\n",
      "  num_steps_sampled: 588\n",
      "  num_steps_trained: 588\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 147\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.5\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12504039360351019\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.40168169113281\n",
      "  mean_inference_ms: 10.832839581451157\n",
      "  mean_raw_obs_processing_ms: 45.988381465221174\n",
      "time_since_restore: 312.27544140815735\n",
      "time_this_iter_s: 2.1000208854675293\n",
      "time_total_s: 312.27544140815735\n",
      "timers:\n",
      "  learn_throughput: 2.216\n",
      "  learn_time_ms: 1805.232\n",
      "  load_throughput: 2580.912\n",
      "  load_time_ms: 1.55\n",
      "  sample_throughput: 2.049\n",
      "  sample_time_ms: 1951.717\n",
      "  update_time_ms: 3.588\n",
      "timestamp: 1655985717\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 588\n",
      "training_iteration: 147\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 148 of 500 -------\n",
      "agent_timesteps_total: 592\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-01-59\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659369659645646\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 592\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.121038771459854e-45\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.509390417734782\n",
      "        entropy_coeff: 0.0\n",
      "        kl: -1.9670449683909887e-08\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 0.00013930412654493315\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 0.00013930412654493315\n",
      "  num_agent_steps_sampled: 592\n",
      "  num_agent_steps_trained: 592\n",
      "  num_steps_sampled: 592\n",
      "  num_steps_trained: 592\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 148\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.633333333333336\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1248089977652139\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.33762707932947\n",
      "  mean_inference_ms: 10.812195196450489\n",
      "  mean_raw_obs_processing_ms: 45.93813316581015\n",
      "time_since_restore: 314.342084646225\n",
      "time_this_iter_s: 2.066643238067627\n",
      "time_total_s: 314.342084646225\n",
      "timers:\n",
      "  learn_throughput: 2.192\n",
      "  learn_time_ms: 1824.673\n",
      "  load_throughput: 2489.977\n",
      "  load_time_ms: 1.606\n",
      "  sample_throughput: 2.025\n",
      "  sample_time_ms: 1975.268\n",
      "  update_time_ms: 3.358\n",
      "timestamp: 1655985719\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 592\n",
      "training_iteration: 148\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 149 of 500 -------\n",
      "agent_timesteps_total: 596\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-01\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659455731760165\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 596\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 5.60519385729927e-46\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5082635402679445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.4987284847715425e-05\n",
      "        policy_loss: -0.0004947848618030548\n",
      "        total_loss: -0.0003977428190410137\n",
      "        vf_explained_var: 3.134012222290039e-05\n",
      "        vf_loss: 9.704208811551022e-05\n",
      "  num_agent_steps_sampled: 596\n",
      "  num_agent_steps_trained: 596\n",
      "  num_steps_sampled: 596\n",
      "  num_steps_trained: 596\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 149\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.566666666666666\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12458060548209214\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.27192685032045\n",
      "  mean_inference_ms: 10.791853377323088\n",
      "  mean_raw_obs_processing_ms: 45.880611565086895\n",
      "time_since_restore: 316.26556849479675\n",
      "time_this_iter_s: 1.9234838485717773\n",
      "time_total_s: 316.26556849479675\n",
      "timers:\n",
      "  learn_throughput: 2.185\n",
      "  learn_time_ms: 1830.716\n",
      "  load_throughput: 2299.351\n",
      "  load_time_ms: 1.74\n",
      "  sample_throughput: 2.003\n",
      "  sample_time_ms: 1996.576\n",
      "  update_time_ms: 3.029\n",
      "timestamp: 1655985721\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 596\n",
      "training_iteration: 149\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 150 of 500 -------\n",
      "agent_timesteps_total: 600\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-03\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659359760150804\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 600\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.802596928649635e-46\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5060012340545654\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.87451796939331e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 6.65238848644852e-05\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 6.65238848644852e-05\n",
      "  num_agent_steps_sampled: 600\n",
      "  num_agent_steps_trained: 600\n",
      "  num_steps_sampled: 600\n",
      "  num_steps_trained: 600\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 150\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.566666666666666\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.1243876816690896\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.20602251050174\n",
      "  mean_inference_ms: 10.773587889128482\n",
      "  mean_raw_obs_processing_ms: 45.824726737734416\n",
      "time_since_restore: 318.3425440788269\n",
      "time_this_iter_s: 2.0769755840301514\n",
      "time_total_s: 318.3425440788269\n",
      "timers:\n",
      "  learn_throughput: 2.168\n",
      "  learn_time_ms: 1845.267\n",
      "  load_throughput: 2003.561\n",
      "  load_time_ms: 1.996\n",
      "  sample_throughput: 1.998\n",
      "  sample_time_ms: 2001.685\n",
      "  update_time_ms: 2.933\n",
      "timestamp: 1655985723\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 600\n",
      "training_iteration: 150\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 151 of 500 -------\n",
      "agent_timesteps_total: 604\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-05\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659265985929503\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 604\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.4012984643248175e-46\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.505350128809611\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 2.4598108620802843e-06\n",
      "        policy_loss: -0.00014493552347024283\n",
      "        total_loss: -9.932083388169607e-05\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 4.5614645205205304e-05\n",
      "  num_agent_steps_sampled: 604\n",
      "  num_agent_steps_trained: 604\n",
      "  num_steps_sampled: 604\n",
      "  num_steps_trained: 604\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 151\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.2\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12420424856005435\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.14289663018783\n",
      "  mean_inference_ms: 10.756910273868318\n",
      "  mean_raw_obs_processing_ms: 45.771341860490196\n",
      "time_since_restore: 320.30801272392273\n",
      "time_this_iter_s: 1.9654686450958252\n",
      "time_total_s: 320.30801272392273\n",
      "timers:\n",
      "  learn_throughput: 2.166\n",
      "  learn_time_ms: 1846.956\n",
      "  load_throughput: 1991.337\n",
      "  load_time_ms: 2.009\n",
      "  sample_throughput: 1.982\n",
      "  sample_time_ms: 2018.612\n",
      "  update_time_ms: 2.996\n",
      "timestamp: 1655985725\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 604\n",
      "training_iteration: 151\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 152 of 500 -------\n",
      "agent_timesteps_total: 608\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-07\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659358448101585\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 608\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 7.006492321624088e-47\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.503998287518819\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 5.665276173279684e-06\n",
      "        policy_loss: -0.00022087773929039636\n",
      "        total_loss: -0.00019000091900428135\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 3.0876760987060456e-05\n",
      "  num_agent_steps_sampled: 608\n",
      "  num_agent_steps_trained: 608\n",
      "  num_steps_sampled: 608\n",
      "  num_steps_trained: 608\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 152\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.033333333333335\n",
      "  ram_util_percent: 12.6\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12402653565597196\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.08605213979625\n",
      "  mean_inference_ms: 10.740436903696285\n",
      "  mean_raw_obs_processing_ms: 45.72060296932962\n",
      "time_since_restore: 322.39259696006775\n",
      "time_this_iter_s: 2.0845842361450195\n",
      "time_total_s: 322.39259696006775\n",
      "timers:\n",
      "  learn_throughput: 2.157\n",
      "  learn_time_ms: 1854.778\n",
      "  load_throughput: 2005.189\n",
      "  load_time_ms: 1.995\n",
      "  sample_throughput: 1.98\n",
      "  sample_time_ms: 2020.562\n",
      "  update_time_ms: 3.116\n",
      "timestamp: 1655985727\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 608\n",
      "training_iteration: 152\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 153 of 500 -------\n",
      "agent_timesteps_total: 612\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-09\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659358448047142\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 612\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 3.503246160812044e-47\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.5026267528533936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.609855211863002e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 2.068952423239049e-05\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 2.068952423239049e-05\n",
      "  num_agent_steps_sampled: 612\n",
      "  num_agent_steps_trained: 612\n",
      "  num_steps_sampled: 612\n",
      "  num_steps_trained: 612\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 153\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.400000000000002\n",
      "  ram_util_percent: 12.666666666666666\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12384517695795343\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 74.02605150319536\n",
      "  mean_inference_ms: 10.723848032995171\n",
      "  mean_raw_obs_processing_ms: 45.69459952896445\n",
      "time_since_restore: 324.8214228153229\n",
      "time_this_iter_s: 2.428825855255127\n",
      "time_total_s: 324.8214228153229\n",
      "timers:\n",
      "  learn_throughput: 2.146\n",
      "  learn_time_ms: 1863.73\n",
      "  load_throughput: 1998.073\n",
      "  load_time_ms: 2.002\n",
      "  sample_throughput: 1.938\n",
      "  sample_time_ms: 2063.757\n",
      "  update_time_ms: 3.058\n",
      "timestamp: 1655985729\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 612\n",
      "training_iteration: 153\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 154 of 500 -------\n",
      "agent_timesteps_total: 616\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-11\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.01865935963479479\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 616\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.751623080406022e-47\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.501893885930379\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.946050039384014e-06\n",
      "        policy_loss: -0.00027281505366166434\n",
      "        total_loss: -0.0002590237185359001\n",
      "        vf_explained_var: -5.066394805908203e-05\n",
      "        vf_loss: 1.3791771198157222e-05\n",
      "  num_agent_steps_sampled: 616\n",
      "  num_agent_steps_trained: 616\n",
      "  num_steps_sampled: 616\n",
      "  num_steps_trained: 616\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 154\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.899999999999995\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12367838570939524\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.96775618226327\n",
      "  mean_inference_ms: 10.709868219398128\n",
      "  mean_raw_obs_processing_ms: 45.67024819480674\n",
      "time_since_restore: 326.8781244754791\n",
      "time_this_iter_s: 2.05670166015625\n",
      "time_total_s: 326.8781244754791\n",
      "timers:\n",
      "  learn_throughput: 2.141\n",
      "  learn_time_ms: 1868.327\n",
      "  load_throughput: 1982.958\n",
      "  load_time_ms: 2.017\n",
      "  sample_throughput: 1.931\n",
      "  sample_time_ms: 2071.975\n",
      "  update_time_ms: 3.193\n",
      "timestamp: 1655985731\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 616\n",
      "training_iteration: 154\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 155 of 500 -------\n",
      "agent_timesteps_total: 620\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-14\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659446207599545\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 620\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 8.75811540203011e-48\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.4996299266815187\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.550183559189122e-05\n",
      "        policy_loss: -0.0004704277962446213\n",
      "        total_loss: -0.00046158075953523317\n",
      "        vf_explained_var: -3.5881996154785156e-05\n",
      "        vf_loss: 8.84686107080294e-06\n",
      "  num_agent_steps_sampled: 620\n",
      "  num_agent_steps_trained: 620\n",
      "  num_steps_sampled: 620\n",
      "  num_steps_trained: 620\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 155\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.833333333333332\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12352884828392377\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.91414542606623\n",
      "  mean_inference_ms: 10.696749525831349\n",
      "  mean_raw_obs_processing_ms: 45.6460370043977\n",
      "time_since_restore: 328.9740562438965\n",
      "time_this_iter_s: 2.0959317684173584\n",
      "time_total_s: 328.9740562438965\n",
      "timers:\n",
      "  learn_throughput: 2.117\n",
      "  learn_time_ms: 1889.313\n",
      "  load_throughput: 2080.714\n",
      "  load_time_ms: 1.922\n",
      "  sample_throughput: 1.925\n",
      "  sample_time_ms: 2077.919\n",
      "  update_time_ms: 2.938\n",
      "timestamp: 1655985734\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 620\n",
      "training_iteration: 155\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 156 of 500 -------\n",
      "agent_timesteps_total: 624\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-16\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.01865935347912226\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 624\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 4.379057701015055e-48\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.497298256556193\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.0846533814354872e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 5.473472416876272e-06\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 5.473472416876272e-06\n",
      "  num_agent_steps_sampled: 624\n",
      "  num_agent_steps_trained: 624\n",
      "  num_steps_sampled: 624\n",
      "  num_steps_trained: 624\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 156\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.133333333333336\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12338133225272666\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.86309175819704\n",
      "  mean_inference_ms: 10.683640705182109\n",
      "  mean_raw_obs_processing_ms: 45.62279654711435\n",
      "time_since_restore: 331.0737900733948\n",
      "time_this_iter_s: 2.099733829498291\n",
      "time_total_s: 331.0737900733948\n",
      "timers:\n",
      "  learn_throughput: 2.101\n",
      "  learn_time_ms: 1904.171\n",
      "  load_throughput: 1985.798\n",
      "  load_time_ms: 2.014\n",
      "  sample_throughput: 1.907\n",
      "  sample_time_ms: 2097.826\n",
      "  update_time_ms: 2.995\n",
      "timestamp: 1655985736\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 624\n",
      "training_iteration: 156\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 157 of 500 -------\n",
      "agent_timesteps_total: 628\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-18\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659443254957178\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 628\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.1895288505075274e-48\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.4965372641881305\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 3.7453030917807457e-06\n",
      "        policy_loss: -0.00018210082004467646\n",
      "        total_loss: -0.0001786155936618646\n",
      "        vf_explained_var: -5.76178232828776e-07\n",
      "        vf_loss: 3.4853505061012886e-06\n",
      "  num_agent_steps_sampled: 628\n",
      "  num_agent_steps_trained: 628\n",
      "  num_steps_sampled: 628\n",
      "  num_steps_trained: 628\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 157\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 31.666666666666668\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12322554929899797\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.81500007873444\n",
      "  mean_inference_ms: 10.669860546190792\n",
      "  mean_raw_obs_processing_ms: 45.59941341306281\n",
      "time_since_restore: 333.0485064983368\n",
      "time_this_iter_s: 1.9747164249420166\n",
      "time_total_s: 333.0485064983368\n",
      "timers:\n",
      "  learn_throughput: 2.116\n",
      "  learn_time_ms: 1890.742\n",
      "  load_throughput: 1967.24\n",
      "  load_time_ms: 2.033\n",
      "  sample_throughput: 1.893\n",
      "  sample_time_ms: 2113.336\n",
      "  update_time_ms: 3.029\n",
      "timestamp: 1655985738\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 628\n",
      "training_iteration: 157\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 158 of 500 -------\n",
      "agent_timesteps_total: 632\n",
      "custom_metrics: {}\n",
      "date: 2022-06-23_13-02-20\n",
      "done: false\n",
      "episode_len_mean: 1.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 0.0013693919704467286\n",
      "episode_reward_mean: -0.018659347807220422\n",
      "episode_reward_min: -1.0\n",
      "episodes_this_iter: 4\n",
      "episodes_total: 632\n",
      "experiment_id: 07575fbcc2df4014a1a7692320f0bfdf\n",
      "hostname: mammoth.ee.ucl.ac.uk\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0947644252537637e-48\n",
      "        cur_lr: 4.999999999999999e-05\n",
      "        entropy: 2.4954856395721436\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1508387918487036e-06\n",
      "        policy_loss: 0.0\n",
      "        total_loss: 2.179053103645856e-06\n",
      "        vf_explained_var: .nan\n",
      "        vf_loss: 2.179053103645856e-06\n",
      "  num_agent_steps_sampled: 632\n",
      "  num_agent_steps_trained: 632\n",
      "  num_steps_sampled: 632\n",
      "  num_steps_trained: 632\n",
      "  num_steps_trained_this_iter: 0\n",
      "iterations_since_restore: 158\n",
      "node_ip: 128.40.41.23\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 32.833333333333336\n",
      "  ram_util_percent: 12.699999999999998\n",
      "pid: 2902978\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12308909366033839\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 73.76954009398615\n",
      "  mean_inference_ms: 10.658787638528572\n",
      "  mean_raw_obs_processing_ms: 45.57728115613052\n",
      "time_since_restore: 335.10061049461365\n",
      "time_this_iter_s: 2.0521039962768555\n",
      "time_total_s: 335.10061049461365\n",
      "timers:\n",
      "  learn_throughput: 2.117\n",
      "  learn_time_ms: 1889.055\n",
      "  load_throughput: 2022.667\n",
      "  load_time_ms: 1.978\n",
      "  sample_throughput: 1.903\n",
      "  sample_time_ms: 2101.755\n",
      "  update_time_ms: 3.129\n",
      "timestamp: 1655985740\n",
      "timesteps_since_restore: 0\n",
      "timesteps_this_iter: 0\n",
      "timesteps_total: 632\n",
      "training_iteration: 158\n",
      "trial_id: default\n",
      "\n",
      "\n",
      "------- Epoch 159 of 500 -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-b2c08f591d47>\", line 8, in <module>\n",
      "    result = epoch_loop.train()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/tune/trainable.py\", line 314, in train\n",
      "    result = self.step()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 867, in step\n",
      "    result = self.step_attempt()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 920, in step_attempt\n",
      "    step_results = next(self.train_exec_impl)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 791, in apply_foreach\n",
      "    result = fn(item)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/execution/train_ops.py\", line 197, in __call__\n",
      "    results = policy.learn_on_loaded_batch(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 554, in learn_on_loaded_batch\n",
      "    tower_outputs = self._multi_gpu_parallel_grad_calc(device_batches)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 1081, in _multi_gpu_parallel_grad_calc\n",
      "    _worker(shard_idx, model, sample_batch, device)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 1012, in _worker\n",
      "    self._loss(self, model, self.dist_class, sample_batch))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 81, in loss\n",
      "    logits, state = model(train_batch)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/models/modelv2.py\", line 244, in __call__\n",
      "    res = self.forward(restored, state or [], seq_lens)\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/ml_models/policies.py\", line 218, in forward\n",
      "    embs = self.gnn(graph)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/ml_models/models.py\", line 164, in forward\n",
      "    output = layer(graph)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/ml_models/models.py\", line 62, in forward\n",
      "    graph.update_all(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/heterograph.py\", line 4895, in update_all\n",
      "    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/core.py\", line 372, in message_passing\n",
      "    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/core.py\", line 156, in invoke_udf_reduce\n",
      "    retf.update_row(merged_nodes, merged_rst)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/frame.py\", line 636, in update_row\n",
      "    self.add_column(key, scheme, ctx)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/frame.py\", line 568, in add_column\n",
      "    init_data = initializer((self.num_rows,) + scheme.shape, scheme.dtype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/init.py\", line 61, in zero_initializer\n",
      "    return F.zeros(shape, dtype, ctx)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py\", line 226, in zeros\n",
      "    return th.zeros(shape, dtype=dtype, device=ctx)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 752, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 721, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 706, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-b2c08f591d47>\", line 8, in <module>\n",
      "    result = epoch_loop.train()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/tune/trainable.py\", line 314, in train\n",
      "    result = self.step()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 867, in step\n",
      "    result = self.step_attempt()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 920, in step_attempt\n",
      "    step_results = next(self.train_exec_impl)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 791, in apply_foreach\n",
      "    result = fn(item)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/execution/train_ops.py\", line 197, in __call__\n",
      "    results = policy.learn_on_loaded_batch(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 554, in learn_on_loaded_batch\n",
      "    tower_outputs = self._multi_gpu_parallel_grad_calc(device_batches)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 1081, in _multi_gpu_parallel_grad_calc\n",
      "    _worker(shard_idx, model, sample_batch, device)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 1012, in _worker\n",
      "    self._loss(self, model, self.dist_class, sample_batch))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 81, in loss\n",
      "    logits, state = model(train_batch)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/models/modelv2.py\", line 244, in __call__\n",
      "    res = self.forward(restored, state or [], seq_lens)\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/ml_models/policies.py\", line 218, in forward\n",
      "    embs = self.gnn(graph)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/ml_models/models.py\", line 164, in forward\n",
      "    output = layer(graph)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/ml_models/models.py\", line 62, in forward\n",
      "    graph.update_all(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/heterograph.py\", line 4895, in update_all\n",
      "    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/core.py\", line 372, in message_passing\n",
      "    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/core.py\", line 156, in invoke_udf_reduce\n",
      "    retf.update_row(merged_nodes, merged_rst)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/frame.py\", line 636, in update_row\n",
      "    self.add_column(key, scheme, ctx)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/frame.py\", line 568, in add_column\n",
      "    init_data = initializer((self.num_rows,) + scheme.shape, scheme.dtype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/init.py\", line 61, in zero_initializer\n",
      "    return F.zeros(shape, dtype, ctx)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py\", line 226, in zeros\n",
      "    return th.zeros(shape, dtype=dtype, device=ctx)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2079, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/posixpath.py\", line 392, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/posixpath.py\", line 426, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-b2c08f591d47>\", line 8, in <module>\n",
      "    result = epoch_loop.train()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/tune/trainable.py\", line 314, in train\n",
      "    result = self.step()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 867, in step\n",
      "    result = self.step_attempt()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\", line 920, in step_attempt\n",
      "    step_results = next(self.train_exec_impl)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 756, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 843, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 783, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/util/iter.py\", line 791, in apply_foreach\n",
      "    result = fn(item)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/execution/train_ops.py\", line 197, in __call__\n",
      "    results = policy.learn_on_loaded_batch(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 554, in learn_on_loaded_batch\n",
      "    tower_outputs = self._multi_gpu_parallel_grad_calc(device_batches)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 1081, in _multi_gpu_parallel_grad_calc\n",
      "    _worker(shard_idx, model, sample_batch, device)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 1012, in _worker\n",
      "    self._loss(self, model, self.dist_class, sample_batch))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 81, in loss\n",
      "    logits, state = model(train_batch)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/models/modelv2.py\", line 244, in __call__\n",
      "    res = self.forward(restored, state or [], seq_lens)\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/ml_models/policies.py\", line 218, in forward\n",
      "    embs = self.gnn(graph)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/ml_models/models.py\", line 164, in forward\n",
      "    output = layer(graph)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/ml_models/models.py\", line 62, in forward\n",
      "    graph.update_all(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/heterograph.py\", line 4895, in update_all\n",
      "    ndata = core.message_passing(g, message_func, reduce_func, apply_node_func)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/core.py\", line 372, in message_passing\n",
      "    ndata = invoke_udf_reduce(g, rfunc, msgdata, orig_nid=orig_nid)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/core.py\", line 156, in invoke_udf_reduce\n",
      "    retf.update_row(merged_nodes, merged_rst)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/frame.py\", line 636, in update_row\n",
      "    self.add_column(key, scheme, ctx)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/frame.py\", line 568, in add_column\n",
      "    init_data = initializer((self.num_rows,) + scheme.shape, scheme.dtype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/init.py\", line 61, in zero_initializer\n",
      "    return F.zeros(shape, dtype, ctx)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py\", line 226, in zeros\n",
      "    return th.zeros(shape, dtype=dtype, device=ctx)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2079, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3396, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2079, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 752, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 721, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 706, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "agent_name = 'PPO'\n",
    "num_epochs = 500\n",
    "rl_training_stats = defaultdict(lambda: [])\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\n------- Epoch {epoch+1} of {num_epochs} -------')\n",
    "    result = epoch_loop.train()\n",
    "    \n",
    "    # print epoch data\n",
    "    # print(pretty_print(result))\n",
    "    \n",
    "    # save epoch data\n",
    "    for key, val in result['hist_stats'].items():\n",
    "        rl_training_stats[key].extend(val)\n",
    "    for _ in range(len(val)):\n",
    "        rl_training_stats['seed'].append(result['config']['seed'])\n",
    "        rl_training_stats['agent'].append(agent_name)\n",
    "        rl_training_stats['epoch'].append(epoch)\n",
    "        \n",
    "# display(pd.DataFrame(rl_training_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f04b3d-d4ca-471b-bdc6-f758f9abbff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting metric episode_lengths\n",
      "ERROR! Session/line number was not unique in database. History logging moved to new session 2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-a9472bd64616>\", line 11, in <module>\n",
      "    fig = plot_line(pd.DataFrame(rl_training_stats),\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/plotting/plotting.py\", line 314, in plot_line\n",
      "    g = sns.lineplot(data=df,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/_decorators.py\", line 46, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 710, in lineplot\n",
      "    p.plot(ax, kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 499, in plot\n",
      "    x, y, y_ci = self.aggregate(y, x, u)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 414, in aggregate\n",
      "    cis = grouped.apply(bootstrapped_cis)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/generic.py\", line 244, in apply\n",
      "    return super().apply(func, *args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1414, in apply\n",
      "    result = self._python_apply_general(f, self._selected_obj)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1455, in _python_apply_general\n",
      "    values, mutated = self.grouper.apply(f, data, self.axis)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/ops.py\", line 761, in apply\n",
      "    res = f(group)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 395, in bootstrapped_cis\n",
      "    boots = bootstrap(vals, func=func, n_boot=n_boot, seed=seed)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/algorithms.py\", line 83, in bootstrap\n",
      "    resampler = integers(0, n, n, dtype=np.intp)  # intp is indexing dtype\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 752, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 721, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 706, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-a9472bd64616>\", line 11, in <module>\n",
      "    fig = plot_line(pd.DataFrame(rl_training_stats),\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/plotting/plotting.py\", line 314, in plot_line\n",
      "    g = sns.lineplot(data=df,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/_decorators.py\", line 46, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 710, in lineplot\n",
      "    p.plot(ax, kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 499, in plot\n",
      "    x, y, y_ci = self.aggregate(y, x, u)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 414, in aggregate\n",
      "    cis = grouped.apply(bootstrapped_cis)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/generic.py\", line 244, in apply\n",
      "    return super().apply(func, *args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1414, in apply\n",
      "    result = self._python_apply_general(f, self._selected_obj)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1455, in _python_apply_general\n",
      "    values, mutated = self.grouper.apply(f, data, self.axis)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/ops.py\", line 761, in apply\n",
      "    res = f(group)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 395, in bootstrapped_cis\n",
      "    boots = bootstrap(vals, func=func, n_boot=n_boot, seed=seed)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/algorithms.py\", line 83, in bootstrap\n",
      "    resampler = integers(0, n, n, dtype=np.intp)  # intp is indexing dtype\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2079, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 752, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 721, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 706, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-a9472bd64616>\", line 11, in <module>\n",
      "    fig = plot_line(pd.DataFrame(rl_training_stats),\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/plotting/plotting.py\", line 314, in plot_line\n",
      "    g = sns.lineplot(data=df,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/_decorators.py\", line 46, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 710, in lineplot\n",
      "    p.plot(ax, kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 499, in plot\n",
      "    x, y, y_ci = self.aggregate(y, x, u)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 414, in aggregate\n",
      "    cis = grouped.apply(bootstrapped_cis)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/generic.py\", line 244, in apply\n",
      "    return super().apply(func, *args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1414, in apply\n",
      "    result = self._python_apply_general(f, self._selected_obj)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1455, in _python_apply_general\n",
      "    values, mutated = self.grouper.apply(f, data, self.axis)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/ops.py\", line 761, in apply\n",
      "    res = f(group)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 395, in bootstrapped_cis\n",
      "    boots = bootstrap(vals, func=func, n_boot=n_boot, seed=seed)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/algorithms.py\", line 83, in bootstrap\n",
      "    resampler = integers(0, n, n, dtype=np.intp)  # intp is indexing dtype\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2079, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3396, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2079, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/posixpath.py\", line 392, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/posixpath.py\", line 425, in _joinrealpath\n",
      "    newpath = join(path, name)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/posixpath.py\", line 88, in join\n",
      "    path += sep + b\n",
      "KeyboardInterrupt\n",
      "Error in callback <function install_repl_displayhook.<locals>.post_execute at 0x7efbab98d0d0> (for post_execute):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-a9472bd64616>\", line 11, in <module>\n",
      "    fig = plot_line(pd.DataFrame(rl_training_stats),\n",
      "  File \"/home/zciccwf/phd_project/projects/ddls/ddls/plotting/plotting.py\", line 314, in plot_line\n",
      "    g = sns.lineplot(data=df,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/_decorators.py\", line 46, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 710, in lineplot\n",
      "    p.plot(ax, kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 499, in plot\n",
      "    x, y, y_ci = self.aggregate(y, x, u)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 414, in aggregate\n",
      "    cis = grouped.apply(bootstrapped_cis)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/generic.py\", line 244, in apply\n",
      "    return super().apply(func, *args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1414, in apply\n",
      "    result = self._python_apply_general(f, self._selected_obj)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/groupby.py\", line 1455, in _python_apply_general\n",
      "    values, mutated = self.grouper.apply(f, data, self.axis)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/pandas/core/groupby/ops.py\", line 761, in apply\n",
      "    res = f(group)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/relational.py\", line 395, in bootstrapped_cis\n",
      "    boots = bootstrap(vals, func=func, n_boot=n_boot, seed=seed)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/seaborn/algorithms.py\", line 83, in bootstrap\n",
      "    resampler = integers(0, n, n, dtype=np.intp)  # intp is indexing dtype\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3377, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3474, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2079, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2960, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3185, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3396, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2079, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2914, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2965, in _run_cell\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2079, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/events.py\", line 89, in trigger\n",
      "    func(*args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/pyplot.py\", line 138, in post_execute\n",
      "    draw_all()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/_pylab_helpers.py\", line 137, in draw_all\n",
      "    manager.canvas.draw_idle()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/backend_bases.py\", line 2060, in draw_idle\n",
      "    self.draw(*args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\", line 436, in draw\n",
      "    self.figure.draw(self.renderer)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/artist.py\", line 73, in draw_wrapper\n",
      "    result = draw(artist, renderer, *args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/artist.py\", line 50, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/figure.py\", line 2810, in draw\n",
      "    mimage._draw_list_compositing_images(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/artist.py\", line 50, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/axes/_base.py\", line 3082, in draw\n",
      "    mimage._draw_list_compositing_images(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/image.py\", line 132, in _draw_list_compositing_images\n",
      "    a.draw(renderer)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/artist.py\", line 50, in draw_wrapper\n",
      "    return draw(artist, renderer)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/axis.py\", line 1159, in draw\n",
      "    ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/axis.py\", line 1085, in _get_tick_bboxes\n",
      "    return ([tick.label1.get_window_extent(renderer)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/axis.py\", line 1085, in <listcomp>\n",
      "    return ([tick.label1.get_window_extent(renderer)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/text.py\", line 910, in get_window_extent\n",
      "    bbox, info, descent = self._get_layout(self._renderer)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/text.py\", line 309, in _get_layout\n",
      "    _, lp_h, lp_d = renderer.get_text_width_height_descent(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py\", line 259, in get_text_width_height_descent\n",
      "    w, h, d = texmanager.get_text_width_height_descent(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/texmanager.py\", line 338, in get_text_width_height_descent\n",
      "    page, = dvi\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/dviread.py\", line 237, in __iter__\n",
      "    while self._read():\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/dviread.py\", line 316, in _read\n",
      "    self._dtable[byte](self, byte)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/dviread.py\", line 166, in wrapper\n",
      "    return method(self, *[f(self, byte-min) for f in get_args])\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/dviread.py\", line 467, in _fnt_def\n",
      "    self._fnt_def_real(k, c, s, d, a, l)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/dviread.py\", line 472, in _fnt_def_real\n",
      "    tfm = _tfmfile(fontname)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/dviread.py\", line 1063, in _fontfile\n",
      "    filename = find_tex_file(texname + suffix)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/_api/deprecation.py\", line 386, in wrapper\n",
      "    return func(*inner_args, **inner_kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/dviread.py\", line 1034, in find_tex_file\n",
      "    lk = _LuatexKpsewhich()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/dviread.py\", line 980, in __new__\n",
      "    self._proc = self._new_proc()\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/matplotlib/dviread.py\", line 984, in _new_proc\n",
      "    return subprocess.Popen(\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/subprocess.py\", line 951, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/subprocess.py\", line 1777, in _execute_child\n",
      "    part = os.read(errpipe_read, 50000)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 752, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 721, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/inspect.py\", line 706, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "x = 'epoch'\n",
    "scaling_factor = 1\n",
    "metrics_to_plot = {'episode_reward', 'episode_lengths'}\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    print(f'Plotting metric {metric}')\n",
    "    fig = plt.figure()\n",
    "    fig = plot_line(pd.DataFrame(rl_training_stats), \n",
    "                    x=x, \n",
    "                    y=metric, \n",
    "                    hue='agent', \n",
    "                    xlabel=x, \n",
    "                    ylabel=metric, \n",
    "                    err_style='band', # 'band' 'bars'\n",
    "                    ci=68, # 95 68\n",
    "                    scaling_factor=scaling_factor,\n",
    "                    show_fig=False)\n",
    "#     plt.axhline(y=np.mean(random_baseline_stats[metric]), linestyle='--', color='#a84a32', label='Random')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2fac5-8e99-4cd8-abcc-f9b821a2f6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e4b45-ca53-48ed-9ff2-12cdec74c5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2232cfe0-7145-40fb-a471-9f454d659be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcd66f-41d2-452c-892e-3939e66810d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a18c99-e790-4380-b8f4-9214334a1ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddls",
   "language": "python",
   "name": "ddls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
