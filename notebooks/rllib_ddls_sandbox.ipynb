{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72242bad",
   "metadata": {},
   "source": [
    "# Testing compatability of `ddls` with `rllib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef9735c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "from ddls.environments.job_placing.job_placing_all_nodes_environment import JobPlacingAllNodesEnvironment\n",
    "from ddls.devices.processors.gpus.A100 import A100\n",
    "from ddls.distributions.uniform import Uniform\n",
    "from ddls.dgl_tests.rllib_model_test import GNNPolicy\n",
    "from ddls.plotting.plotting import plot_line\n",
    "\n",
    "import ray\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f2aaff",
   "metadata": {},
   "source": [
    "### Register the custom environment with `ray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf272118",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('job_placing_all_nodes_environment', lambda env_config: JobPlacingAllNodesEnvironment(**env_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbaa9f4",
   "metadata": {},
   "source": [
    "### Register the custom model with `rllib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b117a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model('my_model', GNNPolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c456568",
   "metadata": {},
   "source": [
    "### Load `rllib` config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbbfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load config\n",
    "# config = OmegaConf.load('configs/ddls_job_placing_rllib.yaml')\n",
    "# print(OmegaConf.to_yaml(config))\n",
    "\n",
    "# # convert config to dict so that is comparible with rllib\n",
    "# config = OmegaConf.to_container(config, resolve=False)\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed6ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'env': 'job_placing_all_nodes_environment', 'env_config': {'node_config': {'type_1': {'num_nodes': 16, 'workers_config': [{'num_workers': 4, 'worker': 'ddls.devices.processors.gpus.A100.A100'}]}}, 'topology_config': {'type': 'torus', 'kwargs': {'x_dims': 4, 'y_dims': 4}}, 'jobs_config': {'path_to_files': '/scratch/datasets/ddls/jobs/tensorflow_synthetic_graphs/valid', 'job_interarrival_time_dist': <ddls.distributions.uniform.Uniform object at 0x7fe3d06440d0>, 'max_files': 1, 'job_sampling_mode': 'remove'}, 'reward_function': 'mean_job_completion_time'}, 'batch_mode': 'complete_episodes', 'train_batch_size': 1, 'sgd_minibatch_size': 1, 'model': {'fcnet_hiddens': [8], 'fcnet_activation': 'relu', 'custom_model': 'my_model', 'custom_model_config': {'in_features_node': 5, 'in_features_edge': 1, 'out_features_msg': 8, 'out_features_hidden': 16, 'out_features': 4, 'in_features_graph': 130, 'out_features_graph': 4, 'num_layers': 1, 'aggregator_type': 'mean'}}, 'framework': 'torch'}\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "node_config = {'type_1':\n",
    "                  {\n",
    "                      'num_nodes': 16,\n",
    "                      'workers_config': \n",
    "                          [\n",
    "                              {\n",
    "                               'num_workers': 4,\n",
    "#                                'worker': A100\n",
    "                               'worker': 'ddls.devices.processors.gpus.A100.A100'\n",
    "                              }\n",
    "                          ]\n",
    "                  }\n",
    "              }\n",
    "\n",
    "topology_config = {'type':\n",
    "                      'torus',\n",
    "                   'kwargs':\n",
    "                      {\n",
    "                          'x_dims': 4,\n",
    "                          'y_dims': 4\n",
    "                      }\n",
    "                  }\n",
    "\n",
    "jobs_config = {'path_to_files': '/scratch/datasets/ddls/jobs/tensorflow_synthetic_graphs/valid',\n",
    "               'job_interarrival_time_dist': Uniform(min_val=1, max_val=1000),\n",
    "               'max_files': 1,\n",
    "               'job_sampling_mode': 'remove'}\n",
    "\n",
    "\n",
    "env_config = {'node_config': node_config,\n",
    "              'topology_config': topology_config,\n",
    "              'jobs_config': jobs_config,\n",
    "              'reward_function': 'mean_job_completion_time'}\n",
    "\n",
    "\n",
    "model_config = {\n",
    "        'in_features_node':5,\n",
    "        'in_features_edge':1,\n",
    "        'out_features_msg':8,\n",
    "        'out_features_hidden':16,\n",
    "        'out_features':4,\n",
    "        'in_features_graph':130,\n",
    "        'out_features_graph':4,\n",
    "        'num_layers':1,\n",
    "        'aggregator_type':'mean'\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "rllib_config = {\n",
    "    \n",
    "    'seed': 0,\n",
    "    \n",
    "    'env': 'job_placing_all_nodes_environment',\n",
    "    \n",
    "    'env_config': env_config,\n",
    "    \n",
    "    'batch_mode': 'complete_episodes',\n",
    "    'train_batch_size': 1, # 128\n",
    "    'sgd_minibatch_size': 1, # 128\n",
    "    \n",
    "    'model':{\n",
    "            'fcnet_hiddens':[8],\n",
    "            'fcnet_activation':'relu',\n",
    "            'custom_model':'my_model',\n",
    "            'custom_model_config': model_config\n",
    "        },\n",
    "    \n",
    "    'framework': 'torch'\n",
    "    \n",
    "    }\n",
    "\n",
    "# print(OmegaConf.to_yaml(rllib_config))\n",
    "print(rllib_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a9c9d",
   "metadata": {},
   "source": [
    "### Initialise an `rllib` epoch loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be406d9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 16:22:07,006\tWARNING ppo.py:223 -- `train_batch_size` (1) cannot be achieved with your other settings (num_workers=2 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "{'num_workers': 2, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 200, 'batch_mode': 'complete_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 1, 'model': {'fcnet_hiddens': [8], 'fcnet_activation': 'relu', 'custom_model': 'my_model', 'custom_model_config': {'in_features_node': 5, 'in_features_edge': 1, 'out_features_msg': 8, 'out_features_hidden': 16, 'out_features': 4, 'in_features_graph': 130, 'out_features_graph': 4, 'num_layers': 1, 'aggregator_type': 'mean'}}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'job_placing_all_nodes_environment', 'observation_space': None, 'action_space': None, 'env_config': {'node_config': {'type_1': {'num_nodes': 16, 'workers_config': [{'num_workers': 4, 'worker': 'ddls.devices.processors.gpus.A100.A100'}]}}, 'topology_config': {'type': 'torus', 'kwargs': {'x_dims': 4, 'y_dims': 4}}, 'jobs_config': {'path_to_files': '/scratch/datasets/ddls/jobs/tensorflow_synthetic_graphs/valid', 'job_interarrival_time_dist': <ddls.distributions.uniform.Uniform object at 0x7fe3d06440d0>, 'max_files': 1, 'job_sampling_mode': 'remove'}, 'reward_function': 'mean_job_completion_time'}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': None, 'seed': 0, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': None, 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 1, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m 2022-04-14 16:22:13,859\tWARNING catalog.py:558 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m 2022-04-14 16:22:13,907\tERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2044987, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f98b4c902e0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 586, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1577, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     self[policy_id] = class_(observation_space, action_space,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 50, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     self._initialize_loss_from_dummy_batch()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 832, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 294, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     return self._compute_action_helper(input_dict, state_batches,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 934, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     dist_inputs, state_out = self.model(input_dict, state_batches,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/models/modelv2.py\", line 243, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/home/zciccwf/phd_project/projects/ddls/ddls/dgl_tests/rllib_model_test.py\", line 150, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     graph.edata['z'] = torch.Tensor(edge_features)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/view.py\", line 198, in __setitem__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     self._graph._set_e_repr(self._etid, self._edges, {key : val})\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/heterograph.py\", line 4220, in _set_e_repr\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m     raise DGLError('Expect number of features to match number of edges.'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044987)\u001b[0m dgl._ffi.base.DGLError: Expect number of features to match number of edges. Got 13120 and 145920 instead.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m 2022-04-14 16:22:13,934\tWARNING catalog.py:558 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']!\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2044934, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f170fb422e0>)\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 586, in __init__\n    self._build_policy_map(\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1577, in _build_policy_map\n    self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n    self[policy_id] = class_(observation_space, action_space,\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 50, in __init__\n    self._initialize_loss_from_dummy_batch()\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 832, in _initialize_loss_from_dummy_batch\n    self.compute_actions_from_input_dict(\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 294, in compute_actions_from_input_dict\n    return self._compute_action_helper(input_dict, state_batches,\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n    return func(self, *a, **k)\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 934, in _compute_action_helper\n    dist_inputs, state_out = self.model(input_dict, state_batches,\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/models/modelv2.py\", line 243, in __call__\n    res = self.forward(restored, state or [], seq_lens)\n  File \"/home/zciccwf/phd_project/projects/ddls/ddls/dgl_tests/rllib_model_test.py\", line 150, in forward\n    graph.edata['z'] = torch.Tensor(edge_features)\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/view.py\", line 198, in __setitem__\n    self._graph._set_e_repr(self._etid, self._edges, {key : val})\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/heterograph.py\", line 4220, in _set_e_repr\n    raise DGLError('Expect number of features to match number of edges.'\ndgl._ffi.base.DGLError: Expect number of features to match number of edges. Got 13120 and 145920 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m         \u001b[0;31m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m    924\u001b[0m               env_creator: EnvCreator) -> None:\n\u001b[0;32m--> 925\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3b642e741662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# initialise rllib trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mepoch_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mppo_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nInitialised trainer.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    744\u001b[0m         }\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m         super().__init__(config, logger_creator, remote_checkpoint_dir,\n\u001b[0m\u001b[1;32m    747\u001b[0m                          sync_function_tpl)\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_ip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_ip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0;31m# This matches the behavior of using `build_trainer()`, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# should no longer be used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m             self.workers = self._make_workers(\n\u001b[0m\u001b[1;32m    823\u001b[0m                 \u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mvalidate_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_make_workers\u001b[0;34m(self, env_creator, validate_env, policy_class, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m   1993\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mWorkerSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \"\"\"\n\u001b[0;32m-> 1995\u001b[0;31m         return WorkerSet(\n\u001b[0m\u001b[1;32m   1996\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0mvalidate_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     (not trainer_config.get(\"observation_space\") or\n\u001b[1;32m    100\u001b[0m                      not trainer_config.get(\"action_space\")):\n\u001b[0;32m--> 101\u001b[0;31m                 remote_spaces = ray.get(self.remote_workers(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     lambda p, pid: (pid, p.observation_space, p.action_space)))\n",
      "\u001b[0;32m/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1763\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_individual_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2044934, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f170fb422e0>)\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 586, in __init__\n    self._build_policy_map(\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1577, in _build_policy_map\n    self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n    self[policy_id] = class_(observation_space, action_space,\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 50, in __init__\n    self._initialize_loss_from_dummy_batch()\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 832, in _initialize_loss_from_dummy_batch\n    self.compute_actions_from_input_dict(\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 294, in compute_actions_from_input_dict\n    return self._compute_action_helper(input_dict, state_batches,\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n    return func(self, *a, **k)\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 934, in _compute_action_helper\n    dist_inputs, state_out = self.model(input_dict, state_batches,\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/models/modelv2.py\", line 243, in __call__\n    res = self.forward(restored, state or [], seq_lens)\n  File \"/home/zciccwf/phd_project/projects/ddls/ddls/dgl_tests/rllib_model_test.py\", line 150, in forward\n    graph.edata['z'] = torch.Tensor(edge_features)\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/view.py\", line 198, in __setitem__\n    self._graph._set_e_repr(self._etid, self._edges, {key : val})\n  File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/heterograph.py\", line 4220, in _set_e_repr\n    raise DGLError('Expect number of features to match number of edges.'\ndgl._ffi.base.DGLError: Expect number of features to match number of edges. Got 13120 and 145920 instead."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m 2022-04-14 16:22:13,988\tERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=2044934, ip=128.40.41.23, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f170fb422e0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 586, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     self._build_policy_map(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1577, in _build_policy_map\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     self.policy_map.create_policy(name, orig_cls, obs_space, act_space,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/policy_map.py\", line 143, in create_policy\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     self[policy_id] = class_(observation_space, action_space,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/agents/ppo/ppo_torch_policy.py\", line 50, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     self._initialize_loss_from_dummy_batch()\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/policy.py\", line 832, in _initialize_loss_from_dummy_batch\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     self.compute_actions_from_input_dict(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 294, in compute_actions_from_input_dict\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     return self._compute_action_helper(input_dict, state_batches,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/utils/threading.py\", line 21, in wrapper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     return func(self, *a, **k)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/policy/torch_policy.py\", line 934, in _compute_action_helper\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     dist_inputs, state_out = self.model(input_dict, state_batches,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/ray/rllib/models/modelv2.py\", line 243, in __call__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/home/zciccwf/phd_project/projects/ddls/ddls/dgl_tests/rllib_model_test.py\", line 150, in forward\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     graph.edata['z'] = torch.Tensor(edge_features)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/view.py\", line 198, in __setitem__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     self._graph._set_e_repr(self._etid, self._edges, {key : val})\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m   File \"/scratch/zciccwf/py36/envs/ddls/lib/python3.9/site-packages/dgl/heterograph.py\", line 4220, in _set_e_repr\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m     raise DGLError('Expect number of features to match number of edges.'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=2044934)\u001b[0m dgl._ffi.base.DGLError: Expect number of features to match number of edges. Got 13120 and 145920 instead.\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# load default PPO config and update with custom config params\n",
    "ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "ppo_config.update(rllib_config)\n",
    "print(f'Config:\\n{ppo_config}')\n",
    "\n",
    "# initialise rllib trainer\n",
    "epoch_loop = ppo.PPOTrainer(config=ppo_config)\n",
    "print('\\nInitialised trainer.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab092971",
   "metadata": {},
   "source": [
    "### Run `rllib` on the `ddls` environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4875f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "agent_name = 'PPO'\n",
    "num_epochs = 50\n",
    "rl_training_stats = defaultdict(lambda: [])\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\n------- Epoch {epoch+1} of {num_epochs} -------')\n",
    "    result = epoch_loop.train()\n",
    "    \n",
    "    # print epoch data\n",
    "    print(pretty_print(result))\n",
    "    \n",
    "    # save epoch data\n",
    "    for key, val in result['hist_stats'].items():\n",
    "        rl_training_stats[key].extend(val)\n",
    "    for _ in range(len(val)):\n",
    "        rl_training_stats['seed'].append(result['config']['seed'])\n",
    "        rl_training_stats['agent'].append(agent_name)\n",
    "        rl_training_stats['epoch'].append(epoch)\n",
    "        \n",
    "# display(pd.DataFrame(rl_training_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4298e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'epoch'\n",
    "scaling_factor = 1\n",
    "metrics_to_plot = {'episode_reward', 'episode_lengths'}\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    print(f'Plotting metric {metric}')\n",
    "    fig = plt.figure()\n",
    "    fig = plot_line(pd.DataFrame(rl_training_stats), \n",
    "                    x=x, \n",
    "                    y=metric, \n",
    "                    hue='agent', \n",
    "                    xlabel=x, \n",
    "                    ylabel=metric, \n",
    "                    err_style='band', # 'band' 'bars'\n",
    "                    ci=68, # 95 68\n",
    "                    scaling_factor=scaling_factor,\n",
    "                    show_fig=False)\n",
    "#     plt.axhline(y=np.mean(random_baseline_stats[metric]), linestyle='--', color='#a84a32', label='Random')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.3), ncol=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a9a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58fd01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddls",
   "language": "python",
   "name": "ddls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
