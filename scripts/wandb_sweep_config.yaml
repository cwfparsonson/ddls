#program: train_rllib_from_config.py # RLLIB
program: test_heuristic_from_config.py # HEURISTIC

method: grid
#method: bayes

metric:
  goal: maximize
  name: valid/episode_stats/return
  #goal: minimize
  #name: valid/episode_stats/job_completion_time
  #goal: maximize
  #name: valid/episode_stats/mean_cluster_throughput
  #goal: maximize
  #name: valid/episode_stats/mean_demand_total_throughput

parameters:

    ## TRAIN RLLIB

    #experiment.train_seed:
        #values: [0, 1, 2]
        ##values: [0, 1, 2, 3, 4]

    #epoch_loop.rllib_config.env_config.max_partitions_per_op:
        #values: [2, 4, 6, 8]
        
    #epoch_loop.rllib_config.env_config.reward_function_kwargs.fail_reward_factor:
        #values: [0.25, 0.50, 0.75, 1.25, 1.50, 1.75, 2.0]
    
    #epoch_loop.rllib_config.env_config.reward_function_kwargs.transform_with_log:
        #values: [true, false]
    #epoch_loop.rllib_config.env_config.reward_function_kwargs.normalise:
        #values: [true, false]
    #epoch_loop.rllib_config.env_config.min_op_run_time_quantum:
        #values: [100, 1000]
        
    #epoch_loop.rllib_config.env_config.reward_function_kwargs.blocking_weight:
        #values: [0.1, 0.25, 0.5, 0.75, 1, 1.5, 2, 5]


    ## ML hparam sweep ONE
    #epoch_loop.rllib_config.model.fcnet_activation:
        #values: ['tanh', 'relu', 'swish']
    #epoch_loop.rllib_config.model.custom_model_config.aggregator_activation:
        #values: ['leaky_relu', 'sigmoid', 'relu', 'elu', 'hard_swish', 'softplus', 'mish', 'softsign']
    #epoch_loop.rllib_config.model.custom_model_config.num_rounds:
        #values: [2, 4]

    ## ML hparam sweep TWO
    #epoch_loop.rllib_config.model.custom_model_config.module_depth:
        #values: [1, 2, 4]
    #epoch_loop.rllib_config.gamma:
        #values: [0.7, 0.9, 0.99, 0.999]
        
    ## ML hparam sweep THREE
    #epoch_loop.rllib_config.train_batch_size:
        #values: [32, 64, 128, 256]
    #epoch_loop.rllib_config.sgd_minibatch_size:
        #values: [32, 64, 128, 256]

    ## ML hparam sweep FOUR (should increase number of epochs to give lower LRs a chance)
    #epoch_loop.rllib_config.lr:
        #values: [0.001, 0.0001, 0.00001]

        





    ## TEST HEURISTIC

    experiment.seed:
        values: [0, 1, 2]
        ##values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    
    eval_loop.actor._target_: 
        #values: ['ddls.environments.ramp_job_partitioning.agents.random.Random', 'ddls.environments.ramp_job_partitioning.agents.no_parallelism.NoParallelism', 'ddls.environments.ramp_job_partitioning.agents.sip_ml.SiPML', 'ddls.environments.ramp_job_partitioning.agents.acceptable_jct.AcceptableJCT']
        values: ['ddls.environments.ramp_job_partitioning.agents.no_parallelism.NoParallelism', 'ddls.environments.ramp_job_partitioning.agents.sip_ml.SiPML']

    #eval_loop.env.min_op_run_time_quantum:
        ##values: [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 25, 50, 75, 100]
        #values: [0.001, 0.01, 0.1, 1, 10, 25, 50, 75, 100, 1000]

    eval_loop.env.jobs_config.job_interarrival_time_dist.val:
        #values: [500, 1000, 2000, 4000, 8000, 16000]
        #values: [2000, 2500, 3000, 3500, 4000]
        values: [2000, 2250, 2500, 2750, 3000, 3250, 3500, 3750, 4000]


command:
  - ${env}
  - python
  - ${program}
  - --config-path=ramp_job_partitioning_configs
  - ${args_no_hyphens}
